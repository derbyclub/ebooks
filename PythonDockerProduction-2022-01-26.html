<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Itamar Turner-Trauring" />
  <title>Python on Docker Production Handbook</title>
  <style>
    html {
      line-height: 1.5;
      font-family: Georgia, serif;
      font-size: 20px;
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 1em;
      }
    }
    @media print {
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, 'Lucida Console', Consolas, monospace;
      font-size: 85%;
      margin: 0;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      background-color: #1a1a1a;
      border: none;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    span.underline{text-decoration: underline;}
    div.column{display: inline-block; vertical-align: top; width: 50%;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
    pre > code.sourceCode { white-space: pre; position: relative; }
    pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
    pre > code.sourceCode > span:empty { height: 1.2em; }
    .sourceCode { overflow: visible; }
    code.sourceCode > span { color: inherit; text-decoration: inherit; }
    div.sourceCode { margin: 1em 0; }
    pre.sourceCode { margin: 0; }
    @media screen {
    div.sourceCode { overflow: auto; }
    }
    @media print {
    pre > code.sourceCode { white-space: pre-wrap; }
    pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
    }
    pre.numberSource code
      { counter-reset: source-line 0; }
    pre.numberSource code > span
      { position: relative; left: -4em; counter-increment: source-line; }
    pre.numberSource code > span > a:first-child::before
      { content: counter(source-line);
        position: relative; left: -1em; text-align: right; vertical-align: baseline;
        border: none; display: inline-block;
        -webkit-touch-callout: none; -webkit-user-select: none;
        -khtml-user-select: none; -moz-user-select: none;
        -ms-user-select: none; user-select: none;
        padding: 0 4px; width: 4em;
        color: #aaaaaa;
      }
    pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
    div.sourceCode
      {   }
    @media screen {
    pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
    }
    code span.al { color: #ff0000; font-weight: bold; } /* Alert */
    code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
    code span.at { color: #7d9029; } /* Attribute */
    code span.bn { color: #40a070; } /* BaseN */
    code span.bu { } /* BuiltIn */
    code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
    code span.ch { color: #4070a0; } /* Char */
    code span.cn { color: #880000; } /* Constant */
    code span.co { color: #60a0b0; font-style: italic; } /* Comment */
    code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
    code span.do { color: #ba2121; font-style: italic; } /* Documentation */
    code span.dt { color: #902000; } /* DataType */
    code span.dv { color: #40a070; } /* DecVal */
    code span.er { color: #ff0000; font-weight: bold; } /* Error */
    code span.ex { } /* Extension */
    code span.fl { color: #40a070; } /* Float */
    code span.fu { color: #06287e; } /* Function */
    code span.im { } /* Import */
    code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
    code span.kw { color: #007020; font-weight: bold; } /* Keyword */
    code span.op { color: #666666; } /* Operator */
    code span.ot { color: #007020; } /* Other */
    code span.pp { color: #bc7a00; } /* Preprocessor */
    code span.sc { color: #4070a0; } /* SpecialChar */
    code span.ss { color: #bb6688; } /* SpecialString */
    code span.st { color: #4070a0; } /* String */
    code span.va { color: #19177c; } /* Variable */
    code span.vs { color: #4070a0; } /* VerbatimString */
    code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
    .display.math{display: block; text-align: center; margin: 0.5rem auto;}
  </style>
  <link href="https://fonts.googleapis.com/css?family=Rosario:400,400i,700|Ubuntu+Mono:400,700" rel="stylesheet">
  <link rel="stylesheet"
    href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.6/styles/default.min.css">
  <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.6/highlight.min.js"></script>
  <script>hljs.initHighlightingOnLoad();</script>
  <style>
  body {
     font-family: 'Rosario', sans-serif;
     max-width: 42rem;
     margin: 4rem auto;
     font-size: 20px;
     line-height: 1.4;
  }
  pre, code {
     font-family: 'Ubuntu Mono', monospace;
     font-size: 90%;
  }
  a {
      text-decoration: underline !important;
  }
  </style>
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
</head>
<body>
<header id="title-block-header">
<h1 class="title">Python on Docker Production Handbook</h1>
<p class="author">Itamar Turner-Trauring</p>
</header>
<nav id="TOC" role="doc-toc">
<ul>
<li><a href="#legal-disclaimer">Legal disclaimer</a></li>
<li><a href="#introduction">Introduction</a></li>
<li><a href="#plan">A phased implementation plan</a>
<ul>
<li><a href="#step-1-it-works">Step 1: It works</a></li>
<li><a href="#step-2-basic-security">Step 2: Basic security</a></li>
<li><a href="#step-3-running-in-ci">Step 3: Running in CI</a></li>
<li><a href="#step-4-easier-to-debug">Step 4: Easier to debug</a></li>
<li><a href="#step-5-additional-operational-correctness">Step 5:
Additional operational correctness</a></li>
<li><a href="#step-6-reproducible-builds">Step 6: Reproducible
builds</a></li>
<li><a href="#step-7-faster-builds">Step 7: Faster builds</a></li>
<li><a href="#step-8-smaller-images">Step 8: Smaller images</a></li>
</ul></li>
<li><a href="#docker-versions-and-buildkit">Docker versions and
BuildKit</a>
<ul>
<li><a href="#docker-19.03-and-20.10">Docker 19.03 and 20.10</a></li>
<li><a href="#buildkit">BuildKit</a></li>
<li><a href="#podman">Podman</a></li>
</ul></li>
<li><a href="#best-practices-security">Best practices: Security</a>
<ul>
<li><a href="#no-root">Don’t run as root</a></li>
<li><a href="#port-1024">Don’t listen on ports &lt; 1024</a></li>
<li><a href="#no-capabilities">Run your container with no
capabilities</a></li>
<li><a href="#update-system-packages">Update system packages</a></li>
<li><a href="#weekly-rebuild">∞ Rebuild without caching and redeploy at
least weekly ∞</a></li>
<li><a href="#apply-security-fixes">∞ Apply security fixes when they
come out ∞</a></li>
<li><a href="#security-scanners">Run security scanners</a></li>
<li><a href="#secret-files">Don’t leak secret files</a></li>
<li><a href="#runtime-secrets">Don’t leak runtime secrets</a></li>
<li><a href="#build-secrets">Don’t leak build secrets</a></li>
<li><a href="#compose-secrets">Using BuildKit secrets from Docker
Compose</a></li>
<li><a href="#use-host-ssh-keys-with-buildkit-ssh-agent-forwarding">Use
host SSH keys with BuildKit <code>ssh-agent</code> forwarding</a></li>
<li><a href="#optional-pre-populate-ssh-known-hosts">Optional:
Pre-populate SSH known hosts</a></li>
</ul></li>
<li><a href="#best-practices-running-in-ci">Best practices: Running in
CI</a>
<ul>
<li><a href="#smoke-test">Add a smoke test for your image</a></li>
<li><a href="#additional-checks">Additional checks:
<code>hadolint</code>, size checks</a></li>
<li><a href="#tag-branch">Tag images based on the version control
branch</a></li>
<li><a href="#latest-tag">Don’t rely on the <code>latest</code>
tag</a></li>
<li><a href="#pull-in-ci">Warm up the build cache</a></li>
<li><a href="#pull-in-ci-2">Warm up the build cache for per-branch
builds</a></li>
<li><a href="#optional-self-warming-cache-with-buildkit">Optional:
Self-warming cache with BuildKit</a></li>
</ul></li>
<li><a href="#best-practices-make-debugging-easier">Best practices: Make
debugging easier</a>
<ul>
<li><a href="#stdout-logs">Write logs to <code>stdout</code> or
<code>stderr</code></a></li>
<li><a href="#faulthandler">Prepare for C crashes</a></li>
<li><a href="#identifiable">Record the build’s version control revision
and branch</a></li>
<li><a href="#useful-tools">Optional: Pre-install useful tools</a></li>
</ul></li>
<li><a href="#best-practices-correct-operation">Best practices: Correct
operation</a>
<ul>
<li><a href="#networkbind">Have public ports listen on
<code>0.0.0.0</code></a></li>
<li><a href="#bash">Avoid bash, or at least use bash strict mode and
<code>shellcheck</code></a></li>
<li><a href="#shutdown">Ensure fast shutdowns</a></li>
<li><a href="#init">Add an <code>init</code> process</a></li>
<li><a href="#deb-interactive">Set a non-interactive frontend for
Debian/Ubuntu package installs</a></li>
<li><a href="#health-checks">Add health checks</a></li>
<li><a href="#bytecode">Pre-compile bytecode for faster startup</a></li>
</ul></li>
<li><a href="#best-practices-reproducible-builds">Best practices:
Reproducible builds</a>
<ul>
<li><a href="#base">Choose a stable base image and tag</a></li>
<li><a href="#redhat-compatible-base-images">RedHat-compatible base
images</a></li>
<li><a href="#pin-python">Pin your Python dependencies</a></li>
<li><a href="#update-dependencies">∞ Update all pinned dependencies once
a month ∞</a></li>
<li><a href="#pin-system">Optional: Pin system packages</a></li>
<li><a href="#custom-base-image">Optional: Create a custom base
image</a></li>
</ul></li>
<li><a href="#best-practices-faster-builds">Best practices: Faster
builds</a>
<ul>
<li><a href="#no-alpine">Don’t use Alpine Linux</a></li>
<li><a href="#copy-late"><code>COPY</code> in files only when
needed</a></li>
<li><a href="#install-dependencies-first">Install dependencies
separately from your code</a></li>
<li><a href="#arg-late">Use <code>ARG</code> only when needed</a></li>
<li><a href="#use-docker-build---label-instead-of-label">Use
<code>docker build --label</code> instead of <code>LABEL</code></a></li>
</ul></li>
<li><a href="#best-practices-small-images">Best practices: Small
images</a>
<ul>
<li><a href="#temporary-files">Keep temporary files from ending up in a
layer</a></li>
<li><a href="#dive">Find large layers and files with
<code>dive</code></a></li>
<li><a href="#dockerignore">Add files to
<code>.dockerignore</code></a></li>
<li><a href="#git-dockerignore">Add <code>.git</code> to
<code>.dockerignore</code> (and some alternatives when you
can’t)</a></li>
<li><a href="#chown">Avoid extra chowns</a></li>
<li><a href="#system-packages-clean">Don’t install unnecessary system
packages, and clean up when you’re done</a></li>
<li><a href="#smaller-pip">Disable pip caching</a></li>
<li><a href="#package-cache-buildkit">Optional: Caching installed
packages using BuildKit</a></li>
<li><a href="#unnecessary-files">Optional: Remove files you don’t
need</a></li>
</ul></li>
<li><a href="#best-practices-application-and-tool-specific">Best
practices: Application and tool-specific</a>
<ul>
<li><a href="#using-a-virtualenv-in-your-dockerfile">Using a virtualenv
in your <code>Dockerfile</code></a></li>
<li><a href="#dont-run-database-upgrades-on-startup">Don’t run database
upgrades on startup</a></li>
<li><a href="#slow-queries">Make sure slow queries don’t break health
checks</a></li>
<li><a href="#enable-running-other-commands-via-the-command-line">Enable
running other commands via the command-line</a></li>
<li><a href="#gunicorn">Configure the Gunicorn web server
correctly</a></li>
<li><a href="#uwsgi">Configure the uWSGI web server correctly</a></li>
<li><a href="#heroku">Respect the <code>PORT</code> environment variable
on Heroku and Google Cloud Run</a></li>
</ul></li>
<li><a href="#conda">Best practices: Conda</a>
<ul>
<li><a href="#using-a-conda-environment-in-your-dockerfile">Using a
Conda environment in your <code>Dockerfile</code></a></li>
<li><a href="#conda-environments-require-activation">Conda environments
require activation</a></li>
<li><a href="#reproducible-builds-with-conda-lock">Reproducible builds
with <code>conda-lock</code></a></li>
<li><a href="#make-conda-images-smaller-with-conda-clean">Make Conda
images smaller with <code>conda clean</code></a></li>
<li><a href="#make-conda-images-smaller-by-using-openblas">Make Conda
images smaller by using OpenBLAS</a></li>
<li><a
href="#even-smaller-conda-images-with-conda-pack-and-multi-stage-builds">Even
smaller Conda images with <code>conda-pack</code> and multi-stage
builds</a></li>
<li><a href="#faster-installs-with-mamba">Faster installs with
Mamba</a></li>
<li><a href="#security-scans-for-conda-environments">Security scans for
Conda environments</a></li>
</ul></li>
<li><a href="#pipenv">Best practices: Pipenv</a>
<ul>
<li><a
href="#install-pipenv-separately-from-your-application-code">Install
Pipenv separately from your application code</a></li>
<li><a href="#installing-dependencies-inside-a-container">Installing
dependencies inside a container</a></li>
</ul></li>
<li><a href="#poetry">Best practices: Poetry</a>
<ul>
<li><a
href="#install-poetry-separately-from-your-application-code">Install
Poetry separately from your application code</a></li>
<li><a
href="#installing-outside-of-poetrys-managed-virtualenvs">Installing
outside of Poetry’s managed virtualenvs</a></li>
<li><a href="#poetry-dev-depencies">Don’t install development
dependencies (unless you want to)</a></li>
<li><a href="#poetry-dependencies-first">Speed up rebuilds by installing
dependencies separately</a></li>
<li><a href="#poetry-version-vs-caching">Application version changes can
lead to slow rebuilds</a></li>
</ul></li>
<li><a href="#multi-stage">Best practices: Multi-stage builds</a>
<ul>
<li><a href="#omit-build-dependencies-from-your-runtime-image">Omit
build dependencies from your runtime image</a></li>
<li><a href="#use-a-virtualenv-to-make-copying-across-stages-easier">Use
a virtualenv to make copying across stages easier</a></li>
<li><a href="#avoid-unnecessary-complete-rebuilds">Avoid unnecessary
complete rebuilds</a></li>
<li><a href="#optional-use-buildkit-for-faster-builds">Optional: Use
BuildKit for faster builds</a></li>
<li><a href="#optional-optimizing-image-size-even-further">Optional:
Optimizing image size even further</a></li>
</ul></li>
<li><a href="#some-final-recommendations">Some final
recommendations</a></li>
<li><a href="#changelog">Changelog</a></li>
</ul>
</nav>
<h1 class="unnumbered" id="legal-disclaimer">Legal disclaimer</h1>
<p>Copyright © 2021 Hyphenated Enterprises LLC. All rights reserved.</p>
<p><strong>This quickstart and the information it contains are provided
“as is”, without warranties of any kind, including without limitation
the implied warranties of merchantability or fitness for any particular
purpose. In no event shall the author, Itamar Turner-Trauring, or the
copyright owner, Hyphenated Enterprises LLC, or any of their employees,
agents or affiliates, be liable for any claim, costs, expenses, damages
or other liability, whether under principles of contract, tort or
otherwise, arising from, out of or in connection with the use of or
reliance on the information contained in this handbook.</strong></p>
<h1 class="unnumbered" id="introduction">Introduction</h1>
<p>Building production-ready Docker images isn’t easy: there’s a huge
list of details you need to remember and get right. The goal of this
handbook is therefore to get you going as quickly as possible, while
still providing a reference to all the relevant details.</p>
<p>How to read this guide:</p>
<ol type="1">
<li>Read the next chapter, which presents a possible implementation
plan.</li>
<li>Skim the guide, so you get a sense of all of the best practices; not
all of them are listed in the implementation plan.</li>
<li>Go back to the implementation and create your own,
situation-specific plan with the best practices you actually care
about.</li>
<li>Start packaging!</li>
<li>Refer back to the guide when you need additional details.</li>
</ol>
<p>A few of the best practices are marked with <strong>∞</strong>
(infinity) symbols, indicating they are ongoing processes. These tasks
require periodic attention, even after you’ve finished creating and
deploying your image.</p>
<p>If you have any questions or suggestions, please email me at <a
href="mailto:itamar@pythonspeed.com">itamar@pythonspeed.com</a>.</p>
<p>—Itamar Turner-Trauring</p>
<h1 id="plan">A phased implementation plan</h1>
<p>There are many best practices to follow, and some are more important
than others. What follows is a proposed order of implementation for your
Docker packaging, with the presumption that your goal is to get
something working as quickly as possible.</p>
<p>To summarize the proposed order:</p>
<ol type="1">
<li>Get something minimal working, so that it’s buildable on your
development laptop or workstation.</li>
<li>Implement basic security.</li>
<li>If relevant, have builds run in CI or your build system, so other
people can use it.</li>
<li>Make the images easier to identify and debug.</li>
<li>Improve operational correctness.</li>
<li>Make the builds reproducible.</li>
<li>Speed up the build time.</li>
<li>Make the image smaller.</li>
</ol>
<blockquote>
<p><strong>Important:</strong> This chapter doesn’t cover all the best
practices. You should also skim the guide and read the other best
practices, to learn about other ways to enhance your build.</p>
</blockquote>
<h2 id="step-1-it-works">Step 1: It works</h2>
<p>The first step is just getting something working, so you have
something you can improve. That means:</p>
<ol type="1">
<li>Writing your initial <code>Dockerfile</code>.</li>
<li>Writing the script that will run <code>docker image build</code>;
for now it might be quite simple, but it’s likely to grow over
time.</li>
</ol>
<p>I suggest starting with the following best practices:</p>
<ul>
<li>Choose a stable base image and tag (<a href="#base">ref</a>), which
is not Alpine Linux (<a href="#no-alpine">ref</a>). Since you’ll need to
choose a base image anyway, you may as well start with a good one.</li>
<li>For network servers, listen on <code>0.0.0.0</code> (<a
href="#networkbind">ref</a>). Your image isn’t working if you can’t
connect to it.</li>
<li>Write logs to <code>stdout</code> (<a href="#stdout-logs">ref</a>),
so you can see what’s going when you run the container.</li>
<li>If you’re writing any shell scripts, make sure they’re not broken
(<a href="#bash">ref</a>). In theory you can wait to do this later, but
my experience is that if you don’t do this from the start, it can be
difficult to add the necessary checks later on.</li>
</ul>
<p>If you’re installing system packages using <code>apt-get</code>,
you’ll want to make sure <code>apt-get</code> never runs in interactive
mode (<a href="#deb-interactive">ref</a>).</p>
<p>See also the best practices for Conda (<a href="#conda">ref</a>) and
best practices for Poetry (<a href="#poetry">ref</a>) and Pipenv (<a
href="#pipenv">ref</a>) if you’re using those tools.</p>
<h3 class="unnumbered" id="partial-example">Partial example</h3>
<p>At this point the <code>Dockerfile</code> for a simple application
might look like this:</p>
<pre class="dockerfile"><code>FROM python:3.9-slim-bullseye
WORKDIR /app
COPY . .
RUN pip install -r requirements.txt
ENTRYPOINT [&quot;python&quot;, &quot;server.py&quot;]</code></pre>
<p>And <code>build.sh</code> will look like this:</p>
<div class="sourceCode" id="cb2"><pre
class="sourceCode bash"><code class="sourceCode bash"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="co">#!/bin/bash</span></span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a><span class="bu">set</span> <span class="at">-euo</span> pipefail</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a><span class="ex">docker</span> image build <span class="at">-t</span> yourimage .</span></code></pre></div>
<h2 id="step-2-basic-security">Step 2: Basic security</h2>
<p>Now that you have something working, the next step is to make your
image sufficiently secure to run somewhere public. If it’s not secure,
you can’t run it in production; if you can’t run it, what’s the
point?</p>
<ul>
<li>Don’t listen on ports &lt; 1024 (<a href="#port-1024">ref</a>).</li>
<li>Don’t run as root (<a href="#no-root">ref</a>).</li>
<li>Run your container with no capabilities (<a
href="#no-capabilities">ref</a>).</li>
<li>Update system packages in your <code>Dockerfile</code> (<a
href="#update-system-packages">ref</a>).</li>
<li>Set up the technical and organizational processes you need to apply
security fixes when they come out (<a
href="#apply-security-fixes">ref</a>).</li>
<li>Don’t leak runtime secrets (<a href="#runtime-secrets">ref</a>) or
build secrets (<a href="#builds-secrets">ref</a>).</li>
</ul>
<p>In some cases you might need to apply other best practices:</p>
<ul>
<li>If you’re using build secrets, make sure you’re not leaking them (<a
href="#build-secrets">ref</a>), and likewise for runtime secrets (<a
href="#runtime-secrets">ref</a>).</li>
<li>If you have any files you don’t want to be in the image for security
reasons, make sure they’re not leaked (<a
href="#secret-files">ref</a>).</li>
</ul>
<h3 class="unnumbered" id="partial-example-1">Partial example</h3>
<p>The <code>Dockerfile</code> might now look like this:</p>
<pre class="dockerfile"><code>FROM python:3.9-slim-bullseye

# Install security updates:
RUN apt-get update &amp;&amp; apt-get -y upgrade

WORKDIR /app
COPY . .
RUN pip install -r requirements.txt

# Run as non-root user:
RUN useradd --create-home appuser
USER appuser

ENTRYPOINT [&quot;python&quot;, &quot;server.py&quot;]</code></pre>
<h2 id="step-3-running-in-ci">Step 3: Running in CI</h2>
<p>Now that you have a working image with basic security, usually the
next step is to build the images automatically in your CI or build
system. This will allow other people on your team to get images built
automatically, and reduce human error caused by manual builds.</p>
<p>You’ll typically want your build system to build the image and push
it to your chosen image registry—perhaps on every commit, perhaps on
pull requests, depending on your particular development workflow. You
might also start thinking about automatic deploys at this point,
depending how your production environment runs.</p>
<p>As you’re doing so, follow these best practices:</p>
<ul>
<li>Tag images based on the version control branch (<a
href="#tag-branch">ref</a>).</li>
<li>Don’t rely on the <code>latest</code> tag (<a
href="#latest-tag">ref</a>).</li>
<li>Set up an automatic rebuild-and-redeploy once a week so you get
security updates to the image’s system packages (<a
href="#weekly-rebuild">ref</a>).</li>
<li>Run security scanners on your code (<a
href="#security-scanners">ref</a>).</li>
</ul>
<p>You can put these off until you’re working on faster builds, or do
them here:</p>
<ul>
<li>For faster builds, warm up the build cache (<a
href="#pull-in-ci">ref</a>).</li>
<li>Warm up the build cache for per-branch builds (<a
href="#pull-in-ci-2">ref</a>).</li>
</ul>
<h3 class="unnumbered" id="partial-example-2">Partial example</h3>
<p>Your build script will now look something like this:</p>
<div class="sourceCode" id="cb4"><pre
class="sourceCode bash"><code class="sourceCode bash"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="co">#!/bin/bash</span></span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a><span class="bu">set</span> <span class="at">-euo</span> pipefail</span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a><span class="va">IMAGE_NAME</span><span class="op">=</span>registry.example.com/yourorg/yourserver</span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a><span class="va">GIT_BRANCH</span><span class="op">=</span><span class="va">$(</span><span class="fu">git</span> rev-parse <span class="at">--abbrev-ref</span> HEAD<span class="va">)</span></span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a><span class="va">GIT_COMMIT</span><span class="op">=</span><span class="va">$(</span><span class="fu">git</span> rev-parse <span class="at">--short</span> HEAD<span class="va">)</span></span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Pull previous version, and use with --cache-now</span></span>
<span id="cb4-10"><a href="#cb4-10" aria-hidden="true" tabindex="-1"></a><span class="co"># for build caching:</span></span>
<span id="cb4-11"><a href="#cb4-11" aria-hidden="true" tabindex="-1"></a><span class="ex">docker</span> pull <span class="va">$IMAGE_NAME</span>:<span class="va">$GIT_BRANCH</span> <span class="kw">||</span> <span class="fu">true</span></span>
<span id="cb4-12"><a href="#cb4-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-13"><a href="#cb4-13" aria-hidden="true" tabindex="-1"></a><span class="co"># Use branch+commit for tagging:</span></span>
<span id="cb4-14"><a href="#cb4-14" aria-hidden="true" tabindex="-1"></a><span class="ex">docker</span> build <span class="at">-t</span> <span class="st">&quot;</span><span class="va">$IMAGE_NAME</span><span class="st">:</span><span class="va">$GIT_BRANCH</span><span class="st">&quot;</span> <span class="dt">\</span></span>
<span id="cb4-15"><a href="#cb4-15" aria-hidden="true" tabindex="-1"></a>             <span class="at">-t</span> <span class="st">&quot;</span><span class="va">$IMAGE_NAME</span><span class="st">:</span><span class="va">$GIT_COMMIT</span><span class="st">&quot;</span> <span class="dt">\</span></span>
<span id="cb4-16"><a href="#cb4-16" aria-hidden="true" tabindex="-1"></a>             <span class="at">--build-arg</span> BUILDKIT_INLINE_CACHE=1 <span class="dt">\</span></span>
<span id="cb4-17"><a href="#cb4-17" aria-hidden="true" tabindex="-1"></a>             <span class="at">--cache-from</span><span class="op">=</span><span class="va">$IMAGE_NAME</span>:<span class="va">$GIT_BRANCH</span> .</span>
<span id="cb4-18"><a href="#cb4-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-19"><a href="#cb4-19" aria-hidden="true" tabindex="-1"></a><span class="co"># Security scanners:</span></span>
<span id="cb4-20"><a href="#cb4-20" aria-hidden="true" tabindex="-1"></a><span class="ex">trivy</span> image <span class="at">--ignore-unfixed</span> <span class="at">--exit-code</span> 1 <span class="dt">\</span></span>
<span id="cb4-21"><a href="#cb4-21" aria-hidden="true" tabindex="-1"></a>    <span class="va">$IMAGE_NAME</span>:<span class="va">$GIT_BRANCH</span></span>
<span id="cb4-22"><a href="#cb4-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-23"><a href="#cb4-23" aria-hidden="true" tabindex="-1"></a><span class="co"># Push to the registry:</span></span>
<span id="cb4-24"><a href="#cb4-24" aria-hidden="true" tabindex="-1"></a><span class="ex">docker</span> push <span class="st">&quot;</span><span class="va">$IMAGE_NAME</span><span class="st">:</span><span class="va">$GIT_BRANCH</span><span class="st">&quot;</span></span>
<span id="cb4-25"><a href="#cb4-25" aria-hidden="true" tabindex="-1"></a><span class="ex">docker</span> push <span class="st">&quot;</span><span class="va">$IMAGE_NAME</span><span class="st">:</span><span class="va">$GIT_COMMIT</span><span class="st">&quot;</span></span></code></pre></div>
<p>You’ll also need additional code to ensure a complete rebuild
(<code>docker build --pull --no-cache</code>) once a week.</p>
<h2 id="step-4-easier-to-debug">Step 4: Easier to debug</h2>
<p>Now that your image is building automatically, you will start
accumulating many more images. Now is the time to make images
identifiable and easier to debug.</p>
<ul>
<li>Prepare for C crashes (<a href="#faulthandler">ref</a>). It’s one
extra line in your <code>Dockerfile</code> that can save hours or even
days of debugging later on.</li>
<li>Record the build’s version control revision and branch in the image
itself (<a href="#identifiable">ref</a>).</li>
<li>Add a smoke test to your CI build (<a
href="#smoke-test">ref</a>).</li>
</ul>
<p>You might also want to:</p>
<ul>
<li>Pre-install useful tools (<a href="#useful-tools">ref</a>).</li>
</ul>
<h3 class="unnumbered" id="partial-example-3">Partial example</h3>
<p>Your <code>Dockerfile</code> will add the following line:</p>
<div class="sourceCode" id="cb5"><pre
class="sourceCode dockerfile"><code class="sourceCode dockerfile"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="kw">ENV</span> PYTHONFAULTHANDLER=1</span></code></pre></div>
<p>And the build script’s <code>docker build</code> line will look like
this:</p>
<div class="sourceCode" id="cb6"><pre
class="sourceCode bash"><code class="sourceCode bash"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="ex">docker</span> build <span class="at">-t</span> <span class="st">&quot;</span><span class="va">$IMAGE_NAME</span><span class="st">:</span><span class="va">$GIT_BRANCH</span><span class="st">&quot;</span> <span class="dt">\</span></span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a>             <span class="at">-t</span> <span class="st">&quot;</span><span class="va">$IMAGE_NAME</span><span class="st">:</span><span class="va">$GIT_COMMIT</span><span class="st">&quot;</span> <span class="dt">\</span></span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a>             <span class="at">--label</span> git-commit=<span class="va">$GIT_COMMIT</span> <span class="dt">\</span></span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a>             <span class="at">--label</span> git-branch=<span class="va">$GIT_BRANCH</span> <span class="dt">\</span></span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a>             <span class="at">--build-arg</span> BUILDKIT_INLINE_CACHE=1 <span class="dt">\</span></span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a>             <span class="at">--cache-from</span><span class="op">=</span><span class="va">$IMAGE_NAME</span>:<span class="va">$GIT_BRANCH</span> .</span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a><span class="ex">python</span> smoketest.py <span class="va">$IMAGE_NAME</span>:<span class="va">$GIT_BRANCH</span></span></code></pre></div>
<h2 id="step-5-additional-operational-correctness">Step 5: Additional
operational correctness</h2>
<p>Next, you should make your images behave correctly.</p>
<ul>
<li>Make sure your image shuts down correctly (<a
href="#shutdown">ref</a>).</li>
<li>Make sure your image can handle zombie processes correctly (<a
href="#init">ref</a>).</li>
<li>If it’s a long-running process, implement health checks (<a
href="#health-checks">ref</a>).</li>
<li>If it’s a server, make sure the server can handle health checks and
slow queries at the same time (<a href="#slow-queries">ref</a>).</li>
<li>Pre-compile bytecode for faster startup (<a
href="#bytecode">ref</a>).</li>
</ul>
<p>You may also want some application-specific and deployment
environment configuration:</p>
<ul>
<li>Gunicorn requires certain options (<a
href="#gunicorn">ref</a>).</li>
<li>uWSGI requires certain options (<a href="#uwsgi">ref</a>).</li>
<li>Heroku and Google Cloud Run require listening on a configurable port
(<a href="#heroku">ref</a>).</li>
</ul>
<h3 class="unnumbered" id="partial-example-4">Partial example</h3>
<p>Your <code>Dockerfile</code> might now look like this:</p>
<pre class="dockerfile"><code>FROM python:3.9-slim-bullseye

RUN apt-get update &amp;&amp; apt-get -y upgrade
# Install tini:
RUN export DEBIAN_FRONTEND=noninteractive &amp;&amp; \
    apt-get install -y tini

WORKDIR /app
COPY . .
RUN pip install -r requirements.txt


RUN useradd --create-home appuser
USER appuser

ENV PYTHONFAULTHANDLER=1
# Use tini as an init process:
ENTRYPOINT [&quot;tini&quot;, &quot;--&quot;, &quot;python&quot;, &quot;server.py&quot;]</code></pre>
<h2 id="step-6-reproducible-builds">Step 6: Reproducible builds</h2>
<p>In the first week or so of packaging and development, it’s unlikely
that any major changes will happen to your Python dependencies. As time
goes on, the software you depend is more and more likely to change. And
that can break your build, or break your application.</p>
<p>So next you should ensure your builds are reproducible, always
installing the software versions you specifically asked for.</p>
<ul>
<li>Pin your Python dependencies (<a href="#pin-python">ref</a>).</li>
<li>Set up the technical and organizational processes to update all
pinned dependencies once a month (<a
href="#update-dependencies">ref</a>).</li>
</ul>
<p>You might also want to:</p>
<ul>
<li>Pin your system package dependencies (<a
href="#pin-system">ref</a>).</li>
<li>Create a custom base image (<a
href="#custom-base-image">ref</a>).</li>
</ul>
<h2 id="step-7-faster-builds">Step 7: Faster builds</h2>
<p>Now that you have good images, it’s time to look at optimizations.
Your time is expensive, so it’s worth spending some effort to get faster
builds, by optimizing Docker’s build caching.</p>
<p>If this is the second or third time you’re packaging an application,
you might have done these automatically earlier on, but if not:</p>
<ul>
<li><code>COPY</code> in files only when needed (<a
href="#copy-late">ref</a>).</li>
<li>Use <code>ARG</code> only when needed (<a
href="#arg-late">ref</a>).</li>
<li>Install dependencies separately from your code (<a
href="#install-dependencies-first">ref</a>).</li>
</ul>
<h3 class="unnumbered" id="partial-example-5">Partial example</h3>
<pre class="dockerfile"><code>FROM python:3.9-slim-bullseye

RUN apt-get update &amp;&amp; apt-get -y upgrade
RUN export DEBIAN_FRONTEND=noninteractive &amp;&amp; \
    apt-get install -y tini

# Only copy in requirements.txt for now:
COPY requirements.txt .
RUN pip install -r requirements.txt

RUN useradd --create-home appuser
USER appuser

WORKDIR /app
# Copy in the rest of the files:
COPY . .

ENV PYTHONFAULTHANDLER=1
ENTRYPOINT [&quot;tini&quot;, &quot;--&quot;, &quot;python&quot;, &quot;server.py&quot;]</code></pre>
<h2 id="step-8-smaller-images">Step 8: Smaller images</h2>
<p>Finally, your images might be quite large at this point, so it’s good
to make them smaller. This will save download time, as well as bandwidth
and disk space.</p>
<ul>
<li>Keep temporary files from ending up in a layer (<a
href="#temporary-files">ref</a>).</li>
<li>Add files to <code>.dockerignore</code> (<a
href="#dockerignore">ref</a>).</li>
<li>Add <code>.git</code> to <code>.dockerignore</code>, or consider
alternatives when you can’t (<a href="#git-dockerignore">ref</a>)</li>
<li>Avoid extra <code>chown</code>s (<a href="#chown">ref</a>).</li>
<li>Minimize system package installation (<a
href="#system-packages-clean">ref</a>).</li>
<li>Reduce disk usage from <code>pip</code> installs (<a
href="#smaller-pip">ref</a>).</li>
</ul>
<p>You might find that you can’t have both small images and fast builds.
Typically this happens when you need to install a compiler toolchain:
caching enables fast builds, but also makes your images larger. In this
situation you may want to consider multi-stage builds (<a
href="#multi-stage">ref</a>).</p>
<h3 class="unnumbered" id="partial-example-6">Partial example</h3>
<p>Your <code>Dockerfile</code> might now look like this:</p>
<pre class="dockerfile"><code>FROM python:3.9-slim-bullseye

# Clean up after installing packages:
RUN export DEBIAN_FRONTEND=noninteractive &amp;&amp; \
    apt-get update &amp;&amp; \
    apt-get -y upgrade &amp;&amp; \
    apt-get install -y --no-install-recommends tini &amp;&amp; \
    apt-get -y clean &amp;&amp; \
    rm -rf /var/lib/apt/lists/*

COPY requirements.txt .
# Don&#39;t cache pip-downloaded packages:
RUN pip install --no-cache-dir -r requirements.txt

RUN useradd --create-home appuser
USER appuser

WORKDIR /app
COPY . .

ENV PYTHONFAULTHANDLER=1
ENTRYPOINT [&quot;tini&quot;, &quot;--&quot;, &quot;python&quot;, &quot;server.py&quot;]</code></pre>
<h1 id="docker-versions-and-buildkit">Docker versions and BuildKit</h1>
<p>Before moving on to the details of the best practices, it’s worth
understanding the available versions of Docker, and what they
support.</p>
<h2 id="docker-19.03-and-20.10">Docker 19.03 and 20.10</h2>
<p>As of February 2021, there are two major versions of Docker
available:</p>
<ul>
<li>Docker 19.03, which is widely available in recent operating systems,
cloud CI services, and so on.</li>
<li>The newly released Docker 20.10, which will become more available
over time on the server side but is still quite new.</li>
</ul>
<p>If you’re using Docker Desktop on macOS and Windows you will end up
getting updated to Docker 20.10 as part of the update process.</p>
<p>Docker 20.10 has a number of new features; I will mention them inline
where relevant.</p>
<p>If you are using Docker 18.09 or older, I recommend upgrading: you’re
missing two years’ worth of bug fixes and enhancements.</p>
<h2 id="buildkit">BuildKit</h2>
<p>BuildKit is a re-implementation of Docker’s image building system,
included in both Docker 19.03 and 20.10. It includes performance
enhancements, like parallel builds where possible, and new features like
build secrets, SSH agent forwarding from the host, and additional
caching features.</p>
<p>To enable it:</p>
<ul>
<li>You can set the environment variable <code>DOCKER_BUILDKIT</code> to
<code>1</code>, e.g. <code>export DOCKER_BUILDKIT=1</code>.</li>
<li>On recent versions of Docker Desktop on macOS and Windows there
should be a setting in the Preferences UI; on new installs it will be
enabled by default.</li>
</ul>
<h3 id="getting-complete-output-with-buildkit">Getting complete output
with BuildKit</h3>
<p>BuildKit changes the way build progress is output. To make sure you
see the output of <code>RUN</code> commands, you can run with the
<code>--progress=plain</code> command-line argument.</p>
<pre><code>$ docker image build --progress=plain .</code></pre>
<h3 id="enabling-buildkit-features-in-your-dockerfile">Enabling BuildKit
features in your Dockerfile</h3>
<p>With the release of Docker 20.10, BuildKit has now stabilized, which
is why it is enabled by default on new macOS and Windows installs.</p>
<p>Among other features, BuildKit allows configuring the builder a
per-<code>Dockerfile</code> basis. On Docker 20.10 it will use BuildKit
v1.3 syntax by default when BuildKit is enabled, but on Docker 19.03 you
need to set this explicitly in your <code>Dockerfile</code>, like
so:</p>
<div class="sourceCode" id="cb11"><pre
class="sourceCode dockerfile"><code class="sourceCode dockerfile"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a><span class="co"># syntax = docker/dockerfile:1.3</span></span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a><span class="kw">FROM</span> python:3.9-slim-bullseye</span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a><span class="co"># ... rest of Dockerfile goes here</span></span></code></pre></div>
<p>On Docker 20.10 this isn’t strictly necessary, but it is still
worthwhile: you’ll automatically get the latest bugfix version
downloaded (it’s a Docker image!), even if it’s not packaged in your
particular release of Docker 20.10. If you’re using BuildKit, then, you
should just add that line at the top of all your
<code>Dockerfile</code>s.</p>
<p>BuildKit mostly works pretty well on Docker 19.03, so you can use it
even if you still haven’t upgraded to 20.10.</p>
<p>Again, I will be covering BuildKit features inline where
relevant.</p>
<h3 id="enabling-buildkit-with-docker-compose">Enabling BuildKit with
Docker Compose</h3>
<p>Docker Compose doesn’t work out of the box with BuildKit. You can
enable support for BuildKit by setting an additional environment
variable beyond <code>DOCKER_BUILDKIT</code>:</p>
<pre class="shell-session"><code>$ export COMPOSE_DOCKER_CLI_BUILD=1</code></pre>
<p>Unfortunately this method of using Docker results in somewhat worse
error messages when builds fail. Not all BuildKit features are
supported, see the best practice below on <a
href="#compose-secrets">build secrets with Compose</a>.</p>
<h3 id="ensuring-caching-works-correctly-with-buildkit">Ensuring caching
works correctly with BuildKit</h3>
<p>BuildKit has a much more sophisticated caching system; you can export
the cache to the filesystem, and store it separately from images. In
order for caching to work the way it previously worked with Docker when
using <code>--cache-from</code>, you will need to add <code>--build-arg
BUILDKIT_INLINE_CACHE=1</code> to your build script. See the best
practice on warming the cache for details (<a
href="#pull-in-ci">ref</a>).</p>
<h2 id="podman">Podman</h2>
<p>On RedHat operating systems, an alternative implementation of Docker
called Podman is installed. Podman is supposed to be completely
compatible with Docker on the CLI side and to some extent on the API
side as well, allowing support for tools like Docker Compose.</p>
<p>It does not yet support the BuildKit features, but it may have
alternative ways to achieve the same goals. You can use BuildKit to
build images and then run them on Podman, and you can use a Podman
container <a href="https://github.com/moby/buildkit#podman">as a
BuildKit builder</a>.</p>
<p>I don’t have personal experience with Podman, so I can’t comment on
how well it works.</p>
<p>In practice you can still install normal Docker; 19.03 had some
issues with the latest Fedora releases, but Docker 20.10 has fixed
those.</p>
<h3 id="references">References</h3>
<ul>
<li><a
href="https://hub.docker.com/r/docker/dockerfile/"><code>docker/dockerfile</code>
reference</a> (hub.docker.com)</li>
<li><a
href="https://docs.docker.com/develop/develop-images/build_enhancements/">BuildKit
features overview (outdated)</a> (docs.docker.com)</li>
<li><a href="https://podman.io/">Podman</a> (podman.io)</li>
<li><a href="https://github.com/moby/buildkit">BuildKit repository</a>
(github.com)</li>
<li><a
href="https://www.docker.com/blog/faster-builds-in-compose-thanks-to-buildkit-support/">BuildKit
in Compose</a> (docker.com)</li>
</ul>
<h1 id="best-practices-security">Best practices: Security</h1>
<h2 id="no-root">Don’t run as root</h2>
<p>Running your Docker image as root exposes you to significant security
risks: it makes it much easier for an attacker to get root on the host.
Better, then, to start your container as a non-root user:</p>
<pre class="dockerfile"><code>FROM python:3.9-slim-bullseye
RUN useradd --create-home appuser
WORKDIR /home/appuser
USER appuser
ENTRYPOINT [&quot;yourprogram&quot;]</code></pre>
<p>Note that <code>USER</code> only affects commands after it is used,
so you can still run commands as root beforehand:</p>
<pre class="dockerfile"><code>RUN useradd --create-home appuser
WORKDIR /home/appuser
# This runs as root:
RUN chmod 777 /var/log
USER appuser
# This runs as appuser:
RUN pip install yourpackage
# And this runs as appuser:
ENTRYPOINT [&quot;python&quot;, &quot;-m&quot;, &quot;yourcode&quot;]</code></pre>
<p>You can use tools like <code>gosu</code> to change away from root
after startup, but that will prevent you from running the container with
reduced capabilities (see below), which is another useful security
measure. Better then to use <code>USER</code>.</p>
<h3 id="references-1">References</h3>
<ul>
<li><a
href="https://pythonspeed.com/articles/root-capabilities-docker-security/">Less
capabilities, more security: minimizing privilege escalation in
Docker</a> (pythonspeed.com)</li>
<li><a
href="https://docs.docker.com/engine/reference/builder/#user"><code>Dockerfile</code>
<code>USER</code> command</a> (docs.docker.com)</li>
<li><a
href="https://www.man7.org/linux/man-pages/man8/useradd.8.html"><code>useradd(8)</code>
man page</a> (www.man7.org)</li>
</ul>
<h2 id="port-1024">Don’t listen on ports &lt; 1024</h2>
<p>In order to listen on TCP or UDP ports less than 1024 on Linux, you
need to either run as root or have the relevant security capability (see
below). Since you don’t want to run as root, nor grant your container
unnecessary capabilities, you should always listen on port 1024 or
higher.</p>
<p>But what if you do need to listen on a low port? For example, an
HTTPS server needs to listen on port 443 if you want a standard
<code>https://</code> URL to work.</p>
<p>Remember that when you run your container you are doing
port-forwarding to expose the service, and the external port on the host
doesn’t have to match the port inside the container. So you can do:</p>
<pre><code>$ docker run -p 443:8443 yourcontainer</code></pre>
<p>And now port 443 on the host maps to port 8443 in the container.</p>
<p>Pretty much every system that can run a container has similar
options. There is therefore no reason to listen on ports &lt; 1024.</p>
<h2 id="no-capabilities">Run your container with no capabilities</h2>
<p>Linux “capabilities” allow you to grant a subset of root’s power to a
process. A process may have some capabilities currently in effect, and
some capabilities it can inherit when it executes an executable with the
ability to grant capabilities.</p>
<p>By default when running as non-root user in Docker, the process has
no effective capabilities, but it can inherit quite a few when it run an
executable. For example, here you can see that running
<code>/usr/bin/ping</code> granted the resulting process the ability to
turn on <code>cap_net_raw</code> capability:</p>
<div class="sourceCode" id="cb16"><pre
class="sourceCode bash"><code class="sourceCode bash"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a><span class="ex">$</span> docker run <span class="at">--user</span><span class="op">=</span>1000 <span class="at">-it</span> centos</span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a><span class="ex">bash-4.4$</span> getcap /usr/bin/ping</span>
<span id="cb16-3"><a href="#cb16-3" aria-hidden="true" tabindex="-1"></a><span class="ex">/usr/bin/ping</span> = cap_net_admin,cap_net_raw+p</span>
<span id="cb16-4"><a href="#cb16-4" aria-hidden="true" tabindex="-1"></a><span class="ex">bash-4.4$</span> ping example.com <span class="op">&gt;</span> /dev/null <span class="kw">&amp;</span></span>
<span id="cb16-5"><a href="#cb16-5" aria-hidden="true" tabindex="-1"></a><span class="ex">[1]</span> 13</span>
<span id="cb16-6"><a href="#cb16-6" aria-hidden="true" tabindex="-1"></a><span class="ex">bash-4.4$</span> getpcaps 13</span>
<span id="cb16-7"><a href="#cb16-7" aria-hidden="true" tabindex="-1"></a><span class="ex">Capabilities</span> for <span class="kw">`</span><span class="ex">13</span><span class="st">&#39;: = cap_net_raw+p</span></span></code></pre></div>
<p>If <code>ping</code> has a security vulnerability that allows
injecting code into it, an attacker now has access to escalated
privileges. You can solve this by running the container with no
capabilities:</p>
<div class="sourceCode" id="cb17"><pre
class="sourceCode bash"><code class="sourceCode bash"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a><span class="ex">$</span> docker run <span class="at">--user</span><span class="op">=</span>1000 <span class="at">--cap-drop</span><span class="op">=</span>ALL <span class="at">-it</span> centos</span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a><span class="ex">bash-4.4$</span> ping example.com</span>
<span id="cb17-3"><a href="#cb17-3" aria-hidden="true" tabindex="-1"></a><span class="ex">ping:</span> socket: Operation not permitted</span></code></pre></div>
<p>Kubernetes has similar options.</p>
<p>Technically this best practice is about running your container, not
about packaging. But as mentioned in the previous two best practices it
does have an impact on how you should package your application.</p>
<h3 id="references-2">References</h3>
<ul>
<li><a
href="https://www.man7.org/linux/man-pages/man7/capabilities.7.html"><code>capabilities(7)</code>
man page</a> (www.man7.org)</li>
<li><a
href="https://docs.docker.com/engine/reference/run/#runtime-privilege-and-linux-capabilities"><code>docker
run</code> capabilities options</a> (docs.docker.com)</li>
<li><a
href="https://kubernetes.io/docs/tasks/configure-pod-container/security-context/">Kubernetes
Pod security context</a> (kubernetes.io)</li>
</ul>
<h2 id="update-system-packages">Update system packages</h2>
<p>The system packages in the base image you are using may or may not be
up-to-date, which means they might not include the latest security
updates. You should therefore update them at the start of your
build.</p>
<pre class="dockerfile"><code>FROM python:3.9-slim-bullseye
RUN apt-get update &amp;&amp; apt-get -y upgrade</code></pre>
<p>There are more fully fleshed-out scripts in <a
href="#system-packages-clean">a later handbook item</a>.</p>
<h2 id="weekly-rebuild">∞ Rebuild without caching and redeploy at least
weekly ∞</h2>
<p>Even if your code hasn’t changed, you still need to rebuild your
image and redeploy it on a regular basis.</p>
<ol type="1">
<li>The system packages you depend on will have new releases, due to
security flaws and other bugs.</li>
<li>The Python base image may also have a new point release with
security fixes.</li>
</ol>
<p>For example, if your image uses <code>nginx</code> and
<code>nginx</code> has a remote execution attack, you don’t want to wait
until a new release of your code to deploy a fixed
<code>nginx</code>.</p>
<p>You should therefore rebuild your images once a week, and then
redeploy them.</p>
<p>You also need to make sure these rebuilds actually get the updates.
Remember that Docker build caching will skip updating a layer if the
command hasn’t changed. That means that if you have a cached version,
<code>apt-get upgrade</code> won’t run even if there are new
packages!</p>
<p>You should therefore rebuild with <code>--no-cache</code>, so all
layers are rebuilt, and with <code>--pull</code> so that you’re using
the latest base image:</p>
<pre><code>$ docker build -t example.com/myorg/myimage --no-cache --pull .
$ docker push example.com/myorg/myimage</code></pre>
<h3 id="references-3">References</h3>
<ul>
<li><a
href="https://docs.docker.com/engine/reference/commandline/build/"><code>docker
build</code> reference</a> (docs.docker.com)</li>
<li><a
href="https://pythonspeed.com/articles/docker-cache-insecure-images/">Avoiding
insecure images from Docker build caching</a> (pythonspeed.com)</li>
</ul>
<h2 id="apply-security-fixes">∞ Apply security fixes when they come out
∞</h2>
<p>Over time, software packages you depend on will get security fixes as
well as fixes to critical bugs. Insofar as you’ve pinned your packages,
you will need to apply these fixes yourself. And you want to get
notified when new packages are released with these fixes, so that you
know an update is needed.</p>
<p>GitHub has scanning functionality built-in for Pip, Pipenv, and
Poetry; see the documentation linked in references. There are also other
services like <a href="https://requires.io/">requires.io</a>, <a
href="https://pyup.io">PyUp</a>, and more.</p>
<p>These tools will notify you of Python vulnerabilities and updates,
but will not track pinned system packages; you’ll need to track those
yourself. If you’re using a Debian-based base image, for example, you
can track the Debian security releases.</p>
<p>Once you’ve realized you have a security vulnerability, either via a
notification or via a security scanner (see the next best practice), you
will need to:</p>
<ol type="1">
<li>Update your package dependencies with the fixed version, if you’re
pinning versions.</li>
<li>Rebuild the image.</li>
<li>Redeploy the image anywhere it’s currently running.</li>
</ol>
<h3 id="references-4">References</h3>
<ul>
<li><a href="https://www.debian.org/security/">Debian security
information</a> (debian.org)</li>
<li><a href="https://usn.ubuntu.com/">Ubuntu security notices</a>
(usn.ubuntu.com)</li>
<li><a
href="https://access.redhat.com/security/security-updates/#/security-advisories">RHEL
security advisories</a> (access.redhat.com)</li>
<li><a
href="https://docs.github.com/en/code-security/supply-chain-security/keeping-your-dependencies-updated-automatically">GitHub
dependency auto-updates</a> (github.com)</li>
<li><a
href="https://docs.github.com/en/code-security/supply-chain-security/managing-vulnerabilities-in-your-projects-dependencies">GitHub
dependency security vulnerability alerts</a>(github.com)</li>
</ul>
<h2 id="security-scanners">Run security scanners</h2>
<p>In your CI system you can run security scanners to scan:</p>
<ol type="1">
<li>Your code for potential security bugs.</li>
<li>Your dependencies for known vulnerabilities.</li>
<li>System packages and packages in other languages for known
vulnerabilities.</li>
</ol>
<h3 id="potential-vulnerabilities-in-your-python-code">Potential
vulnerabilities in your Python code</h3>
<p><code>bandit</code> is a tool for finding vulnerabilities in your
Python code; it will search for things like SQL injection attacks, use
of pickle, and many more. You might want to run it via
<code>flake8-bandit</code>, so you get the benefit of
<code>flake8</code>’s generic skipping and configuration mechanism.</p>
<div class="sourceCode" id="cb20"><pre
class="sourceCode bash"><code class="sourceCode bash"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a><span class="ex">$</span> python3 <span class="at">-m</span> venv /tmp/security</span>
<span id="cb20-2"><a href="#cb20-2" aria-hidden="true" tabindex="-1"></a><span class="ex">$</span> . /tmp/security/bin/activate</span>
<span id="cb20-3"><a href="#cb20-3" aria-hidden="true" tabindex="-1"></a><span class="ex">$</span> pip install bandit</span>
<span id="cb20-4"><a href="#cb20-4" aria-hidden="true" tabindex="-1"></a><span class="ex">$</span> bandit example.py</span>
<span id="cb20-5"><a href="#cb20-5" aria-hidden="true" tabindex="-1"></a><span class="ex">...</span></span>
<span id="cb20-6"><a href="#cb20-6" aria-hidden="true" tabindex="-1"></a><span class="op">&gt;&gt;</span> Issue: <span class="ex">[B403:blacklist]</span> Consider possible security implications associated with pickle module.</span>
<span id="cb20-7"><a href="#cb20-7" aria-hidden="true" tabindex="-1"></a><span class="ex">...</span></span></code></pre></div>
<h3
id="known-vulnerabilities-in-python-dependencies-system-packages-and-other-languages">Known
vulnerabilities in Python dependencies, system packages, and other
languages</h3>
<p><a href="https://github.com/aquasecurity/trivy">Trivy</a> is a
command-line tool that lets you scan a Docker image, the filesystem, or
a remote repository, for many kinds of security vulnerabilities, both
system packages (RHEL, Debian, Ubuntu, and more) and programming
language-specific packages. <code>trivy</code> also supports checking
<code>requirements.txt</code>, Poetry, and Pipenv dependency files.</p>
<p>Trivy reports <em>known</em> vulnerabilities that someone already
reported. That’s different than Bandit, which looks for
<em>possible</em> vulnerabilities–you could in theory also run Bandit
against dependencies and find some real issues that no one knew
existed.</p>
<blockquote>
<p><strong>Note:</strong> <code>trivy</code> has some data sources it
uses that are licensed for non-commercial use only. This problem should
be fixed as of v0.23 (at the time of writing, January 2022, the latest
release is v0.22). See the <a
href="https://aquasecurity.github.io/trivy/v0.22.0/vulnerability/detection/data-source/">Trivy
data sources</a> page for a list of which are OK to use.</p>
<p>Many hosted image registries will also do scans for you. You should
always check what a particular scanner supports; not all of them will
necessarily check Python dependencies.</p>
</blockquote>
<p>By default <code>trivy</code> will show vulnerabilities that don’t
have any fixes available, including many vulnerabilities that will
<em>never</em> have any fixes available. You may wish to ignore these
(if there’s no fix, what can you do?) by using the
<code>--ignore-unfixed</code> option. Note that this is only supported
for some operating systems.</p>
<p>Additionally, by default <code>trivy</code> won’t set a non-zero exit
code when vulnerabilities are found. If you’re using Trivy as part of CI
or other automated script, you’ll want to use the
<code>--exit-code</code> option to make sure found vulnerabilities are
noted as a failure, e.g. <code>--exit-code 1</code>.</p>
<p>To scan the image <code>yourorg/yourimage</code>, you can do:</p>
<div class="sourceCode" id="cb21"><pre
class="sourceCode bash"><code class="sourceCode bash"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a><span class="ex">$</span> trivy image <span class="at">--ignore-unfixed</span> <span class="at">--exit-code</span> 1 yourorg/yourimage</span></code></pre></div>
<p>To scan a local directory, you can do:</p>
<div class="sourceCode" id="cb22"><pre
class="sourceCode bash"><code class="sourceCode bash"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a><span class="ex">$</span> trivy fs <span class="at">--ignore-unfixed</span> <span class="at">--exit-code</span> 1 ./path/to/code/</span></code></pre></div>
<p>The Trivy documentation explains how to use it with many CI systems
(GitHub Actions, Circle CI, GitLab CI, and more); see the “Advanced”
section of the docs.</p>
<p>An alternative Trivy is the Grype security scanner.</p>
<h3 id="references-5">References</h3>
<ul>
<li><a href="https://bandit.readthedocs.io">Bandit documentation</a>
(bandit.readthedocs.io)</li>
<li><a href="https://pypi.org/project/flake8-bandit/">flake8-bandit</a>
(pypi.org)</li>
<li><a href="https://aquasecurity.github.io/trivy/">Trivy
documentation</a> (aquasecurity.github.io)</li>
<li><a href="https://github.com/anchore/grype">Grype security
scanner</a> (github.com)</li>
</ul>
<h2 id="secret-files">Don’t leak secret files</h2>
<p>Imagine you have the following files in your build directory:</p>
<pre><code>Dockerfile
app/
  __init__.py
  code.py
run.sh
id_rsa</code></pre>
<p>The <code>id_rsa</code> file is a private SSH key used to access a
private repository–you do not want it to end up in the image. If it
does, any attacker who gains access to the image will be to extract this
secret file.</p>
<p>If you do the following:</p>
<div class="sourceCode" id="cb24"><pre
class="sourceCode dockerfile"><code class="sourceCode dockerfile"><span id="cb24-1"><a href="#cb24-1" aria-hidden="true" tabindex="-1"></a><span class="kw">COPY</span> . .  <span class="co"># INSECURE</span></span></code></pre></div>
<p>Then by default <code>id_rsa</code> will get copied in to the
image.</p>
<p>You can avoid this by explicitly copying in only the files you
need:</p>
<div class="sourceCode" id="cb25"><pre
class="sourceCode dockerfile"><code class="sourceCode dockerfile"><span id="cb25-1"><a href="#cb25-1" aria-hidden="true" tabindex="-1"></a><span class="kw">COPY</span> app/ run.sh ./</span></code></pre></div>
<p>You can also add <code>id_rsa</code> to <a
href="#dockerignore"><code>.dockerignore</code></a> so it doesn’t get
copied in.</p>
<p>Finally, you can avoid having secrets in the same directory as your
Docker build context.</p>
<h2 id="runtime-secrets">Don’t leak runtime secrets</h2>
<p>Your Docker image may require a variety of secrets to run correctly.
For example, your web application might require the password to its
MySQL database. These are known as runtime secrets, and you should not
store them in your image, since an attacker who gains access to the
image will be able to read them.</p>
<p>There are a variety of runtime-specific mechanisms for passing
secrets into a running container:</p>
<ul>
<li>Using environment variables passed in at runtime, using
e.g. <code>docker run --env/--env-file</code>, Docker Compose
<code>environment/env_file</code> keys, or the Kubernetes
equivalents.</li>
<li>Mounting a volume or host directory with the secret.</li>
<li>Secret-specific mechanisms like Kubernetes secrets (which actually
uses both mechanisms above).</li>
<li>In some cloud environments, you can give permissions to containers
to talk to the cloud environment, e.g. IAM roles for ECS tasks on
AWS.</li>
<li>Retrieving them from some sort of external key store like
HashiCorp’s Vault.</li>
</ul>
<blockquote>
<p>Note that “volume” is used inconsistently in Docker. Sometimes it
means “a mini-filesystem managed by Docker”, or if you’re using
Kubernetes a “mini-filesystem managed by Kubernetes or a system it talks
to.” Sometimes it is used more generically as the configuration option
to mount either volumes in the previous sense or bind mount directories
from the host system.</p>
<p>Either way, the key point is that the secret should not be in the
image itself.</p>
</blockquote>
<h3 id="references-6">References</h3>
<ul>
<li><a
href="https://docs.docker.com/engine/reference/commandline/run/#set-environment-variables--e---env---env-file">Setting
Docker environment variables at runtime</a> (docs.docker.com)</li>
<li><a href="https://docs.docker.com/storage/bind-mounts/">Bind
mounting</a> (docs.docker.com)</li>
<li><a
href="https://kubernetes.io/docs/concepts/configuration/secret/">Kubernetes
secrets</a> (kubernetes.io)</li>
<li><a
href="https://docs.aws.amazon.com/AmazonECS/latest/developerguide/instance_IAM_role.html">IAM
in ECS</a> (docs.aws.amazon.com)</li>
<li><a href="https://www.vaultproject.io/">HashiCorp Vault</a>
(vaultproject.io)</li>
</ul>
<h2 id="build-secrets">Don’t leak build secrets</h2>
<p>Sometimes during a build you need to download source code or packages
using a secret: a password, or an SSH private key. This is not the same
as a <em>runtime</em> secret. Rather, it’s a secret you will only need
during the Docker image build, to download some build dependencies.</p>
<p>The obvious mechanism to pass secrets in is <code>docker build
--build-arg</code>, but this is insecure: the secret will be embedded in
the image, so anyone who has access to the image can see the secret by
running <code>docker history</code>. You can also <code>COPY</code> in
secrets, but then they will be leaked via the images.</p>
<p>To solve this you have a number of alternatives:</p>
<ol type="1">
<li>Use a short-term key or access token that will expire after a few
minutes.</li>
<li>Pre-download the necessary packages or source code outside of the
Docker build, then <code>COPY</code> the files in as normal. In this
case you don’t need to pass the secret in to the Docker build at
all.</li>
<li>Use the new BuildKit backend for Docker builds, which supports
passing in secrets as well as SSH authentication-agent forwarding.</li>
<li>Pass in the secrets over the network.</li>
</ol>
<p>For this last method of passing in secrets, via the network, see the
references below. Given that BuildKit support is now stable, there is no
reason to use the network variant, except perhaps with RedHat’s
alternative Docker implementation, Podman.</p>
<h3 id="a-short-term-expiring-access-token">A short term expiring access
token</h3>
<p>Some package repositories support the creation of short term access
tokens. If your access token expires after 5 minutes, it doesn’t matter
if you leak it in your image so long as the push happens more than 5
minutes after token creation. You can then use build args or
<code>COPY</code> to get the secret in.</p>
<h3 id="pre-download-necessary-files">Pre-download necessary files</h3>
<p>Here’s how this option would work if, for example, you need a private
SSH key to download some code:</p>
<div class="sourceCode" id="cb26"><pre
class="sourceCode bash"><code class="sourceCode bash"><span id="cb26-1"><a href="#cb26-1" aria-hidden="true" tabindex="-1"></a><span class="ex">$</span> eval <span class="va">$(</span><span class="fu">ssh-agent</span><span class="va">)</span></span>
<span id="cb26-2"><a href="#cb26-2" aria-hidden="true" tabindex="-1"></a><span class="ex">$</span> ssh-add ~/.ssh/id_rsa</span>
<span id="cb26-3"><a href="#cb26-3" aria-hidden="true" tabindex="-1"></a><span class="ex">$</span> git clone git@github.com:yourorg/yourprivatecode.git</span>
<span id="cb26-4"><a href="#cb26-4" aria-hidden="true" tabindex="-1"></a><span class="ex">$</span> docker build <span class="at">-t</span> yourimage .</span></code></pre></div>
<p>And the <code>Dockerfile</code> would just need to copy in the
code:</p>
<div class="sourceCode" id="cb27"><pre
class="sourceCode dockerfile"><code class="sourceCode dockerfile"><span id="cb27-1"><a href="#cb27-1" aria-hidden="true" tabindex="-1"></a><span class="kw">FROM</span> python:3.9-slim-bullseye</span>
<span id="cb27-2"><a href="#cb27-2" aria-hidden="true" tabindex="-1"></a><span class="kw">COPY</span> yourprivatecode .</span>
<span id="cb27-3"><a href="#cb27-3" aria-hidden="true" tabindex="-1"></a><span class="co"># RUN the build, etc.</span></span></code></pre></div>
<h3 id="buildkit-secrets">BuildKit secrets</h3>
<p>Here’s how you would use BuildKit. Let’s say you have a file with a
secret:</p>
<pre><code>$ cat secret-file
THIS IS SECRET</code></pre>
<p>First, configure your <code>Dockerfile</code> to use BuildKit, and
add a flag to <code>RUN</code> telling it to expose a particular
secret:</p>
<pre class="dockerfile"><code># syntax = docker/dockerfile:1.3
FROM python:3.9-slim-bullseye
COPY build-script.sh .
RUN --mount=type=secret,id=mysecret ./build-script.sh</code></pre>
<p>The <code>build-script.sh</code> will be able to find the secret at
<code>/run/secrets/mysecret</code>.</p>
<p>Then, to build your image with the secret set the appropriate
environment variable and pass in the newly enabled command-line
arguments:</p>
<pre><code>$ export DOCKER_BUILDKIT=1
$ docker build --secret id=mysecret,src=secret-file .</code></pre>
<p>Docker 20.10 adds the additional ability to load secrets from
environment variables, not just files. For example, if you have an
environment variable <code>MYSECRET</code>, you can access it like
this:</p>
<pre><code>$ export MYSECRET=theverysecretpassword
$ export DOCKER_BUILDKIT=1
$ docker build --secret --secret id=mysecret,env=MYSECRET .</code></pre>
<p>If you’re OK with the secret ID being the same as the name as the
environment variable, you can replace the last command with:</p>
<pre><code>$ docker build --secret --secret id=MYSECRET .</code></pre>
<p>Note that it will still be exposed inside the build as a file in
<code>/run/secrets</code>, it is merely read from an environment
variable on the host.</p>
<h3 id="references-7">References</h3>
<ul>
<li><a
href="https://docs.docker.com/develop/develop-images/build_enhancements/">Docker
BuildKit backend documentation</a> (docs.docker.com)</li>
<li><a
href="https://pythonspeed.com/articles/docker-build-secrets/">Docker
build secrets, via the network</a> (pythonspeed.com)</li>
</ul>
<h2 id="compose-secrets">Using BuildKit secrets from Docker Compose</h2>
<p>Docker Compose does not yet support using BuildKit secrets. Until
that is supported (see the GitHub issue in the references) it is
possible to use a workaround.</p>
<p>The basic idea is to have code that supports either secrets file or
environment variables (via build args). When using <code>docker
build</code> to build the released image, you use the secure secrets
file; when building locally in Docker Compose, you use the insecure
build argument.</p>
<p>Let’s say you have a script <code>use_secret.sh</code> that wants to
download a file using a password. You’d write it like this:</p>
<div class="sourceCode" id="cb33"><pre
class="sourceCode bash"><code class="sourceCode bash"><span id="cb33-1"><a href="#cb33-1" aria-hidden="true" tabindex="-1"></a><span class="co">#!/bin/bash</span></span>
<span id="cb33-2"><a href="#cb33-2" aria-hidden="true" tabindex="-1"></a><span class="bu">set</span> <span class="at">-euo</span> pipefail</span>
<span id="cb33-3"><a href="#cb33-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-4"><a href="#cb33-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Support both secrets file and an env variable:</span></span>
<span id="cb33-5"><a href="#cb33-5" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> <span class="bu">[</span> <span class="ot">-f</span> /run/secrets/thepassword <span class="bu">]</span><span class="kw">;</span> <span class="cf">then</span></span>
<span id="cb33-6"><a href="#cb33-6" aria-hidden="true" tabindex="-1"></a>   <span class="bu">export</span> <span class="va">THEPASSWORD</span><span class="op">=</span><span class="va">$(</span><span class="fu">cat</span> /run/secrets/thepassword<span class="va">)</span></span>
<span id="cb33-7"><a href="#cb33-7" aria-hidden="true" tabindex="-1"></a><span class="cf">fi</span></span>
<span id="cb33-8"><a href="#cb33-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-9"><a href="#cb33-9" aria-hidden="true" tabindex="-1"></a><span class="fu">wget</span> <span class="at">-o</span> download.zip https://admin:<span class="va">$THEPASSWORD</span>@example.com/download.zip</span></code></pre></div>
<p>The <code>Dockerfile</code> would look like this:</p>
<pre class="dockerfile"><code># syntax = docker/dockerfile:1.3
FROM python:3.9-slim-bullseye
# Only use the build arg for local development:
ARG THEPASSWORD
COPY use_secret.sh .
# Mount the secret to /run/secrets:
RUN --mount=type=secret,id=thepassword ./use_secret.sh</code></pre>
<p>When you did a normal build, you would pass the secret in using a
secure mechanism:</p>
<div class="sourceCode" id="cb35"><pre
class="sourceCode bash"><code class="sourceCode bash"><span id="cb35-1"><a href="#cb35-1" aria-hidden="true" tabindex="-1"></a><span class="co">#!/bin/bash</span></span>
<span id="cb35-2"><a href="#cb35-2" aria-hidden="true" tabindex="-1"></a><span class="bu">set</span> <span class="at">-euo</span> pipefail</span>
<span id="cb35-3"><a href="#cb35-3" aria-hidden="true" tabindex="-1"></a><span class="bu">export</span> <span class="va">DOCKER_BUILDKIT</span><span class="op">=</span>1</span>
<span id="cb35-4"><a href="#cb35-4" aria-hidden="true" tabindex="-1"></a><span class="ex">docker</span> build <span class="at">-t</span> myimage <span class="dt">\</span></span>
<span id="cb35-5"><a href="#cb35-5" aria-hidden="true" tabindex="-1"></a>    <span class="at">--secret</span> id=thepassword,src=mypassword.txt .</span></code></pre></div>
<p>Your Compose file in contrast would use build arguments:</p>
<div class="sourceCode" id="cb36"><pre
class="sourceCode yaml"><code class="sourceCode yaml"><span id="cb36-1"><a href="#cb36-1" aria-hidden="true" tabindex="-1"></a><span class="fu">version</span><span class="kw">:</span><span class="at"> </span><span class="st">&quot;3.7&quot;</span></span>
<span id="cb36-2"><a href="#cb36-2" aria-hidden="true" tabindex="-1"></a><span class="fu">services</span><span class="kw">:</span></span>
<span id="cb36-3"><a href="#cb36-3" aria-hidden="true" tabindex="-1"></a><span class="at">  </span><span class="fu">yourapp</span><span class="kw">:</span></span>
<span id="cb36-4"><a href="#cb36-4" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="fu">build</span><span class="kw">:</span></span>
<span id="cb36-5"><a href="#cb36-5" aria-hidden="true" tabindex="-1"></a><span class="at">      </span><span class="fu">context</span><span class="kw">:</span><span class="at"> </span><span class="st">&quot;.&quot;</span></span>
<span id="cb36-6"><a href="#cb36-6" aria-hidden="true" tabindex="-1"></a><span class="at">      </span><span class="fu">args</span><span class="kw">:</span></span>
<span id="cb36-7"><a href="#cb36-7" aria-hidden="true" tabindex="-1"></a><span class="at">        </span><span class="fu">THEPASSWORD</span><span class="kw">:</span><span class="at"> </span><span class="st">&quot;$THEPASSWORD&quot;</span></span></code></pre></div>
<p>And to use it you set <code>$THEPASSWORD</code> and the environment
variables needed to make Compose use BuildKit:</p>
<pre><code>$ export THEPASSWORD=$(cat mypassword.txt)
$ export DOCKER_BUILDKIT=1
$ export COMPOSE_DOCKER_CLI_BUILD=1
$ docker-compose up</code></pre>
<h3 id="references-8">References</h3>
<ul>
<li><a
href="https://pythonspeed.com/articles/build-secrets-docker-compose/">Build
secrets in Docker Compose, the secure way</a> (pythonspeed.com)</li>
<li><a
href="https://www.docker.com/blog/faster-builds-in-compose-thanks-to-buildkit-support/">BuildKit
support in Docker Compose 1.25.1+</a> (docker.com)</li>
<li><a href="https://github.com/docker/compose/issues/6358">Issue #6358:
Support for BuildKit secrets in Docker Compose</a> (github.com)</li>
</ul>
<h2 id="use-host-ssh-keys-with-buildkit-ssh-agent-forwarding">Use host
SSH keys with BuildKit <code>ssh-agent</code> forwarding</h2>
<p>If you have an SSH private key on your host, perhaps
password-protected, and you want to make it available to multiple
processes without having to type that password each time, you can use
<code>ssh-agent</code>.</p>
<pre><code>$ eval `ssh-agent`
$ ssh-add ~/.ssh/id_rsa
Password: ******</code></pre>
<p>Now, all future <code>ssh</code> calls will be able to use that
private key without having to type in the password, since the agent has
it cached in memory.</p>
<p>If you’re using BuildKit, you can give the Docker build access to the
private SSH keys on the host by talking the <code>ssh-agent</code>
running on the host, and without leaking them into the image.</p>
<p>Your <code>Dockerfile</code> will look like this:</p>
<pre class="dockerfile"><code># syntax = docker/dockerfile:1.3
FROM python:3.9-slim-bullseye

RUN apt-get update &amp;&amp; \
    apt-get -y install --no-install-recommends openssh-client git

RUN --mount=type=ssh git clone git@github.com:yourorg/private.git</code></pre>
<p>And then you’d build it like this:</p>
<div class="sourceCode" id="cb40"><pre
class="sourceCode bash"><code class="sourceCode bash"><span id="cb40-1"><a href="#cb40-1" aria-hidden="true" tabindex="-1"></a><span class="co">#!/bin/bash</span></span>
<span id="cb40-2"><a href="#cb40-2" aria-hidden="true" tabindex="-1"></a><span class="bu">set</span> <span class="at">-euo</span> pipefail</span>
<span id="cb40-3"><a href="#cb40-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-4"><a href="#cb40-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Enable the ssh-agent:</span></span>
<span id="cb40-5"><a href="#cb40-5" aria-hidden="true" tabindex="-1"></a><span class="bu">eval</span> <span class="kw">`</span><span class="fu">ssh-agent</span><span class="kw">`</span></span>
<span id="cb40-6"><a href="#cb40-6" aria-hidden="true" tabindex="-1"></a><span class="fu">ssh-add</span></span>
<span id="cb40-7"><a href="#cb40-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-8"><a href="#cb40-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Build with BuildKit and ssh-agent forwarding:</span></span>
<span id="cb40-9"><a href="#cb40-9" aria-hidden="true" tabindex="-1"></a><span class="bu">export</span> <span class="va">DOCKER_BUILDKIT</span><span class="op">=</span>1</span>
<span id="cb40-10"><a href="#cb40-10" aria-hidden="true" tabindex="-1"></a><span class="ex">docker</span> build <span class="at">--ssh</span> default <span class="at">-t</span> yourimage .</span></code></pre></div>
<p>Note that by default this will fail with an error about an unverified
host; see the next best practice for details about to fix this.</p>
<h3 id="references-9">References</h3>
<ul>
<li><a
href="https://docs.docker.com/develop/develop-images/build_enhancements/#using-ssh-to-access-private-data-in-builds">Using
SSH with BuildKit</a> (docs.docker.com)</li>
<li><a
href="https://man7.org/linux/man-pages/man1/ssh-add.1.html"><code>ssh-add</code>
man page</a> (man7.org)</li>
<li><a
href="https://man7.org/linux/man-pages/man1/ssh-agent.1.html"><code>ssh-agent</code>
man page</a> (man7.org)</li>
</ul>
<h2 id="optional-pre-populate-ssh-known-hosts">Optional: Pre-populate
SSH known hosts</h2>
<p>If you are using SSH within your Docker build, SSHing to a new host
will ask you to verify the host’s key, which will fail because Docker
builds don’t allow interactive input. You can disable checking the key
with the <code>StrictHostKeyChecking=no</code> option, but this puts you
at risk of man-in-the-middle attacks.</p>
<p>Better to copy in a <code>.ssh/known_hosts</code> prepopulated with
the public key of the SSH host you will be accessing. You can download a
key as follows:</p>
<div class="sourceCode" id="cb41"><pre
class="sourceCode bash"><code class="sourceCode bash"><span id="cb41-1"><a href="#cb41-1" aria-hidden="true" tabindex="-1"></a><span class="ex">$</span> ssh-keyscan <span class="at">-t</span> rsa ssh-host.example.com <span class="op">&gt;</span> ssh_known_hosts</span></code></pre></div>
<p>If you’re worried about attackers, compare it to the value in your
existing <code>.ssh/known_hosts</code>. Some services like GitHub also
post their SSH public key fingerprint, so you can check the value
out-of-band.</p>
<p>Then in your <code>Dockerfile</code> you can copy in the
<code>known_hosts</code> file:</p>
<pre class="dockerfile"><code>RUN mkdir /root/.ssh &amp;&amp; chmod 700 /root/.ssh
COPY ssh_known_hosts /root/.ssh/known_hosts
RUN chmod 600 /root/.ssh/known_hosts</code></pre>
<h3 id="references-10">References</h3>
<ul>
<li><a
href="https://serverfault.com/questions/132970/can-i-automatically-add-a-new-host-to-known-hosts/132973#132973">Automatically
add new host to SSH known_hosts</a> (serverfault.com)</li>
<li><a href="https://www.openssh.com/manual.html">OpenSSH
documentation</a> (openssh.com)</li>
<li><a
href="https://help.github.com/en/github/authenticating-to-github/githubs-ssh-key-fingerprints">GitHub
SSH public key fingerprints</a> (help.github.com)</li>
</ul>
<h1 id="best-practices-running-in-ci">Best practices: Running in CI</h1>
<h2 id="smoke-test">Add a smoke test for your image</h2>
<p>Your application might have bugs, and you’ll have unit tests and
end-to-end tests to catch those. But you might also have issues with
your Docker image: it might not start at all, for example.</p>
<p>So before you push your newly built image to the image registry, you
should implement a smoke test. For example, if your image is a web
server, you can run the newly built image and make sure you can send a
successful HTTP query to the status endpoint:</p>
<div class="sourceCode" id="cb43"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb43-1"><a href="#cb43-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> time</span>
<span id="cb43-2"><a href="#cb43-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> subprocess <span class="im">import</span> check_call</span>
<span id="cb43-3"><a href="#cb43-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> urllib.request <span class="im">import</span> urlopen</span>
<span id="cb43-4"><a href="#cb43-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb43-5"><a href="#cb43-5" aria-hidden="true" tabindex="-1"></a>check_call(</span>
<span id="cb43-6"><a href="#cb43-6" aria-hidden="true" tabindex="-1"></a>    <span class="st">&quot;docker run --rm --name=mycontainer -p 8080:80 -d httpd&quot;</span>.split()</span>
<span id="cb43-7"><a href="#cb43-7" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb43-8"><a href="#cb43-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Wait for the server to start. A better implementation would</span></span>
<span id="cb43-9"><a href="#cb43-9" aria-hidden="true" tabindex="-1"></a><span class="co"># poll in a loop:</span></span>
<span id="cb43-10"><a href="#cb43-10" aria-hidden="true" tabindex="-1"></a>time.sleep(<span class="dv">5</span>)</span>
<span id="cb43-11"><a href="#cb43-11" aria-hidden="true" tabindex="-1"></a><span class="co"># Check if the server started (it&#39;ll throw an exception if not):</span></span>
<span id="cb43-12"><a href="#cb43-12" aria-hidden="true" tabindex="-1"></a><span class="cf">try</span>:</span>
<span id="cb43-13"><a href="#cb43-13" aria-hidden="true" tabindex="-1"></a>    urlopen(<span class="st">&quot;http://localhost:8080&quot;</span>).read()</span>
<span id="cb43-14"><a href="#cb43-14" aria-hidden="true" tabindex="-1"></a><span class="cf">finally</span>:</span>
<span id="cb43-15"><a href="#cb43-15" aria-hidden="true" tabindex="-1"></a>    check_call(<span class="st">&quot;docker kill mycontainer&quot;</span>.split())</span></code></pre></div>
<p>The smoke test won’t catch all problems, but it will ensure you don’t
push a completely broken image.</p>
<p>Notice the use of <code>--rm</code>, to ensure you’re not leaking
containers. In a build system that spins up an empty environment each
time this won’t matter, but in a persistent setup you don’t want to leak
resources.</p>
<h3 id="references-11">References</h3>
<ul>
<li><a
href="https://pythonspeed.com/articles/test-your-docker-build/">Your
Docker build needs a smoke test</a> (pythonspeed.com)</li>
</ul>
<h2 id="additional-checks">Additional checks: <code>hadolint</code>,
size checks</h2>
<p>You can do additional checks on both your <code>Dockerfile</code> and
image.</p>
<p>First, the <code>hadolint</code> <code>Dockerfile</code> linter will
catch some problems, though very definitely not all. Additionally, some
of its recommendations are completely wrong; for example, it recommends
not using <code>apt-get upgrade</code>, which is a very bad
recommendation. So only use it if you’re prepared to override many of
its incorrect suggestions.</p>
<p>Second, it’s useful to check your image size isn’t any higher than a
particular value: if you expect images to be 300-400MB and they’re
suddenly 1GB, something has gone wrong. You can get the image size by
running:</p>
<div class="sourceCode" id="cb44"><pre
class="sourceCode bash"><code class="sourceCode bash"><span id="cb44-1"><a href="#cb44-1" aria-hidden="true" tabindex="-1"></a><span class="ex">$</span> docker image inspect <span class="at">--format</span><span class="op">=</span>{{.Size}} yourorg/yourimage</span>
<span id="cb44-2"><a href="#cb44-2" aria-hidden="true" tabindex="-1"></a><span class="ex">237117212</span></span></code></pre></div>
<p>If you’re current image is 250MB, check for 300MB; better to be a
little lenient so you don’t get spurious failures.</p>
<h3 id="references-12">References</h3>
<ul>
<li><a
href="https://github.com/hadolint/hadolint"><code>hadolint</code></a>
(github.com)</li>
<li><a
href="https://docs.docker.com/engine/reference/commandline/image_inspect/"><code>docker
image inspect</code> help</a> (docs.docker.com)</li>
</ul>
<h2 id="tag-branch">Tag images based on the version control branch</h2>
<p>You might have a CI system that automatically builds images from Git
branches:</p>
<ol type="1">
<li>A developer pushes to branch <code>mybranch</code>.</li>
<li>The CI/build system automatically builds a new image, and pushes it
to an image registry.</li>
</ol>
<p>You want to ensure that a developer working on a feature branch won’t
accidentally overwrite the stable image used in production. The simplest
solution is to have the CI script choose the image tag based on the
branch.</p>
<p>While you’re at it, it’s worth tagging based on Git commit so you can
also distinguish different images on the same branch:</p>
<div class="sourceCode" id="cb45"><pre
class="sourceCode bash"><code class="sourceCode bash"><span id="cb45-1"><a href="#cb45-1" aria-hidden="true" tabindex="-1"></a><span class="va">GIT_BRANCH</span><span class="op">=</span><span class="va">$(</span><span class="fu">git</span> rev-parse <span class="at">--abbrev-ref</span> HEAD<span class="va">)</span></span>
<span id="cb45-2"><a href="#cb45-2" aria-hidden="true" tabindex="-1"></a><span class="va">GIT_COMMIT</span><span class="op">=</span><span class="va">$(</span><span class="fu">git</span> rev-parse <span class="at">--short</span> HEAD<span class="va">)</span></span>
<span id="cb45-3"><a href="#cb45-3" aria-hidden="true" tabindex="-1"></a><span class="va">IMAGE_NAME</span><span class="op">=</span><span class="st">&quot;imageregistry.example.com/org/theapp&quot;</span></span>
<span id="cb45-4"><a href="#cb45-4" aria-hidden="true" tabindex="-1"></a><span class="ex">docker</span> build <span class="at">-t</span> <span class="st">&quot;</span><span class="va">$IMAGE_NAME</span><span class="st">:</span><span class="va">$GIT_BRANCH</span><span class="st">&quot;</span> <span class="dt">\</span></span>
<span id="cb45-5"><a href="#cb45-5" aria-hidden="true" tabindex="-1"></a>             <span class="at">-t</span> <span class="st">&quot;</span><span class="va">$IMAGE_NAME</span><span class="st">:</span><span class="va">$GIT_COMMIT</span><span class="st">&quot;</span> .</span>
<span id="cb45-6"><a href="#cb45-6" aria-hidden="true" tabindex="-1"></a><span class="ex">docker</span> push <span class="st">&quot;</span><span class="va">$IMAGE_NAME</span><span class="st">:</span><span class="va">$GIT_BRANCH</span><span class="st">&quot;</span></span>
<span id="cb45-7"><a href="#cb45-7" aria-hidden="true" tabindex="-1"></a><span class="ex">docker</span> push <span class="st">&quot;</span><span class="va">$IMAGE_NAME</span><span class="st">:</span><span class="va">$GIT_COMMIT</span><span class="st">&quot;</span></span></code></pre></div>
<h2 id="latest-tag">Don’t rely on the <code>latest</code> tag</h2>
<p>Using the <code>latest</code> tag as your main branch name is
problematic:</p>
<ul>
<li>Its name is confusing, it’s not actually the latest image. From
Docker’s perspective it’s just the default tag, it may well be an old
image.</li>
<li>It’s easy to overwrite by mistake if you’re doing manual pushes, by
forgetting to omit the tag.</li>
<li>If you’re using Kubernetes, it behaves differently for the
<code>latest</code> tag vs other tags, which can lead to confusing
behavior.</li>
</ul>
<p>If you want a default tag for production, better to choose some other
name.</p>
<h3 id="references-13">References</h3>
<ul>
<li><a href="https://vsupalov.com/docker-latest-tag/">What’s wrong with
the Docker <code>:latest</code> tag?</a> (vsupalov.com)</li>
<li><a
href="https://kubernetes.io/docs/concepts/configuration/overview/#container-images">Kubernetes
<code>imagePullPolicy</code> interaction with <code>latest</code></a>
(kubernetes.io)</li>
</ul>
<h2 id="pull-in-ci">Warm up the build cache</h2>
<p>If you’re building your Docker image in your CI/build setup (GitLab
CI, Jenkins, GitHub Actions, Azure Pipelines, etc.), in many cases each
build will start with an empty local image cache. So you’ll want to make
sure the previous version of the image is available locally so that it
can be used when rebuilding. Otherwise your build might not use cached
layers, and will always rebuild from scratch.</p>
<p>You’ll want to pull the image, and then use <code>--cache-from</code>
to make sure Docker uses it:</p>
<div class="sourceCode" id="cb46"><pre
class="sourceCode bash"><code class="sourceCode bash"><span id="cb46-1"><a href="#cb46-1" aria-hidden="true" tabindex="-1"></a><span class="co">#!/bin/bash</span></span>
<span id="cb46-2"><a href="#cb46-2" aria-hidden="true" tabindex="-1"></a><span class="bu">set</span> <span class="at">-euo</span> pipefail</span>
<span id="cb46-3"><a href="#cb46-3" aria-hidden="true" tabindex="-1"></a><span class="ex">docker</span> pull yourimage <span class="kw">||</span> <span class="fu">true</span></span>
<span id="cb46-4"><a href="#cb46-4" aria-hidden="true" tabindex="-1"></a><span class="ex">docker</span> build <span class="at">-t</span> yourimage <span class="dt">\</span></span>
<span id="cb46-5"><a href="#cb46-5" aria-hidden="true" tabindex="-1"></a>    <span class="at">--build-arg</span> BUILDKIT_INLINE_CACHE=1 <span class="dt">\</span></span>
<span id="cb46-6"><a href="#cb46-6" aria-hidden="true" tabindex="-1"></a>    <span class="at">--cache-from</span><span class="op">=</span>yourimage .</span>
<span id="cb46-7"><a href="#cb46-7" aria-hidden="true" tabindex="-1"></a><span class="ex">docker</span> push yourimage</span></code></pre></div>
<p>The <code>|| true</code> allows you to keep going even if the pull
fails; this will be the case the first time you create the image.</p>
<p>The <code>--build-arg BUILDKIT_INLINE_CACHE=1</code> is necessary for
this scheme to work with BuildKit. While this flag is not necessary in
classic Docker builds, you may as well always set it so you don’t forget
to enable it when you turn on BuildKit.</p>
<h3 id="references-14">References</h3>
<ul>
<li><a
href="https://pythonspeed.com/articles/speeding-up-docker-ci/">Speeding
up Docker builds in CI</a> (pythonspeed.com)</li>
</ul>
<h2 id="pull-in-ci-2">Warm up the build cache for per-branch builds</h2>
<p>If you’re building multiple branches in parallel, and tagging images
correspondingly, warming the cache won’t work the first time you build a
new branch, even though it’s identical to your main branch. That is, if
you do <code>docker pull yourimage:newbranch</code> and this is your
first build of <code>newbranch</code>, that base image won’t be
available. The solution is to pull and use <code>--cache-from</code> on
multiple tags, both the branch and your default build.</p>
<p>Presuming your default production build is tagged with
<code>production</code>:</p>
<div class="sourceCode" id="cb47"><pre
class="sourceCode bash"><code class="sourceCode bash"><span id="cb47-1"><a href="#cb47-1" aria-hidden="true" tabindex="-1"></a><span class="co">#!/bin/bash</span></span>
<span id="cb47-2"><a href="#cb47-2" aria-hidden="true" tabindex="-1"></a><span class="bu">set</span> <span class="at">-euo</span> pipefail</span>
<span id="cb47-3"><a href="#cb47-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-4"><a href="#cb47-4" aria-hidden="true" tabindex="-1"></a><span class="va">BRANCH</span><span class="op">=</span><span class="va">$(</span><span class="fu">git</span> rev-parse <span class="at">--abbrev-ref</span> HEAD<span class="va">)</span></span>
<span id="cb47-5"><a href="#cb47-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-6"><a href="#cb47-6" aria-hidden="true" tabindex="-1"></a><span class="ex">docker</span> pull yourimage:production <span class="kw">||</span> <span class="fu">true</span></span>
<span id="cb47-7"><a href="#cb47-7" aria-hidden="true" tabindex="-1"></a><span class="ex">docker</span> pull yourimage:<span class="va">$BRANCH</span> <span class="kw">||</span> <span class="fu">true</span></span>
<span id="cb47-8"><a href="#cb47-8" aria-hidden="true" tabindex="-1"></a><span class="ex">docker</span> build <span class="at">-t</span> yourimage:<span class="va">$BRANCH</span> <span class="dt">\</span></span>
<span id="cb47-9"><a href="#cb47-9" aria-hidden="true" tabindex="-1"></a>    <span class="at">--cache-from</span><span class="op">=</span>yourimage:production <span class="dt">\</span></span>
<span id="cb47-10"><a href="#cb47-10" aria-hidden="true" tabindex="-1"></a>    <span class="at">--cache-from</span><span class="op">=</span>yourimage:<span class="va">$BRANCH</span> <span class="dt">\</span></span>
<span id="cb47-11"><a href="#cb47-11" aria-hidden="true" tabindex="-1"></a>    <span class="at">--build-arg</span> BUILDKIT_INLINE_CACHE=1 <span class="dt">\</span></span>
<span id="cb47-12"><a href="#cb47-12" aria-hidden="true" tabindex="-1"></a>    .</span>
<span id="cb47-13"><a href="#cb47-13" aria-hidden="true" tabindex="-1"></a><span class="ex">docker</span> push yourimage:<span class="va">$BRANCH</span></span></code></pre></div>
<p>Instead of hard-coding something like <code>production</code>, you
can also get the default branch for your Git repository
(<code>master</code> or <code>main</code>):</p>
<div class="sourceCode" id="cb48"><pre
class="sourceCode bash"><code class="sourceCode bash"><span id="cb48-1"><a href="#cb48-1" aria-hidden="true" tabindex="-1"></a><span class="co"># ... see https://stackoverflow.com/q/28666357/6214034</span></span>
<span id="cb48-2"><a href="#cb48-2" aria-hidden="true" tabindex="-1"></a><span class="va">DEFAULT_BRANCH</span><span class="op">=</span><span class="va">$(</span><span class="fu">git</span> rev-parse <span class="at">--abbrev-ref</span> origin/HEAD<span class="va">)</span></span>
<span id="cb48-3"><a href="#cb48-3" aria-hidden="true" tabindex="-1"></a><span class="va">DEFAULT_BRANCH</span><span class="op">=</span><span class="va">$(</span><span class="fu">basename</span> <span class="va">$DEFAULT_BRANCH)</span></span>
<span id="cb48-4"><a href="#cb48-4" aria-hidden="true" tabindex="-1"></a><span class="ex">docker</span> pull yourimage:<span class="va">$DEFAULT_BRANCH</span> <span class="kw">||</span> <span class="fu">true</span></span></code></pre></div>
<h2 id="optional-self-warming-cache-with-buildkit">Optional:
Self-warming cache with BuildKit</h2>
<p>As an alternative to explicitly pulling the previous version of an
image, you can also have Docker automatically pull only those layers it
actually needs. In theory this is faster because layers that can’t be
reused won’t need to be pulled.</p>
<p>You can do this using BuildKit, the alternative and improved Docker
build backend. First, make sure you enable BuildKit in general using the
<code>DOCKER_BUILDKIT</code> environment variable. Second, make sure
you’re enabling inline caching metadata by passing in the
<code>BUILDKIT_INLINE_CACHE</code> build arg. You’ll want to do the
latter regardless!</p>
<p>The build script will look like this:</p>
<div class="sourceCode" id="cb49"><pre
class="sourceCode bash"><code class="sourceCode bash"><span id="cb49-1"><a href="#cb49-1" aria-hidden="true" tabindex="-1"></a><span class="co">#!/bin/bash</span></span>
<span id="cb49-2"><a href="#cb49-2" aria-hidden="true" tabindex="-1"></a><span class="bu">set</span> <span class="at">-euo</span> pipefail</span>
<span id="cb49-3"><a href="#cb49-3" aria-hidden="true" tabindex="-1"></a><span class="bu">export</span> <span class="va">DOCKER_BUILDKIT</span><span class="op">=</span>1</span>
<span id="cb49-4"><a href="#cb49-4" aria-hidden="true" tabindex="-1"></a><span class="ex">docker</span> build <span class="at">-t</span> yourimage <span class="at">--cache-from</span><span class="op">=</span>yourimage <span class="dt">\</span></span>
<span id="cb49-5"><a href="#cb49-5" aria-hidden="true" tabindex="-1"></a>       <span class="at">--build-arg</span> BUILDKIT_INLINE_CACHE=1 .</span>
<span id="cb49-6"><a href="#cb49-6" aria-hidden="true" tabindex="-1"></a><span class="ex">docker</span> push yourimage</span></code></pre></div>
<p>Notice there is no need for a <code>docker pull</code>.</p>
<p>Note that:</p>
<ol type="1">
<li>There were some bugs in the version in 19.03, so you may wish to
only use Docker 20.10 with this feature.</li>
<li>Some registries don’t work with this mechanism. Notably, the
deprecated GitHub Packages registry will fail, and this will break
caching altogether. You may wish to try the replacement GitHub Container
Registry, in beta as of May 2021 and see if it works there.</li>
</ol>
<h3 id="references-15">References</h3>
<ul>
<li><a
href="https://docs.docker.com/engine/reference/commandline/build/#specifying-external-cache-sources">External
cache sources in <code>docker build</code></a> (docs.docker.com)</li>
</ul>
<h1 id="best-practices-make-debugging-easier">Best practices: Make
debugging easier</h1>
<h2 id="stdout-logs">Write logs to <code>stdout</code> or
<code>stderr</code></h2>
<p>In order to debug problems in a running application, you’ll want to
make sure the logs from your application are captured.</p>
<p>Docker runtime environments will capture logs from
<code>stdout</code> and <code>stderr</code>, so just make sure that’s
where your logs go. You can then read the logs using tools like
<code>docker logs</code> or <code>kubectl logs</code>, depending how
you’re running your image, or redirect your logs elsewhere.</p>
<p>With Python’s built-in <code>logging</code> library, you can do:</p>
<div class="sourceCode" id="cb50"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb50-1"><a href="#cb50-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> sys, logging</span>
<span id="cb50-2"><a href="#cb50-2" aria-hidden="true" tabindex="-1"></a>logging.basicConfig(stream<span class="op">=</span>sys.stdout)</span></code></pre></div>
<p>In fact, by default Python’s <code>logging</code> will log to
<code>stdout</code>, so technically you don’t have to do anything.</p>
<h2 id="faulthandler">Prepare for C crashes</h2>
<p>If your Python program crashes due to a segfault or some other bug in
C code, you won’t get a traceback by default. And silent crashes are
hard to debug.</p>
<p>To fix this, set the <code>PYTHONFAULTHANDLER</code> environment
variable in your <code>Dockerfile</code>:</p>
<div class="sourceCode" id="cb51"><pre
class="sourceCode dockerfile"><code class="sourceCode dockerfile"><span id="cb51-1"><a href="#cb51-1" aria-hidden="true" tabindex="-1"></a><span class="kw">ENV</span> PYTHONFAULTHANDLER=1</span></code></pre></div>
<p>And now you’ll get tracebacks from C crashes:</p>
<div class="sourceCode" id="cb52"><pre
class="sourceCode bash"><code class="sourceCode bash"><span id="cb52-1"><a href="#cb52-1" aria-hidden="true" tabindex="-1"></a><span class="ex">$</span> docker run <span class="at">-it</span> crasher</span>
<span id="cb52-2"><a href="#cb52-2" aria-hidden="true" tabindex="-1"></a><span class="ex">About</span> to crash...</span>
<span id="cb52-3"><a href="#cb52-3" aria-hidden="true" tabindex="-1"></a><span class="ex">Fatal</span> Python error: Segmentation fault</span>
<span id="cb52-4"><a href="#cb52-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb52-5"><a href="#cb52-5" aria-hidden="true" tabindex="-1"></a><span class="ex">Current</span> thread 0x00007f2f75f98740 <span class="er">(</span><span class="ex">most</span> recent call first<span class="kw">)</span><span class="bu">:</span></span>
<span id="cb52-6"><a href="#cb52-6" aria-hidden="true" tabindex="-1"></a>  <span class="ex">File</span> <span class="st">&quot;/usr/local/lib/python3.7/ctypes/__init__.py&quot;</span>, line 505 in string_at</span>
<span id="cb52-7"><a href="#cb52-7" aria-hidden="true" tabindex="-1"></a>  <span class="ex">File</span> <span class="st">&quot;crash.py&quot;</span>, line 3 in crash</span>
<span id="cb52-8"><a href="#cb52-8" aria-hidden="true" tabindex="-1"></a>  <span class="ex">File</span> <span class="st">&quot;crash.py&quot;</span>, line 5 in <span class="op">&lt;</span>module<span class="op">&gt;</span></span></code></pre></div>
<h3 id="references-16">References</h3>
<ul>
<li><a
href="https://docs.python.org/3/library/faulthandler.html">Python’s
<code>faulthandler</code> library</a> (docs.python.org)</li>
</ul>
<h2 id="identifiable">Record the build’s version control revision and
branch</h2>
<p>It’s useful to know what exact revision of your application is
running in production—or on your laptop—when you’re trying to reproduce
a problem. You should therefore record the revision of your
application’s source code, as well as the branch, in the image
itself.</p>
<p>First, you can do this by adding metadata labels to the image:</p>
<div class="sourceCode" id="cb53"><pre
class="sourceCode bash"><code class="sourceCode bash"><span id="cb53-1"><a href="#cb53-1" aria-hidden="true" tabindex="-1"></a><span class="co">#!/bin/bash</span></span>
<span id="cb53-2"><a href="#cb53-2" aria-hidden="true" tabindex="-1"></a><span class="bu">set</span> <span class="at">-euo</span> pipefail</span>
<span id="cb53-3"><a href="#cb53-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-4"><a href="#cb53-4" aria-hidden="true" tabindex="-1"></a><span class="va">GIT_COMMIT</span><span class="op">=</span><span class="va">$(</span><span class="fu">git</span> rev-parse <span class="at">--short</span> HEAD<span class="va">)</span></span>
<span id="cb53-5"><a href="#cb53-5" aria-hidden="true" tabindex="-1"></a><span class="va">GIT_BRANCH</span><span class="op">=</span><span class="va">$(</span><span class="fu">git</span> rev-parse <span class="at">--abbrev-ref</span> HEAD<span class="va">)</span></span>
<span id="cb53-6"><a href="#cb53-6" aria-hidden="true" tabindex="-1"></a><span class="ex">docker</span> build <span class="at">-t</span> myimage:latest <span class="dt">\</span></span>
<span id="cb53-7"><a href="#cb53-7" aria-hidden="true" tabindex="-1"></a>      <span class="at">--label</span> git-commit=<span class="va">$GIT_COMMIT</span> <span class="dt">\</span></span>
<span id="cb53-8"><a href="#cb53-8" aria-hidden="true" tabindex="-1"></a>      <span class="at">--label</span> git-branch=<span class="va">$GIT_BRANCH</span> .</span></code></pre></div>
<p>You can inspect these labels using <code>docker inspect
myimage</code>.</p>
<p>Second, you can also pass the git commit and branch into your image,
by using the <code>ARG</code> command in your
<code>Dockerfile</code>:</p>
<pre class="dockerfile"><code>FROM python:3.9-slim-bullseye
ARG git_commit
RUN echo $git_commit &gt; /git-commit.txt</code></pre>
<p>And then passing the information in using
<code>--build-arg</code>:</p>
<div class="sourceCode" id="cb55"><pre
class="sourceCode bash"><code class="sourceCode bash"><span id="cb55-1"><a href="#cb55-1" aria-hidden="true" tabindex="-1"></a><span class="ex">$</span> docker build <span class="at">-t</span> myimage <span class="at">--build-arg</span> git_commit=<span class="va">$GIT_COMMIT</span> .</span></code></pre></div>
<p>You can then have the status API in your web application include this
information, where it can be read by your monitoring infrastructure.</p>
<p>By default build arguments are exposed as environment variables only
during the build. If you want the <code>ARG</code> to be available as an
environment variable at runtime, you can do:</p>
<div class="sourceCode" id="cb56"><pre
class="sourceCode dockerfile"><code class="sourceCode dockerfile"><span id="cb56-1"><a href="#cb56-1" aria-hidden="true" tabindex="-1"></a><span class="kw">ARG</span> git_commit</span>
<span id="cb56-2"><a href="#cb56-2" aria-hidden="true" tabindex="-1"></a><span class="kw">ENV</span> git_commit=$git_commit</span></code></pre></div>
<h3 id="references-17">References</h3>
<ul>
<li><a
href="https://pythonspeed.com/articles/identifying-images/">What’s
running in production? Making your Docker images identifiable</a>
(pythonspeed.com)</li>
<li><a
href="https://nickjanetakis.com/blog/docker-tip-25-adding-metadata-to-your-docker-images-with-labels">Docker
image labels</a> (nickjanetakis.com)</li>
<li><a
href="https://docs.docker.com/engine/reference/builder/#arg">Docker
build arguments</a> (docs.docker.com)</li>
<li><a href="https://git-scm.com/docs/git-rev-parse"><code>git
rev-parse</code> documentation</a> (git-scm.com); personally I rely on
StackOverflow search results because Git is so awful</li>
</ul>
<h2 id="useful-tools">Optional: Pre-install useful tools</h2>
<p>Unless you’re super-worried about security, it’s useful to have a few
common debugging tools installed on your image. Since you <a
href="#no-root">shouldn’t run as root</a>, you won’t be able to install
these packages once the container is running.</p>
<p>For example, if you’re using the <code>slim</code> variants of the
official Python image, you’ll want to install packages like
<code>procps</code> and <code>net-tools</code> so you have access to
<code>ps</code> and <code>netstat</code>:</p>
<pre class="dockerfile"><code>RUN apt-get update &amp;&amp; apt-get -y install procps net-tools</code></pre>
<p>Another useful tool, albeit with potential security risks, is the
<code>manhole</code> library: it gives you a Python prompt into your
running process. See the reference for details.</p>
<h3 id="references-18">References</h3>
<ul>
<li><a href="https://pythonspeed.com/articles/live-debugging-python/">A
Python prompt into your runner process: debugging with Manhole</a>
(pythonspeed.com)</li>
<li><a
href="https://python-manhole.readthedocs.io/en/latest/"><code>manhole</code>
documentation</a> (python-manhole.readthedocs.io)</li>
</ul>
<h1 id="best-practices-correct-operation">Best practices: Correct
operation</h1>
<h2 id="networkbind">Have public ports listen on
<code>0.0.0.0</code></h2>
<p>If your server listens on <code>127.0.0.1</code>, you won’t be able
to access it from the outside of the container. Unless this is your
goal, make sure to listen on <code>0.0.0.0</code> so that it binds to
the container’s external IP.</p>
<h3 id="references-19">References</h3>
<ul>
<li><a
href="https://pythonspeed.com/articles/docker-connection-refused/">Connection
refused? Docker networking and how it impacts your image</a>
(pythonspeed.com)</li>
</ul>
<h2 id="bash">Avoid bash, or at least use bash strict mode and
<code>shellcheck</code></h2>
<p>Shell scripting is a recipe for failure. Whereas errors in a Python
script will cause an exception, in shell scripts the default is to
silently continue. The following script, for example, will print
“Success!”, which is probably not what you want.</p>
<div class="sourceCode" id="cb58"><pre
class="sourceCode bash"><code class="sourceCode bash"><span id="cb58-1"><a href="#cb58-1" aria-hidden="true" tabindex="-1"></a><span class="co">#!/bin/bash</span></span>
<span id="cb58-2"><a href="#cb58-2" aria-hidden="true" tabindex="-1"></a><span class="bu">export</span> <span class="va">VAR</span><span class="op">=</span><span class="va">$(</span><span class="bu">echo</span> hello <span class="kw">|</span> <span class="ex">nonexistentprogram</span><span class="va">)</span></span>
<span id="cb58-3"><a href="#cb58-3" aria-hidden="true" tabindex="-1"></a><span class="bu">echo</span> <span class="st">&quot;Success!&quot;</span></span></code></pre></div>
<p>You can make many errors cause the script to stop by using bash
strict mode (<code>set -euo pipefail</code>), but even that won’t work
in this case:</p>
<div class="sourceCode" id="cb59"><pre
class="sourceCode bash"><code class="sourceCode bash"><span id="cb59-1"><a href="#cb59-1" aria-hidden="true" tabindex="-1"></a><span class="co">#!/bin/bash</span></span>
<span id="cb59-2"><a href="#cb59-2" aria-hidden="true" tabindex="-1"></a><span class="bu">set</span> <span class="at">-euo</span> pipefail</span>
<span id="cb59-3"><a href="#cb59-3" aria-hidden="true" tabindex="-1"></a><span class="bu">export</span> <span class="va">VAR</span><span class="op">=</span><span class="va">$(</span><span class="bu">set</span> <span class="at">-euo</span> pipefail<span class="kw">;</span> <span class="bu">echo</span> hello <span class="kw">|</span> <span class="ex">nonexistentprogram</span><span class="va">)</span></span>
<span id="cb59-4"><a href="#cb59-4" aria-hidden="true" tabindex="-1"></a><span class="bu">echo</span> <span class="st">&quot;Success!&quot;</span></span></code></pre></div>
<p>This will still print “Success!”. Do you know how to fix that?</p>
<p>There are two solutions to the limits of shell scripting:</p>
<ol type="1">
<li>Use <code>set -euo pipefail</code> at the start of every
<code>bash</code> script, and use the <code>shellcheck</code> tool to
lint your shell script.</li>
<li>Replace the shell script with a Python script, since you already
have Python installed.</li>
</ol>
<h3 id="references-20">References</h3>
<ul>
<li><a
href="http://redsymbol.net/articles/unofficial-bash-strict-mode/">bash
strict mode</a> (redsymbol.net)</li>
<li><a href="https://www.shellcheck.net/">The <code>shellcheck</code>
shell linter</a> (shellcheck.net)</li>
</ul>
<h2 id="shutdown">Ensure fast shutdowns</h2>
<p>If you don’t configure your image correctly, signals won’t be
delivered to your process, and shutdowns will take 10 seconds: first the
original signal will be used, then after a timeout SIGKILL will be used,
shutting it down with extreme prejudice (the same way <code>kill
-9</code> does).</p>
<p>To get signal delivery working:</p>
<ol type="1">
<li>Use the <code>[]</code> syntax of <code>ENTRYPOINT</code> and
<code>CMD</code>, not the shell syntax.</li>
<li>If your entrypoint script is a shell script, make sure it ends by
using the shell <code>exec</code> command to run your final program.
This will replace the shell process with your program. In Python you can
use <code>os.execve</code> or one of the related functions.</li>
</ol>
<p>For example, let’s say you have a <code>Dockerfile</code> that runs a
script called <code>entrypoint.sh</code>. The shell script should look
like this:</p>
<div class="sourceCode" id="cb60"><pre
class="sourceCode bash"><code class="sourceCode bash"><span id="cb60-1"><a href="#cb60-1" aria-hidden="true" tabindex="-1"></a><span class="co">#!/bin/bash</span></span>
<span id="cb60-2"><a href="#cb60-2" aria-hidden="true" tabindex="-1"></a><span class="bu">set</span> <span class="at">-euo</span> pipefail</span>
<span id="cb60-3"><a href="#cb60-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-4"><a href="#cb60-4" aria-hidden="true" tabindex="-1"></a><span class="co"># BAD:</span></span>
<span id="cb60-5"><a href="#cb60-5" aria-hidden="true" tabindex="-1"></a><span class="co"># python myserver.py</span></span>
<span id="cb60-6"><a href="#cb60-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-7"><a href="#cb60-7" aria-hidden="true" tabindex="-1"></a><span class="co"># GOOD:</span></span>
<span id="cb60-8"><a href="#cb60-8" aria-hidden="true" tabindex="-1"></a><span class="bu">exec</span> python myserver.py</span></code></pre></div>
<p>And the <code>Dockerfile</code>:</p>
<div class="sourceCode" id="cb61"><pre
class="sourceCode dockerfile"><code class="sourceCode dockerfile"><span id="cb61-1"><a href="#cb61-1" aria-hidden="true" tabindex="-1"></a><span class="co"># BAD:</span></span>
<span id="cb61-2"><a href="#cb61-2" aria-hidden="true" tabindex="-1"></a><span class="co"># ENTRYPOINT ./entrypoint.sh</span></span>
<span id="cb61-3"><a href="#cb61-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-4"><a href="#cb61-4" aria-hidden="true" tabindex="-1"></a><span class="co"># GOOD:</span></span>
<span id="cb61-5"><a href="#cb61-5" aria-hidden="true" tabindex="-1"></a><span class="kw">ENTRYPOINT</span> [<span class="st">&quot;./entrypoint.sh&quot;</span>]</span></code></pre></div>
<p>Additionally, there are two other problems you might encounter:</p>
<ol type="1">
<li>If you’re using a shell script there’s another failure mode
involving pipes: don’t use them on the final command in your entrypoint
script.</li>
<li>If your program expects a signal other than <code>SIGTERM</code>,
use the <code>STOPSIGNAL</code> <code>Dockerfile</code> command. For
example, if your program shuts down just fine outside of Docker when you
hit Ctrl-C and a <code>KeyboardInterrupt</code> is raised, that means it
expects a <code>SIGINT</code>, and you should use <code>STOPSIGNAL
INT</code>.</li>
</ol>
<p>Another approach to Ctrl-C handling can be done with
<code>tini</code>, see the next section.</p>
<h3 id="references-21">References</h3>
<ul>
<li><a href="https://hynek.me/articles/docker-signals/">Why your
Dockerized application isn’t receiving signals</a> (hynek.me)</li>
<li><a
href="https://docs.docker.com/engine/reference/builder/#stopsignal"><code>STOPSIGNAL</code>
reference</a> (docs.docker.com)</li>
<li><a
href="https://docs.python.org/3/library/os.html#os.execl"><code>os.exec*</code>
documentation</a> (docs.python.org)</li>
</ul>
<h2 id="init">Add an <code>init</code> process</h2>
<p>Unix systems are designed to have an <code>init</code> process with
PID 1 to help deal with existing processes. If you’re running
subprocesses in your image you’ll therefore want to ensure you have one
setup.</p>
<p>With Docker you can do this on the command-line when you run an image
(<code>docker run --init</code>). Not all runtime environments have this
option, though, so it’s likely better to include one in your
<code>Dockerfile</code>:</p>
<pre class="dockerfile"><code>FROM python:3.9-slim-bullseye
RUN apt-get update &amp;&amp; apt-get install -y tini
COPY your-entrypoint.sh .
ENTRYPOINT [&quot;tini&quot;, &quot;--&quot;, &quot;./your-entrypoint.sh&quot;]</code></pre>
<p>One useful option to add to <code>tini</code> is the <code>-g</code>
option:</p>
<div class="sourceCode" id="cb63"><pre
class="sourceCode dockerfile"><code class="sourceCode dockerfile"><span id="cb63-1"><a href="#cb63-1" aria-hidden="true" tabindex="-1"></a><span class="kw">ENTRYPOINT</span> [<span class="st">&quot;tini&quot;</span>, <span class="st">&quot;-g&quot;</span>, <span class="st">&quot;--&quot;</span>, <span class="st">&quot;./your-entrypoint.sh&quot;</span>]</span></code></pre></div>
<p>When the parent <code>tini</code> process is killed, instead of just
killing the top-level process (a shell running
<code>your-entrypoint.sh</code> in the above example), it will send a
signal to <em>all</em> descendant processes in the main process group.
In practice that is likely to be all the container’s processes. This is
almost always the behavior you want: shutting down kills all processes
immediately.</p>
<h3 id="references-22">References</h3>
<ul>
<li><a href="https://github.com/krallin/tini">The <code>tini</code> init
process</a> (github.com)</li>
</ul>
<h2 id="deb-interactive">Set a non-interactive frontend for
Debian/Ubuntu package installs</h2>
<p>When installing packages via <code>apt-get</code> on Debian, Ubuntu,
or Debian-based images like the official <code>python</code> image, you
don’t want the build trying to ask you questions about how to configure
the packages you are installing. You do so by setting the
<code>DEBIAN_FRONTEND</code> environment variable to
<code>noninteractive</code>.</p>
<p>One way to do so is like this:</p>
<pre class="dockerfile"><code>RUN export DEBIAN_FRONTEND=noninteractive &amp;&amp; \
    apt-get install -y gcc</code></pre>
<h2 id="health-checks">Add health checks</h2>
<p>Your running container may lock up or stop working due to bugs or
other problems. Most container runtimes can therefore monitor running
containers and check if they’re still alive—but only if you configure
health checks.</p>
<p>There are at least two different ways to configure health checks,
depending on your runtime environment:</p>
<ol type="1">
<li>The <code>Dockerfile</code> format supports one kind, which is used
by tools like Docker Swarm.</li>
<li>Kubernetes has its own mechanism, and doesn’t support the one in
<code>Dockerfile</code>.</li>
</ol>
<p>Here’s an example of the <code>Dockerfile</code> variant, which uses
the <code>HEALTHCHECK</code> command:</p>
<div class="sourceCode" id="cb65"><pre
class="sourceCode dockerfile"><code class="sourceCode dockerfile"><span id="cb65-1"><a href="#cb65-1" aria-hidden="true" tabindex="-1"></a><span class="kw">FROM</span> python:3.9-slim-bullseye</span>
<span id="cb65-2"><a href="#cb65-2" aria-hidden="true" tabindex="-1"></a><span class="kw">HEALTHCHECK</span> <span class="op">--interval=3s</span> <span class="op">--timeout=1s</span> \</span>
<span id="cb65-3"><a href="#cb65-3" aria-hidden="true" tabindex="-1"></a>  <span class="kw">CMD</span> [<span class="st">&quot;python&quot;</span>, <span class="st">&quot;-c&quot;</span>, <span class="op">\</span></span>
<span id="cb65-4"><a href="#cb65-4" aria-hidden="true" tabindex="-1"></a>       <span class="st">&quot;from urllib.request import urlopen; </span><span class="op">\</span></span>
<span id="cb65-5"><a href="#cb65-5" aria-hidden="true" tabindex="-1"></a><span class="st">        urlopen(&#39;http://localhost:8000&#39;).read()&quot;</span>]</span>
<span id="cb65-6"><a href="#cb65-6" aria-hidden="true" tabindex="-1"></a><span class="kw">ENTRYPOINT</span> [<span class="st">&quot;python&quot;</span>, <span class="st">&quot;-m&quot;</span>, <span class="st">&quot;http.server&quot;</span>]</span></code></pre></div>
<h3 id="references-23">References</h3>
<ul>
<li><a
href="https://docs.docker.com/engine/reference/builder/#healthcheck"><code>Dockerfile</code>’s
<code>HEALTHCHECK</code> command</a> (docs.docker.com)</li>
<li><a
href="https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/">Kubernetes
health checks</a> (kubernetes.io)</li>
<li><a
href="https://github.com/willfarrell/docker-autoheal">docker-autoheal</a>
is a tool that will automatically restart Docker containers that failed
their healthcheck.</li>
</ul>
<h2 id="bytecode">Pre-compile bytecode for faster startup</h2>
<p>Python compiles source code to <code>.pyc</code> files, the
corresponding bytecode. If <code>.pyc</code> files aren’t available in
your Docker image, your application will need to compile them, and this
can lead to much slower startup. This leads to a tradeoff between
startup performance and image size:</p>
<ul>
<li>If you want your container to start as quickly as possible, you will
want to precompile the <code>.pyc</code> files.</li>
<li>If you want your image to be as small as possible, you want to have
no <code>.pyc</code> files at all in your image.</li>
</ul>
<p>Presuming you want fast startup, add the following to the end of your
<code>Dockerfile</code> to create <code>.pyc</code> files for all
installed packages:</p>
<pre class="dockerfile"><code>RUN python -c &quot;import compileall; compileall.compile_path(maxlevels=10)&quot;</code></pre>
<p>If you also have code in the current directory you want to compile,
you can compile it like so:</p>
<pre class="dockerfile"><code>RUN python -m compileall yourpackage/</code></pre>
<h3 id="references-24">References</h3>
<ul>
<li><a href="https://docs.python.org/3/library/compileall.html">The
<code>compileall</code> module</a> (docs.python.org)</li>
</ul>
<h1 id="best-practices-reproducible-builds">Best practices: Reproducible
builds</h1>
<p>Imagine you start with a certain revision of your source code and
build a Docker image. A month later, you start with the same revision,
fix a minor bug, and build a new image from scratch.</p>
<p>If your build is not reproducible, you might end up installing
different versions of your Python dependencies, system packages, and
perhaps even a different version of the operating system. The resulting
image might have new bugs, behave in unexpected ways, or even fail to
work completely due to incompatible changes. A minor bug fix has now
spiraled out of control.</p>
<p>But if your build is reproducible, your new image will be mostly the
same as your old image: the only difference will be the bug fix.</p>
<p>Your goal then is to have a reproducible build: the same inputs
should result in the same output.</p>
<h2 id="base">Choose a stable base image and tag</h2>
<p>When choosing a base image for your <code>Dockerfile</code>, you will
likely want:</p>
<ul>
<li><strong>Stability:</strong> You want a build today to give you the
same basic set of libraries, directory structure, and infrastructure as
a build tomorrow, otherwise your application will randomly break.</li>
<li><strong>Security updates:</strong> You want the base image to be
well-maintained, so that you get security updates for the base operating
system in a timely manner.</li>
<li><strong>Up-to-date dependencies:</strong> Unless you’re building a
very simple application, you will likely depend on operating
system-installed libraries and applications, for example a C compiler.
Ideally these dependencies would be fairly modern.</li>
<li><strong>Extensive dependencies:</strong> Some applications will
require less popular dependencies—a base image with access to a large
number of libraries makes this easier.</li>
<li><strong>Up-to-date Python:</strong> Having an up-to-date Python
available out of the box saves you some effort.</li>
<li><strong>Small images:</strong> All things being equal, it’s better
to have a smaller Docker image than a bigger Docker image.</li>
</ul>
<p>Ubuntu Long Term Support (LTS) releases, Debian Stable, and RedHat
Enterprise Linux are all reasonable candidates, since they aim for
backwards compatibility while still providing security updates and
critical bug fixes. As I <a href="#no-alpine">discuss later on</a>, I
recommend avoiding Alpine Linux.</p>
<p>Of course, each of the distributions has different variations
available, including non-LTS releases. So when you choose a base image,
you’ll need to make sure to specify a particular release using a
tag:</p>
<ul>
<li><strong>Bad:</strong> <code>FROM ubuntu</code>—today this might be
Ubuntu 20.04, eventually it will be Ubuntu 22.04.</li>
<li><strong>Good:</strong> <code>FROM ubuntu:20.04</code>, <code>FROM
debian:11</code> aka <code>FROM debian:bullseye</code>.</li>
</ul>
<p>Here’s a comparison between the three. Newer releases may be better
in some cases, insofar as they will have more up-to-date system
packages.</p>
<table>
<thead>
<tr class="header">
<th>Distribution</th>
<th>Released</th>
<th>End-of-life</th>
<th>Python versions</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Debian 11</td>
<td>Aug 2021</td>
<td>Aug 2025</td>
<td>3.9</td>
</tr>
<tr class="even">
<td>Ubuntu 20.04</td>
<td>Apr 2020</td>
<td>Apr 2025</td>
<td>3.9, 3.8</td>
</tr>
<tr class="odd">
<td>RHEL 8</td>
<td>May 2019</td>
<td>May 2024</td>
<td>3.9, 3.8, 3.6</td>
</tr>
</tbody>
</table>
<p>Even though they all include 3.9, at the time of writing (August
2021) they have different point releases, none of which match 3.9.6
which was released in late June 2021:</p>
<table>
<thead>
<tr class="header">
<th>Image</th>
<th>3.9 release</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Debian 11</td>
<td>3.9.2</td>
</tr>
<tr class="even">
<td>Ubuntu 20.04</td>
<td>3.9.5</td>
</tr>
<tr class="odd">
<td>RHEL 8</td>
<td>3.9.2</td>
</tr>
</tbody>
</table>
<p>Another option is the “official” Docker Python image, the one blessed
by Docker-the-company. It has variants with pretty much every version of
Python available, so you’re not tied to whichever versions Linux
distributions decided to include or backport. In particular, you’ll want
the variant based on Debian Stable, the most-up-to-date being
<code>bullseye</code>. And I also recommend using the <code>slim</code>
variant to get a smaller base image.</p>
<p>Given these choices, you can choose different levels of
reproducibility:</p>
<ul>
<li><code>python:3.9-slim-bullseye</code> is the latest sub-release of
Python 3.9, installed on top of Debian “Bullseye” 11. At the time of
writing this will be 3.9.6, later it will be 3.9.7, and so on.</li>
<li><code>python:3.9.6-slim-bullseye</code> is a specific sub-release,
Python 3.9.6. This tag might still point to different images over time,
however; it might be updated with newer releases of <code>pip</code>,
for example, or newer system packages.</li>
<li><code>python@sha256:6331fb167811d3bf7a7a33eaf4ac233ae63fa5d0e21cfd341a22bce2905ec5b5</code>
is a specific image, unchanging even if the tags get pointed at new
images. There’s no guarantee the Hub will keep old images around though,
so you may wish to copy the image into your registry, or even create
your own custom base image. The latter is covered below.</li>
</ul>
<p>Note that at the moment the Python packaged by the official images is
not as fast as some of the alternatives. For example, Python runs 10-20%
faster using Ubuntu and Debian’s version of Python 3.9. So if
performance is critical you may wish to use Ubuntu or Debian.</p>
<h3 id="references-25">References</h3>
<ul>
<li><a href="https://hub.docker.com/_/python">“Official” Python base
image</a> (hub.docker.com)</li>
<li><a href="https://hub.docker.com/_/debian">Debian base image</a>
(hub.docker.com)</li>
<li><a href="https://hub.docker.com/_/ubuntu">Ubuntu base image</a>
(hub.docker.com)</li>
<li><a href="https://developers.redhat.com/products/rhel/ubi">RedHat
Universal Base Images</a> (developers.redhat.com)</li>
<li><a
href="https://docs.docker.com/engine/reference/commandline/pull/#pull-an-image-by-digest-immutable-identifier">Pulling
an image by digest</a> (docs.docker.com)</li>
<li><a
href="https://pythonspeed.com/articles/base-image-python-docker-images/">Choosing
a base image for your Python application</a> (pythonspeed.com)</li>
<li><a
href="https://pythonspeed.com/articles/faster-python/">Performance
comparison between different Python builds</a> (pythonspeed.com)</li>
</ul>
<h2 id="redhat-compatible-base-images">RedHat-compatible base
images</h2>
<p>If you want to use a RedHat-compatible base image, but not pay for
RedHat Enterprise Linux, you would in the past have used CentOS. CentOS
8 is however no longer a stable, maintained distribution; you should not
be using it.</p>
<p>Supported alternatives include:</p>
<ul>
<li>RedHat’s own Universal Base Images. Not all RedHat packages are
available in these images, but they should include most popular
packages.</li>
<li>Oracle Linux is a pre-existing clone of RedHat Enterprise Linux,
maintained by Oracle. You can pay for commercial support, but you can
also just use it for free.</li>
<li>AlmaLinux was created by CloudLinux, a commercial Linux vendor who
used to base their product on CentOS.</li>
<li>RockyLinux was started by one of the original creators of
CentOS.</li>
</ul>
<p>Oracle Linux has been in existence for much longer, so it has better
tooling support from things like security scanners. Otherwise, the last
three options don’t seem much different.</p>
<h3 id="references-26">References</h3>
<ul>
<li><a href="https://developers.redhat.com/products/rhel/ubi">RedHat
Universal Base Images</a> (developers.redhat.com)</li>
<li><a href="https://hub.docker.com/u/redhat"><code>redhat/ubi8</code>
on Docker Hub</a> (hub.docker.com)</li>
<li><a
href="https://catalog.redhat.com/software/containers/search">RedHat’s
container image registry</a> (catalog.redhat.com)</li>
<li><a
href="https://hub.docker.com/_/oraclelinux"><code>oraclelinux</code>
image</a> (hub.docker.com)</li>
<li><a href="https://hub.docker.com/_/almalinux"><code>almalinux</code>
image</a> (hub.docker.com)</li>
<li><a
href="https://hub.docker.com/_/rockylinux"><code>rockylinux</code>
image</a> (hub.docker.com)</li>
</ul>
<h2 id="pin-python">Pin your Python dependencies</h2>
<p>If you run:</p>
<pre class="dockerfile"><code>RUN pip install flask</code></pre>
<p>You will get one version today, and potentially a very different
version in 6 months. So when you install Python dependencies you want to
install “pinned” versions, specific versions of the package and its
transitive dependencies; <code>flask</code> depends on other libraries,
and we want them to be pinned too.</p>
<p>In general you want to maintain two lists of dependencies:</p>
<ol type="1">
<li>The high-level dependencies of your application, the particular
libraries you’re importing. For example, <code>flask</code> and
<code>pandas</code>.</li>
<li>The pinned transitive dependencies, which you use to install the
dependencies as part of the Docker build.</li>
</ol>
<p>The high-level dependencies are used to regenerate the transitive
dependencies; see below in <a href="#update-dependencies">the section on
updating dependencies</a>.</p>
<p>There are three tools for handling these two sets of dependency
files: <code>pip-tools</code> (the simplest of the three),
<code>poetry</code>, and <code>pipenv</code>.</p>
<p>With <code>pip-tools</code>, for example, you would have a
<code>requirements.in</code> file that looks like this:</p>
<div class="sourceCode" id="cb69"><pre
class="sourceCode default"><code class="sourceCode default"><span id="cb69-1"><a href="#cb69-1" aria-hidden="true" tabindex="-1"></a>flask</span>
<span id="cb69-2"><a href="#cb69-2" aria-hidden="true" tabindex="-1"></a>pandas</span></code></pre></div>
<p>You would then compile it to a <code>requirements.txt</code> by
running <code>pip-compile --generate-hashes requirements.in</code>.
Hashes are useful to ensure you’re getting the exact same package, and
that it hasn’t been replaced on PyPI by a malicious attacker. The
resulting <code>requirements.txt</code> would look like this:</p>
<div class="sourceCode" id="cb70"><pre
class="sourceCode default"><code class="sourceCode default"><span id="cb70-1"><a href="#cb70-1" aria-hidden="true" tabindex="-1"></a>click==7.0 \</span>
<span id="cb70-2"><a href="#cb70-2" aria-hidden="true" tabindex="-1"></a>    --hash=sha256:2335065e6395b9e67ca716de5f7526736bfa6ceead690adf616d925bdc622b13 \</span>
<span id="cb70-3"><a href="#cb70-3" aria-hidden="true" tabindex="-1"></a>    --hash=sha256:5b94b49521f6456670fdb30cd82a4eca9412788a93fa6dd6df72c94d5a8ff2d7 \</span>
<span id="cb70-4"><a href="#cb70-4" aria-hidden="true" tabindex="-1"></a>    # via flask</span>
<span id="cb70-5"><a href="#cb70-5" aria-hidden="true" tabindex="-1"></a>flask==1.1.1 \</span>
<span id="cb70-6"><a href="#cb70-6" aria-hidden="true" tabindex="-1"></a>    --hash=sha256:13f9f196f330c7c2c5d7a5cf91af894110ca0215ac051b5844701f2bfd934d52 \</span>
<span id="cb70-7"><a href="#cb70-7" aria-hidden="true" tabindex="-1"></a>    --hash=sha256:45eb5a6fd193d6cf7e0cf5d8a5b31f83d5faae0293695626f539a823e93b13f6</span>
<span id="cb70-8"><a href="#cb70-8" aria-hidden="true" tabindex="-1"></a>itsdangerous==1.1.0 \</span>
<span id="cb70-9"><a href="#cb70-9" aria-hidden="true" tabindex="-1"></a>    --hash=sha256:321b033d07f2a4136d3ec762eac9f16a10ccd60f53c0c91af90217ace7ba1f19 \</span>
<span id="cb70-10"><a href="#cb70-10" aria-hidden="true" tabindex="-1"></a>    --hash=sha256:b12271b2047cb23eeb98c8b5622e2e5c5e9abd9784a153e9d8ef9cb4dd09d749 \</span>
<span id="cb70-11"><a href="#cb70-11" aria-hidden="true" tabindex="-1"></a>    # via flask</span>
<span id="cb70-12"><a href="#cb70-12" aria-hidden="true" tabindex="-1"></a>...</span></code></pre></div>
<p>Your <code>Dockerfile</code> would do:</p>
<pre class="dockerfile"><code>RUN pip install -r requirements.txt</code></pre>
<h3 id="references-27">References</h3>
<ul>
<li><a
href="https://blog.ometer.com/2017/01/10/dear-package-managers-dependency-resolution-results-should-be-in-version-control/">Dear
package managers: dependency resolution results should be in version
control</a> (blog.ometer.com)</li>
<li><a href="https://pythonspeed.com/articles/pipenv-docker/">Faster
Docker builds with pipenv, poetry, and pip-tools</a>
(pythonspeed.com)</li>
<li><a
href="https://github.com/jazzband/pip-tools"><code>pip-tools</code>
documentation</a> (github.com)</li>
<li><a href="https://python-poetry.org/"><code>poetry</code>
documentation</a> (python-poetry.org)</li>
<li><a
href="https://pipenv.kennethreitz.org/en/latest/"><code>pipenv</code>
documentation</a> (pipenv.kennethreitz.org)</li>
</ul>
<h2 id="update-dependencies">∞ Update all pinned dependencies once a
month ∞</h2>
<p>As we discussed above, in order to ensure reproducible builds you
want to keep the following from changing by pinning them to specific
versions:</p>
<ol type="1">
<li>Base image.</li>
<li>System packages (optional).</li>
<li>Version of Python.</li>
<li>Python dependencies.</li>
</ol>
<p>It can be tempting to leave these versions unchanged for long periods
of time, to ensure a stable baseline for your application. This would be
a mistake:</p>
<ol type="1">
<li>You need to get security updates and other critical bug fixes that
you’ve missed.</li>
<li>A series of small upgrades are much safer and easier than one
massive upgrade.</li>
</ol>
<p>To expand on the second point: if you only upgrade dependencies once
a year, you now potentially have a new base operating system, a new
version of Python, and major changes to three libraries you depend on.
If these updates cause your program to have issues, it can be difficult
to figure out what caused them.</p>
<p>It’s also difficult to convince management that you should spend a
week upgrading your dependencies; what about all those features and bug
fixes on the product plan?</p>
<p>On the other hand, if every month you update your dependencies, you
will only be changing one or two things at a time. That means problems
can be easily pinpointed: if you’ve only updated Flask this month, it’s
clear what caused the regression in your application.</p>
<p>What’s more, the chunks of time you spend on these upgrades will also
be much shorter, causing less disruption to other work.</p>
<p>This doesn’t mean that every time a new major, incompatible release
comes out you should immediately rewrite your software. “Once a month”
is a starting point, not the correct timespan for every dependency. But
if you decide not to upgrade a dependency for now, make it an explicit
decision with an explicit and ideally short-term deadline for when you
will upgrade.</p>
<h2 id="pin-system">Optional: Pin system packages</h2>
<p>One of the benefits of using a stable base operating system like
Debian, Ubuntu LTS, or RHEL is compatibility over time. As long as you
stick to a major release, the maintainers will try to release critical
bug fixes and security updates to libraries without making incompatible
changes.</p>
<p>This is the theory.</p>
<p>In practice, that might not be good enough for you. If you really
want to ensure specific package versions get installed, instead of
doing:</p>
<pre class="dockerfile"><code>RUN apt-get install -y nginx</code></pre>
<p>You can install a specific release:</p>
<pre class="dockerfile"><code>RUN apt-get install -y nginx=1.14.2-2+deb10u3</code></pre>
<p>On RedHat-based images, <code>dnf</code> and <code>yum</code> support
a similar syntax.</p>
<p>If you want to be even more paranoid about stability, you may want to
look into Nix, which is a completely different approach to software
packaging.</p>
<h3 id="references-28">References</h3>
<ul>
<li><a href="https://www.debian.org/doc/user-manuals#apt-guide">APT
User’s Guide</a> (debian.org)</li>
<li><a href="https://dnf.readthedocs.io/en/latest/index.html">DNF
Documentation</a> (dnf.readthedocs.io)</li>
<li><a href="https://nixos.org/nix/">The Nix package manager</a>
(nixos.org)</li>
</ul>
<h2 id="custom-base-image">Optional: Create a custom base image</h2>
<p>If your <code>Dockerfile</code> looks like this:</p>
<pre class="dockerfile"><code>FROM python:3.9-slim-bullseye
RUN apt-get update &amp;&amp; apt-get -y upgrade

# Your actual application:
# ...</code></pre>
<p>Then rebuilding your image from scratch with security updates once a
week will lead to a lack of reproducibility: the base image will change
over time, as will the security updates.</p>
<p>You can just assume that security updates won’t make too much of a
semantic difference, and live with it. Or, you can create a custom base
image to ensure reproducibility.</p>
<p>Here is an example of how you might do that; you might need to modify
this scheme to meet your particular workflow. You create a
<code>Dockerfile.base</code> that looks like this:</p>
<pre class="dockerfile"><code>FROM python:3.9-slim-bullseye
RUN apt-get update &amp;&amp; apt-get -y upgrade
# ...whatever else you want in a base image...</code></pre>
<p>And then build it with both a tag and label that store some permanent
identifier, the current date for example. You make sure never to change
the image for the tag once it’s created:</p>
<div class="sourceCode" id="cb76"><pre
class="sourceCode bash"><code class="sourceCode bash"><span id="cb76-1"><a href="#cb76-1" aria-hidden="true" tabindex="-1"></a><span class="ex">$</span> TODAY=2020-05-29</span>
<span id="cb76-2"><a href="#cb76-2" aria-hidden="true" tabindex="-1"></a><span class="ex">$</span> docker build <span class="at">-f</span> Dockerfile.base <span class="at">-t</span> yourorg/baseimage <span class="dt">\</span></span>
<span id="cb76-3"><a href="#cb76-3" aria-hidden="true" tabindex="-1"></a>    <span class="at">-t</span> yourorg/baseimage:<span class="va">$TODAY</span> <span class="at">--label</span> BASE_BUILT=<span class="va">$TODAY</span> .</span>
<span id="cb76-4"><a href="#cb76-4" aria-hidden="true" tabindex="-1"></a><span class="ex">$</span> docker push yourorg/baseimage</span>
<span id="cb76-5"><a href="#cb76-5" aria-hidden="true" tabindex="-1"></a><span class="ex">$</span> docker push yourorg/baseimage:<span class="va">$TODAY</span></span></code></pre></div>
<p>Now you can build your normal image using that base image:</p>
<pre class="dockerfile"><code>FROM yourog/baseimage
COPY requirements.txt .
RUN pip install -r requirements.txt
# ... etc ...</code></pre>
<p>If you ever need to know which base image was used to create the
image, you can just look for the label:</p>
<div class="sourceCode" id="cb78"><pre
class="sourceCode bash"><code class="sourceCode bash"><span id="cb78-1"><a href="#cb78-1" aria-hidden="true" tabindex="-1"></a><span class="ex">$</span> docker inspect myapp <span class="kw">|</span> <span class="fu">grep</span> BASE_BUILT</span>
<span id="cb78-2"><a href="#cb78-2" aria-hidden="true" tabindex="-1"></a>       <span class="st">&quot;BASE_BUILT&quot;</span><span class="ex">:</span> <span class="st">&quot;2020-05-29&quot;</span></span></code></pre></div>
<p>And you can always rebuild with that exact version of the base image
if need be:</p>
<pre class="dockerfile"><code>FROM yourorg/baseimage:2020-05-29
COPY requirements.txt .
RUN pip install -r requirements.txt
# ... etc ...</code></pre>
<h1 id="best-practices-faster-builds">Best practices: Faster builds</h1>
<h2 id="no-alpine">Don’t use Alpine Linux</h2>
<p>When installing Python packages from PyPI, <code>pip</code> can
usually speed up installation by downloading pre-compiled binary wheels
provided by the package maintainers. However, these wheels don’t work on
Alpine Linux, which means you have to rebuild every single package
yourself.</p>
<p>For example, if you want to install <code>pandas</code> and
<code>matplotlib</code> from PyPI, build time will go from ~30 seconds
on a Debian-based image to ~1500 seconds on Alpine, a 50× increase.
Alpine Linux has other issues as well, like a slower standard C library;
see the reference below for details.</p>
<h3 id="references-29">References</h3>
<ul>
<li><a
href="https://pythonspeed.com/articles/alpine-docker-python/">Using
Alpine can make Python Docker builds 50× slower</a>
(pythonspeed.com)</li>
</ul>
<h2 id="copy-late"><code>COPY</code> in files only when needed</h2>
<p>To a first approximation, each command in the <code>Dockerfile</code>
creates a new layer in the new image. Docker builds can use cached
layers to speed up the build: if the command hasn’t changed, or input
files haven’t changed if you’re doing a <code>COPY</code>, the cached
layer can be reused.</p>
<p>If a layer can’t be loaded from the cache, none of the later steps in
the <code>Dockerfile</code> can use caching either. So you want to
ensure you don’t invalidate the cache unnecessarily.</p>
<p>Consider the following <code>Dockerfile</code>:</p>
<pre class="dockerfile"><code>FROM python:3.9-slim-bullseye
COPY requirements.txt .
RUN apt-get update &amp;&amp; apt-get install -y gcc
RUN pip install -r requirements.txt</code></pre>
<p>If <code>requirements.txt</code> changes, that will invalidate the
<code>apt-get</code> command too, even though <code>apt-get</code>
doesn’t need <code>requirements.txt</code>. Better to copy in the file
only when you need it:</p>
<pre class="dockerfile"><code>FROM python:3.9-slim-bullseye
RUN apt-get update &amp;&amp; apt-get install -y gcc
COPY requirements.txt .
RUN pip install -r requirements.txt</code></pre>
<p>More broadly, you’ll want commands that don’t depend on
<code>COPY</code>, like <code>apt-get</code> or <code>dnf</code>, to run
before commands that do.</p>
<h3 id="references-30">References</h3>
<ul>
<li><a
href="https://pythonspeed.com/articles/docker-caching-model/">Faster or
slower: the basics of Docker build caching</a> (pythonspeed.com)</li>
</ul>
<h2 id="install-dependencies-first">Install dependencies separately from
your code</h2>
<p>Expanding on the previous point, if you’re installing your Python
application’s dependencies via <code>setup.py</code>, any change to your
application code will invalidate the list of Python dependencies you
installed. So that means downloading and installing all the packages
from PyPI again.</p>
<p>Instead of listing dependencies in <code>setup.py</code>, you should
list dependencies in a separate file, either
<code>requirements.txt</code> or the configuration files used by
<code>poetry</code> or <code>pipenv</code>. Then you can install 3rd
party libraries first and have that layer cached:</p>
<pre class="dockerfile"><code>FROM python:3.9-slim-bullseye
COPY requirements.txt .
RUN pip install -r requirements.txt
COPY . .
RUN pip install .</code></pre>
<p>See the best practices for Poetry (<a href="#poetry">ref</a>), Pipenv
(<a href="#pipenv">ref</a>), and Conda (<a href="#conda">ref</a>) for
details on installing dependencies with those tools.</p>
<h3 id="references-31">References</h3>
<ul>
<li><a
href="https://pip.pypa.io/en/stable/reference/pip_install/#requirements-file-format"><code>requirements.txt</code>
file format</a> (pip.pypa.io)</li>
</ul>
<h2 id="arg-late">Use <code>ARG</code> only when needed</h2>
<p>Just like changes to files can invalidate the cache, so can changing
build arguments passed in to a <code>ARG</code> command. So much like
you want to only <code>COPY</code> in files when you actually need them,
you also only want to add the <code>ARG</code> to the
<code>Dockerfile</code> at the point where you’ll actually be using
it.</p>
<p>For example, let’s say you want to pass in the Git commit to the
build. The Git commit will change on every build, so don’t do the
following or you’ll completely disable build caching:</p>
<pre class="dockerfile"><code>FROM python:3.9-slim-bullseye
# Bad location, it&#39;s not used yet and will invalidate cache:
ARG git_commit
RUN apt-get install ...
RUN pip install ...
RUN echo $git_commit &gt; /var/run/git-commit.txt</code></pre>
<p>Instead, do this:</p>
<pre class="dockerfile"><code>FROM python:3.9-slim-bullseye
RUN apt-get install ...
RUN pip install ...

# Good location, won&#39;t invalidate cache of apt-get or pip:
ARG git_commit
RUN echo $git_commit &gt; /var/run/git-commit.txt</code></pre>
<h2 id="use-docker-build---label-instead-of-label">Use <code>docker
build --label</code> instead of <code>LABEL</code></h2>
<p>If you’re passing in a build argument only so you can use it for a
<code>LABEL</code>, e.g.</p>
<div class="sourceCode" id="cb85"><pre
class="sourceCode dockerfile"><code class="sourceCode dockerfile"><span id="cb85-1"><a href="#cb85-1" aria-hidden="true" tabindex="-1"></a><span class="co"># ...</span></span>
<span id="cb85-2"><a href="#cb85-2" aria-hidden="true" tabindex="-1"></a><span class="kw">ARG</span> git_commit</span>
<span id="cb85-3"><a href="#cb85-3" aria-hidden="true" tabindex="-1"></a><span class="kw">LABEL</span> git_commit=$git_commit</span></code></pre></div>
<p>Better to just set the label using the <code>--label</code> argument
to <code>docker build</code>, e.g.</p>
<pre><code>$ docker build --label=git_commit=$GIT_COMMIT</code></pre>
<p>This way you don’t have to worry about the impact of the
<code>ARG</code> on layer caching.</p>
<h1 id="best-practices-small-images">Best practices: Small images</h1>
<h2 id="temporary-files">Keep temporary files from ending up in a
layer</h2>
<p>Each <code>RUN</code> and <code>COPY</code> command adds another
layer to your Docker image, rather like commits in the Git history. As a
result deleting files that are in a previous layer won’t shrink your
image, since they will still be present in the previous layer:</p>
<pre class="dockerfile"><code>RUN wget https://example.com/largefile.tar.gz
RUN tar xvfz largefile.tar.gz
RUN largefile/install.sh
# BAD, This will not shrink your image:
RUN rm -rf largefile.tar.gz largefile/</code></pre>
<p>Instead, you can combine these commands into a single layer, and then
the temporary files won’t end up in the image:</p>
<pre class="dockerfile"><code># GOOD, temporary files deleted before RUN ends:
RUN wget https://example.com/largefile.tar.gz &amp;&amp; \
    tar xvfz largefile.tar.gz &amp;&amp; \
    largefile/install.sh &amp;&amp; \
    rm -rf largefile.tar.gz largefile/</code></pre>
<p>This works because files are only stored in a layer at the end of the
<code>RUN</code>. If the file doesn’t exist when the <code>RUN</code>
finishes, it won’t get stored.</p>
<h3 id="references-32">References</h3>
<ul>
<li><a
href="https://pythonspeed.com/articles/smaller-docker-images/">Shrinking
your Python application’s Docker image: an overview</a>
(pythonspeed.com)</li>
</ul>
<h2 id="dive">Find large layers and files with <code>dive</code></h2>
<p>To figure out why your image is too large, you can use <code>docker
image history yourimage</code>, but that just tells you which layers are
using a lot of disk space. The <code>dive</code> tools shows you
per-layer sizes, and shows the differences and what files were added, so
you can get a much more nuanced view of where image size is coming
from.</p>
<h3 id="references-33">References</h3>
<ul>
<li><a href="https://github.com/wagoodman/dive"><code>dive</code></a>
(github.com)</li>
</ul>
<h2 id="dockerignore">Add files to <code>.dockerignore</code></h2>
<p>When you run <code>docker build</code>, all files in the source
directory are copied in to the context. The context is then used as the
source of files copied in with the <code>COPY</code> or <code>ADD</code>
commands. Usually you’ll pass in the current directory as the source
directory.</p>
<p>This has some issues:</p>
<ol type="1">
<li>If there are many large files, copying them into the context can
slow down your builds.</li>
<li>If you’re <code>COPY</code>ing whole directories, you might copy
files you didn’t mean to, which can mean larger images or even leaking
secret information.</li>
<li>Files you don’t care about may invalidate the cache.</li>
</ol>
<p>The solution is to create a <code>.dockerignore</code> file, listing
files you don’t want to be included in the context and therefore the
final Docker image. For example:</p>
<pre><code>Dockerfile
.dockerignore
venv/
**/__pycache__</code></pre>
<h3 id="references-34">References</h3>
<ul>
<li><a
href="https://docs.docker.com/engine/reference/builder/#dockerignore-file"><code>.dockerignore</code>
reference</a> (docs.docker.com)</li>
</ul>
<h2 id="git-dockerignore">Add <code>.git</code> to
<code>.dockerignore</code> (and some alternatives when you can’t)</h2>
<p>If you’re using Git, a useful entry to add to your
<code>.dockerignore</code> is the <code>.git</code> directory: by
default it includes the full history of your repository, which can be
quite large.</p>
<p>In some cases, this may not be possible.</p>
<p>If you’re using a tool like <code>setuptools-scm</code> or
<code>versioneer</code> to set your Python application’s version using
Git tags. In this case you can try one of the following options to
minimize the size of the <code>.git</code> directory:</p>
<ul>
<li>Minimize the size of the <code>.git</code> directory by doing a
“shallow” clone, e.g. <code>git clone --depth=50 yourrepo.git</code>,
where you only download the last N revisions instead of the full
history. Many CI services (GitHub Actions, GitLab CI) already do this by
default. The downside is that tags that are in older revisions outside
the shallowly clone history won’t show up, which can break the
auto-versioning tools mentioned above.</li>
<li>Use <code>git clone --filter=blob:none yourrepo.git</code>. This
only downloads the contents of history when you need access to it. That
means tags will all be visible, but the <code>.git</code> folder will be
much smaller unless you explicitly check out lots of tags and
branches.</li>
</ul>
<p>You can also rely on a <a href="#multi-stage">multi-stage Docker
build</a> to ensure the <code>.git</code> directory doesn’t make it into
the runtime image, at least.</p>
<p>If your application’s runtime logic relies on <code>.git</code> to
figure out its version, you can make this logic conditional, with a
fallback to environment variables which can then be <a
href="#identifiable">provided by the Docker image</a>.</p>
<h3 id="references-35">References</h3>
<ul>
<li><a href="https://git-scm.com/docs/git-clone"><code>git clone</code>
documentation</a> (git-scm.com)</li>
</ul>
<h2 id="chown">Avoid extra chowns</h2>
<p>Any time you modify a file in any way in the <code>Dockerfile</code>,
a whole new copy is stored in the next layer. Recursive
<code>chown</code>s can therefore result in very large images, since
you’re duplicating every file you change.</p>
<p>Therefore, instead of doing:</p>
<pre class="dockerfile"><code>COPY code .
RUN chown -R youruser code</code></pre>
<p>You should instead do:</p>
<div class="sourceCode" id="cb91"><pre
class="sourceCode dockerfile"><code class="sourceCode dockerfile"><span id="cb91-1"><a href="#cb91-1" aria-hidden="true" tabindex="-1"></a><span class="kw">COPY</span> <span class="op">--chown=youruser</span> code .</span></code></pre></div>
<p>This will do the <code>chown</code> as part of the copy, ensuring
only one copy of the files is kept.</p>
<h3 id="references-36">References</h3>
<ul>
<li><a
href="https://docs.docker.com/engine/reference/builder/#copy"><code>Dockerfile</code>
<code>COPY</code> reference</a> (docs.docker.com)</li>
</ul>
<h2 id="system-packages-clean">Don’t install unnecessary system
packages, and clean up when you’re done</h2>
<p>When you install system packages, whether DEB or RPM, the package
manager will typically keep a copy of the original package around, and
also store the package listing from the package index. This wastes
space.</p>
<p>You can’t delete these unneeded files in a separate <code>RUN</code>
because Docker layers are always additive. So the solution is to have a
single shell script that installs the packages, and then cleans up
unnecessary files.</p>
<p>Additionally, when you install a package, non-essential but
recommended dependencies may get installed, wasting space. Instead, you
want to install only the minimal necessary dependencies to ensure
smaller images.</p>
<p>For Debian/Ubuntu systems, the following shell script will do the
trick:</p>
<div class="sourceCode" id="cb92"><pre
class="sourceCode bash"><code class="sourceCode bash"><span id="cb92-1"><a href="#cb92-1" aria-hidden="true" tabindex="-1"></a><span class="co">#!/bin/bash</span></span>
<span id="cb92-2"><a href="#cb92-2" aria-hidden="true" tabindex="-1"></a><span class="bu">set</span> <span class="at">-euo</span> pipefail</span>
<span id="cb92-3"><a href="#cb92-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb92-4"><a href="#cb92-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Tell apt-get we&#39;re never going to be able to give manual</span></span>
<span id="cb92-5"><a href="#cb92-5" aria-hidden="true" tabindex="-1"></a><span class="co"># feedback:</span></span>
<span id="cb92-6"><a href="#cb92-6" aria-hidden="true" tabindex="-1"></a><span class="bu">export</span> <span class="va">DEBIAN_FRONTEND</span><span class="op">=</span>noninteractive</span>
<span id="cb92-7"><a href="#cb92-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb92-8"><a href="#cb92-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Update the package listing, so we know what package exist:</span></span>
<span id="cb92-9"><a href="#cb92-9" aria-hidden="true" tabindex="-1"></a><span class="ex">apt-get</span> update</span>
<span id="cb92-10"><a href="#cb92-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb92-11"><a href="#cb92-11" aria-hidden="true" tabindex="-1"></a><span class="co"># Install security updates:</span></span>
<span id="cb92-12"><a href="#cb92-12" aria-hidden="true" tabindex="-1"></a><span class="ex">apt-get</span> <span class="at">-y</span> upgrade</span>
<span id="cb92-13"><a href="#cb92-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb92-14"><a href="#cb92-14" aria-hidden="true" tabindex="-1"></a><span class="co"># Install a new package, without unnecessary recommended packages:</span></span>
<span id="cb92-15"><a href="#cb92-15" aria-hidden="true" tabindex="-1"></a><span class="ex">apt-get</span> <span class="at">-y</span> install <span class="at">--no-install-recommends</span> YOUR_PACKAGE_LIST_GOES_HERE</span>
<span id="cb92-16"><a href="#cb92-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb92-17"><a href="#cb92-17" aria-hidden="true" tabindex="-1"></a><span class="co"># Delete cached files we don&#39;t need anymore:</span></span>
<span id="cb92-18"><a href="#cb92-18" aria-hidden="true" tabindex="-1"></a><span class="ex">apt-get</span> clean</span>
<span id="cb92-19"><a href="#cb92-19" aria-hidden="true" tabindex="-1"></a><span class="fu">rm</span> <span class="at">-rf</span> /var/lib/apt/lists/<span class="pp">*</span></span></code></pre></div>
<p>In practice, modern Debian base images (both the official Debian and
official Ubuntu images) will actually do at least part of the clean-up
step automatically themselves, so you can skip that if you want, but
can’t hurt to make sure.</p>
<p>For RPM-based systems like RHEL:</p>
<div class="sourceCode" id="cb93"><pre
class="sourceCode bash"><code class="sourceCode bash"><span id="cb93-1"><a href="#cb93-1" aria-hidden="true" tabindex="-1"></a><span class="co">#!/bin/bash</span></span>
<span id="cb93-2"><a href="#cb93-2" aria-hidden="true" tabindex="-1"></a><span class="bu">set</span> <span class="at">-euo</span> pipefail</span>
<span id="cb93-3"><a href="#cb93-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-4"><a href="#cb93-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Install security updates, bug fixes and enhancements only.</span></span>
<span id="cb93-5"><a href="#cb93-5" aria-hidden="true" tabindex="-1"></a><span class="co"># --nodocs means documentation isn&#39;t installed, leading to a slightly smaller image.</span></span>
<span id="cb93-6"><a href="#cb93-6" aria-hidden="true" tabindex="-1"></a><span class="ex">dnf</span> <span class="at">-y</span> <span class="at">--nodocs</span> upgrade-minimal</span>
<span id="cb93-7"><a href="#cb93-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-8"><a href="#cb93-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Install a new package, without unnecessary recommended packages:</span></span>
<span id="cb93-9"><a href="#cb93-9" aria-hidden="true" tabindex="-1"></a><span class="ex">dnf</span> <span class="at">-y</span> <span class="at">--nodocs</span> install <span class="at">--setopt</span><span class="op">=</span>install_weak_deps=False YOUR_PACKAGE_LIST</span>
<span id="cb93-10"><a href="#cb93-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-11"><a href="#cb93-11" aria-hidden="true" tabindex="-1"></a><span class="co"># Delete cached files we don&#39;t need anymore:</span></span>
<span id="cb93-12"><a href="#cb93-12" aria-hidden="true" tabindex="-1"></a><span class="ex">dnf</span> clean all</span></code></pre></div>
<h3 id="references-37">References</h3>
<ul>
<li><a
href="https://pythonspeed.com/articles/system-packages-docker/">Installing
system packages in Docker with minimal bloat</a> (pythonspeed.com)</li>
<li><a href="https://www.debian.org/doc/user-manuals#apt-guide">APT
User’s Guide</a> (debian.org)</li>
<li><a href="https://dnf.readthedocs.io/en/latest/index.html">DNF
Documentation</a> (dnf.readthedocs.io)</li>
</ul>
<h2 id="smaller-pip">Disable pip caching</h2>
<p>By default <code>pip</code> keeps a copy of downloaded packages. This
wastes space.</p>
<p>To fix this, run <code>pip</code> with caching disabled:</p>
<pre class="dockerfile"><code>FROM python:3.9-slim-bullseye
RUN pip install --no-cache-dir flask</code></pre>
<h2 id="package-cache-buildkit">Optional: Caching installed packages
using BuildKit</h2>
<p>By default <code>pip</code> caches downloaded packages so that new
virtualenvs won’t require you to redownload everything. With classic
Docker you are forced to redownload the same packages every time
<code>requirements.txt</code> changed.</p>
<p>BuildKit, however, allows you to cache a directory outside of your
image and across builds, which allows you to get a similar set of
benefits. This level of caching is distinct from Docker’s layer caching.
If you change <code>requirements.txt</code>, <code>pip</code> will still
need to be rerun when you rebuild your Docker image, but at least it
won’t have to download everything from scratch.</p>
<p>If we’re using this feature we do not need to use best practices like
<code>pip install --no-cache</code>: saving downloaded packages is
actually desirable because they’re cached outside the image. And since
the official Debian Docker base image actually deletes downloaded
packages and archive indexes automatically, we want to disable that
feature too because we’re making sure to cache outside the image.</p>
<p>Here’s how to use this form of caching:</p>
<pre class="dockerfile"><code># syntax = docker/dockerfile:1.3
FROM python:3.9-slim-bullseye

# Disable auto-cleanup after install:
RUN rm /etc/apt/apt.conf.d/docker-clean
# Cache downloaded packages across runs:
RUN --mount=type=cache,target=/var/cache/apt,id=apt \
  apt-get update &amp;&amp; apt-get -y upgrade

COPY requirements.txt .
# Cache downloaded packages across runs:
RUN --mount=type=cache,target=/root/.cache,id=pip \
  pip install -r requirements.txt</code></pre>
<p>If you rebuild this (or another!) image on the same machine and have
to redo the <code>apt-get</code> or <code>pip install</code>, the
already downloaded packages will be available to the installer and won’t
need to be redownloaded.</p>
<p>Make sure to enable BuildKit with <code>export
DOCKER_BUILDKIT=1</code>.</p>
<p>For the <code>pip</code> packages I’m caching the
<code>~/.cache</code> directory, since that is also where Pipenv and
Poetry will store their files; <code>pip</code> uses
<code>~/.cache/pip</code> by default. You’ll want to use the absolute
path version of <code>~/.cache</code>,
e.g. <code>/home/youruser/.cache</code>.</p>
<h3 id="references-38">References</h3>
<ul>
<li><a
href="https://hub.docker.com/r/docker/dockerfile/"><code>Dockerfile</code>
BuildKit extensions</a> (hub.docker.com)</li>
<li><a
href="https://github.com/debuerreotype/debuerreotype/blob/master/scripts/debuerreotype-minimizing-config">Debian
base image apt config scripts</a> (github.com)</li>
<li><a
href="https://pythonspeed.com/articles/docker-cache-pip-downloads/">Speed
up pip downloads in Docker with BuildKit’s new caching</a>
(pythonspeed.com)</li>
</ul>
<h2 id="unnecessary-files">Optional: Remove files you don’t need</h2>
<p>Your Docker image may end up with files in it that you don’t need,
and whose existence you can’t prevent with other techniques. You have a
number of options to get rid of these extraneous files, if they’re
taking up too much space:</p>
<ul>
<li><a href="#multi-stage">Multi-stage builds</a>; see the relevant
chapter of the handbook.</li>
<li>The <code>docker-squash</code> tool allows you to merge multiple
layers into one.</li>
<li>The <code>docker-slim</code> tool uses runtime instrumentation to
figure out which files your container actually uses (probably a tiny
subset!) and creates a new image with only those files.</li>
</ul>
<h3 id="references-39">References</h3>
<ul>
<li><a
href="https://github.com/goldmann/docker-squash"><code>docker-squash</code></a>
(github.com)</li>
<li><a
href="https://github.com/docker-slim/docker-slim"><code>docker-slim</code></a>
(github.com)</li>
</ul>
<h1 id="best-practices-application-and-tool-specific">Best practices:
Application and tool-specific</h1>
<h2 id="using-a-virtualenv-in-your-dockerfile">Using a virtualenv in
your <code>Dockerfile</code></h2>
<p>The <code>activate</code> script included in the virtualenv isn’t
very usable from a <code>Dockerfile</code>, because each
<code>RUN</code> is a separate shell session. Instead, to activate a
virtualenv, you just need to set two environment variables:</p>
<pre class="dockerfile"><code>ENV VIRTUAL_ENV=/opt/venv
RUN python -m venv $VIRTUAL_ENV
ENV PATH=&quot;$VIRTUAL_ENV/bin:$PATH&quot;</code></pre>
<h3 id="references-40">References</h3>
<ul>
<li><a
href="https://pythonspeed.com/articles/activate-virtualenv-dockerfile/">Elegantly
activating a virtualenv in a Dockerfile</a> (pythonspeed.com)</li>
</ul>
<h2 id="dont-run-database-upgrades-on-startup">Don’t run database
upgrades on startup</h2>
<p>Your application might require database schema upgrades. You should
not, however, run those schema upgrades as part of your container’s
startup:</p>
<ol type="1">
<li>Most schema management systems will break if you do the same schema
upgrade concurrently, and it’s common to run multiple copies of the same
container.</li>
<li>It will encourage you to tightly couple your code to the database
schema, making schema rollbacks difficult or impossible.</li>
</ol>
<p>Instead, you should decouple the two. The usual solution for this is
to make your schema changes purely additive, e.g. adding columns but not
deleting them:</p>
<ol type="1">
<li>Migrate from schema S to schema S+1, with only additive
changes.</li>
<li>Over time upgrade some of your processes from application version V
to V+1.</li>
<li>Eventually everything is on V+1, and you don’t ever expect to
rollback to V.</li>
<li>Finally, migrate from schema S+1 to S+2, and now you can do
destructive schema changes to anything that V+1 no longer uses.</li>
</ol>
<h3 id="references-41">References</h3>
<ul>
<li><a
href="https://pythonspeed.com/articles/schema-migrations-server-startup/">Decoupling
database migrations from server startup: why and how</a>
(pythonspeed.com)</li>
<li><a href="https://flywaydb.org/">Flyway</a> (flywaydb.org) does
support concurrent schema upgrades, unlike most tools</li>
<li><a
href="https://www.martinfowler.com/articles/evodb.html">Evolutionary
Database Design</a> (martinfowler.com)</li>
</ul>
<h2 id="slow-queries">Make sure slow queries don’t break health
checks</h2>
<p>If you’ve implemented health checks for your server, the runtime
environment is occasionally sending queries to your server to see if
it’s still alive. An HTTP server will get HTTP queries, for example.</p>
<p>Now, imagine you have a single-threaded HTTP server that can only
handle one query at a time. And this server sometimes takes a long time
to answer queries, and in particular it’s blocking while handling that
query. Consider the following timeline:</p>
<ol type="1">
<li>A slow query is sent to the server. The server starts processing,
but doesn’t respond yet.</li>
<li>The runtime environment sends a liveness check to server, and gets
no response because slow query is still running.</li>
<li>The runtime environment sends another liveness check.</li>
<li>The runtime environment sends yet another liveness check.</li>
<li>The runtime environment decides the server is dead and kills it
before it can finish responding to the slow query.</li>
</ol>
<p>If you expect slow queries, you should configure your application
with enough concurrency that it can handle health check liveness
queries. For typical WSGI web applications this can be done by running
multiple threads, or multiple processes, depending on the application
configuration.</p>
<h2 id="enable-running-other-commands-via-the-command-line">Enable
running other commands via the command-line</h2>
<p>The combination of <code>ENTRYPOINT</code> and <code>CMD</code>
allows you to have default command-line arguments for your application,
and allows users to run your container with different command-line
options.</p>
<p>In some cases, however, you never expect the user to pass in any
command-line options to the entrypoint. Instead, you want them to be
able to run a <em>different</em> program when they give command-line
options. This can be useful for debugging, for example.</p>
<p>Thus by default the official <code>python</code> image runs
Python:</p>
<pre><code>$ docker run -it python:3.9-slim-bullseye
Python 3.8.3 (default, Jun  9 2020, 17:49:41) 
[GCC 8.3.0] on linux
Type &quot;help&quot;, &quot;copyright&quot;, &quot;credits&quot; or &quot;license&quot; for more information.
&gt;&gt;&gt;</code></pre>
<p>But you can pass it other commands instead, in this case
<code>bash</code>:</p>
<pre><code>$ docker run -it python:3.9-slim-bullseye \
    bash -c &#39;echo hello, I am $(whoami)&#39;
hello, I am root</code></pre>
<p>You can do this with the following combination of
<code>ENTRYPOINT</code> and <code>CMD</code> in your
<code>Dockerfile</code>:</p>
<div class="sourceCode" id="cb99"><pre
class="sourceCode dockerfile"><code class="sourceCode dockerfile"><span id="cb99-1"><a href="#cb99-1" aria-hidden="true" tabindex="-1"></a><span class="kw">ENTRYPOINT</span> []</span>
<span id="cb99-2"><a href="#cb99-2" aria-hidden="true" tabindex="-1"></a><span class="kw">CMD</span> [<span class="st">&quot;/app/entrypoint.sh&quot;</span>]</span></code></pre></div>
<h3 id="references-42">References</h3>
<ul>
<li><a
href="https://docs.docker.com/engine/reference/builder/#entrypoint"><code>ENTRYPOINT</code>
reference</a> (docs.docker.com)</li>
<li><a
href="https://docs.docker.com/engine/reference/builder/#cmd"><code>CMD</code>
reference</a> (docs.docker.com)</li>
</ul>
<h2 id="gunicorn">Configure the Gunicorn web server correctly</h2>
<p>In addition to the need to support multiple threads (see above),
Gunicorn uses <code>/tmp</code> as a default location for its internal
heartbeat system for communicating with workers. On Docker that is an
on-disk filesystem, which means the heartbeat system can be slow, making
Gunicorn unresponsive.</p>
<p>You should therefore use <code>/dev/shm</code>, an in-memory
filesystem, as the location for the heartbeat file.</p>
<p>A working configuration where logs also go to stdout might therefore
look like this:</p>
<div class="sourceCode" id="cb100"><pre
class="sourceCode bash"><code class="sourceCode bash"><span id="cb100-1"><a href="#cb100-1" aria-hidden="true" tabindex="-1"></a><span class="co">#!/bin/bash</span></span>
<span id="cb100-2"><a href="#cb100-2" aria-hidden="true" tabindex="-1"></a><span class="bu">set</span> <span class="at">-euo</span> pipefail</span>
<span id="cb100-3"><a href="#cb100-3" aria-hidden="true" tabindex="-1"></a><span class="bu">exec</span> gunicorn <span class="at">--worker-tmp-dir</span> /dev/shm <span class="at">--workers</span><span class="op">=</span>2 <span class="dt">\</span></span>
<span id="cb100-4"><a href="#cb100-4" aria-hidden="true" tabindex="-1"></a>              <span class="at">--threads</span><span class="op">=</span>4 <span class="at">--worker-class</span> gthread <span class="dt">\</span></span>
<span id="cb100-5"><a href="#cb100-5" aria-hidden="true" tabindex="-1"></a>              <span class="at">--log-file</span><span class="op">=</span>- <span class="at">--bind</span> 0.0.0.0:8000 yourmodule:yourapp</span></code></pre></div>
<h3 id="references-43">References</h3>
<ul>
<li><a
href="https://pythonspeed.com/articles/gunicorn-in-docker/">Configuring
Gunicorn for Docker</a> (pythonspeed.com)</li>
<li><a href="https://docs.gunicorn.org/en/stable/index.html">Gunicorn
documentation</a> (docs.gunicorn.org)</li>
</ul>
<h2 id="uwsgi">Configure the uWSGI web server correctly</h2>
<p>Configuring uWSGI is complex.</p>
<p>In addition to the need to support multiple threads and logging to
<code>stdout</code> (see above), uWSGI has some oddities. Just a couple
worth mentioning:</p>
<ol type="1">
<li>By default it will <code>fork()</code> workers without
<code>exec()</code>ing a new Python process, which supposedly saves
memory but actually just makes your Python program much more likely to
crash if you use the wrong library.</li>
<li>The default behavior for environment dictionaries doesn’t match the
WSGI standard.</li>
</ol>
<p>You can fix these issues in the <code>uwsgi.ini</code> file:</p>
<div class="sourceCode" id="cb101"><pre
class="sourceCode default"><code class="sourceCode default"><span id="cb101-1"><a href="#cb101-1" aria-hidden="true" tabindex="-1"></a>[uwsgi]</span>
<span id="cb101-2"><a href="#cb101-2" aria-hidden="true" tabindex="-1"></a>lazy-apps = true</span>
<span id="cb101-3"><a href="#cb101-3" aria-hidden="true" tabindex="-1"></a>wsgi-env-behaviour = holy</span></code></pre></div>
<p>The Tech at Bloomberg article referenced below on uWSGI in production
doesn’t even mention these two, but does mention many other details you
need to configure to get uWSGI working correctly.</p>
<p>While it’s true that uWSGI is fast, in most web applications the
bottleneck is the database, not the web server. And personally I feel
the <code>fork()</code>-without-<code>exec()</code> design choice shows
such bad judgment that I don’t trust uWSGI, but that’s just me; it seems
like it works for some people with enough configuration tweaks.</p>
<p>Technically Gunicorn does an <code>exec()</code> without
<code>fork()</code> too, but it does it much earlier in the process,
before you load your code, so it’s less likely to be an issue. Other
WSGI containers may be better than either.</p>
<h3 id="references-44">References</h3>
<ul>
<li><a
href="https://engineering.ticketea.com/uwsgi-preforking-lazy-apps/">uWSGI
Preforking and Lazy Apps</a> (engineering.ticketea.com)</li>
<li><a
href="https://uwsgi-docs.readthedocs.io/en/latest/articles/WSGIEnvBehaviour.html">uWSGI
env behaviour policies</a> (uwsgi-docs.readthedocs.io)</li>
<li><a
href="https://www.techatbloomberg.com/blog/configuring-uwsgi-production-deployment/">Configuring
uWSGI for Production Deployment</a> (techatbloomberg.com)</li>
</ul>
<h2 id="heroku">Respect the <code>PORT</code> environment variable on
Heroku and Google Cloud Run</h2>
<p>If you’re deploying your image on Heroku or Google Cloud Run, you
will be expected to have your server listen on a port number specified
in the <code>PORT</code> environment variable.</p>
<p>Note that just like binding to <code>0.0.0.0</code>, the port to
listen to is specific to your server. Gunicorn, uWSGI, etc. will all
have different command line or configuration file options–read the
relevant documentation for your server.</p>
<p>As an example, you’ll probably want an <code>entrypoint.sh</code>.
The <code>${PORT:-8080}</code> means that if no <code>$PORT</code>
environment variable was set, a default value of <code>8080</code> will
be used:</p>
<div class="sourceCode" id="cb102"><pre
class="sourceCode bash"><code class="sourceCode bash"><span id="cb102-1"><a href="#cb102-1" aria-hidden="true" tabindex="-1"></a><span class="co">#!/bin/bash</span></span>
<span id="cb102-2"><a href="#cb102-2" aria-hidden="true" tabindex="-1"></a><span class="bu">set</span> <span class="at">-euo</span> pipefail</span>
<span id="cb102-3"><a href="#cb102-3" aria-hidden="true" tabindex="-1"></a><span class="bu">exec</span> python <span class="at">-m</span> http.server <span class="va">${PORT</span><span class="op">:-</span>8080<span class="va">}</span></span></code></pre></div>
<p>The <code>Dockerfile</code> will then use this entrypoint:</p>
<div class="sourceCode" id="cb103"><pre
class="sourceCode dockerfile"><code class="sourceCode dockerfile"><span id="cb103-1"><a href="#cb103-1" aria-hidden="true" tabindex="-1"></a><span class="kw">FROM</span> python:3.9-slim-bullseye</span>
<span id="cb103-2"><a href="#cb103-2" aria-hidden="true" tabindex="-1"></a><span class="kw">COPY</span> entrypoint.sh .</span>
<span id="cb103-3"><a href="#cb103-3" aria-hidden="true" tabindex="-1"></a><span class="kw">ENTRYPOINT</span> [<span class="st">&quot;./entrypoint.sh&quot;</span>]</span></code></pre></div>
<p>Heroku also requires you to run as a non-root user.</p>
<h3 id="references-45">References</h3>
<ul>
<li><a
href="https://devcenter.heroku.com/articles/container-registry-and-runtime#dockerfile-commands-and-runtime">Heroku’s
Docker image requirements</a> (devcenter.heroku.com)</li>
<li><a
href="https://cloud.google.com/run/docs/reference/container-contract">Google
Cloud Run’s Docker image requirements</a> (cloud.google.com)</li>
</ul>
<h1 id="conda">Best practices: Conda</h1>
<h2 id="using-a-conda-environment-in-your-dockerfile">Using a Conda
environment in your <code>Dockerfile</code></h2>
<p>Activating a Conda environment is a much more complex task than
activating a virtualenv, because environment variables won’t work.</p>
<h3 id="activating-with-conda-run">Activating with <code>conda
run</code></h3>
<p>One approach is to use <code>conda run</code>, which allows you to
run a command inside a specified environment:</p>
<pre class="dockerfile"><code>FROM continuumio/miniconda3

WORKDIR /app

# Create the environment:
COPY environment.yml .
# We give it a custom name, &quot;myenv&quot;:
RUN conda env create -n myenv

# Make RUN commands use the new environment:
SHELL [&quot;conda&quot;, &quot;run&quot;, &quot;--no-capture-output&quot;, &quot;-n&quot;, &quot;myenv&quot;, &quot;/bin/bash&quot;, &quot;-c&quot;]

# ... the rest of the build ...

# The code to run when container is started:
COPY run.py .
ENTRYPOINT [&quot;conda&quot;, &quot;run&quot;, &quot;--no-capture-output&quot;, &quot;-n&quot;, &quot;myenv&quot;, &quot;python&quot;, &quot;run.py&quot;]</code></pre>
<p>The <code>--no-capture-output</code> ensures the stdout and stderr
output from Python isn’t hidden by <code>conda run</code>.</p>
<h3 id="activating-with-shell-startup">Activating with shell
startup</h3>
<p>Another approach takes advantage of the fact that
<code>continuumio/miniconda3</code> configures <code>bash</code> to load
the Conda startup shell scripts, and activate the environment in the
entrypoint.</p>
<p>There’s one tricky point here: typically I would recommend using <a
href="#bash">bash strict mode</a>. However, some Conda activation
scripts break with this mode is enabled! So what we have to do is
disable strict mode while activating the script.</p>
<p>We have an <code>entrypoint.sh</code> to run the program:</p>
<div class="sourceCode" id="cb105"><pre
class="sourceCode bash"><code class="sourceCode bash"><span id="cb105-1"><a href="#cb105-1" aria-hidden="true" tabindex="-1"></a><span class="co">#!/bin/bash --login</span></span>
<span id="cb105-2"><a href="#cb105-2" aria-hidden="true" tabindex="-1"></a><span class="co"># The --login ensures the bash configuration is loaded,</span></span>
<span id="cb105-3"><a href="#cb105-3" aria-hidden="true" tabindex="-1"></a><span class="co"># enabling Conda.</span></span>
<span id="cb105-4"><a href="#cb105-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb105-5"><a href="#cb105-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Enable strict mode.</span></span>
<span id="cb105-6"><a href="#cb105-6" aria-hidden="true" tabindex="-1"></a><span class="bu">set</span> <span class="at">-euo</span> pipefail</span>
<span id="cb105-7"><a href="#cb105-7" aria-hidden="true" tabindex="-1"></a><span class="co"># ... Run whatever additional commands ...</span></span>
<span id="cb105-8"><a href="#cb105-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb105-9"><a href="#cb105-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Temporarily disable strict mode and activate conda:</span></span>
<span id="cb105-10"><a href="#cb105-10" aria-hidden="true" tabindex="-1"></a><span class="bu">set</span> +euo pipefail</span>
<span id="cb105-11"><a href="#cb105-11" aria-hidden="true" tabindex="-1"></a><span class="ex">conda</span> activate myenv</span>
<span id="cb105-12"><a href="#cb105-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb105-13"><a href="#cb105-13" aria-hidden="true" tabindex="-1"></a><span class="co"># Re-enable strict mode:</span></span>
<span id="cb105-14"><a href="#cb105-14" aria-hidden="true" tabindex="-1"></a><span class="bu">set</span> <span class="at">-euo</span> pipefail</span>
<span id="cb105-15"><a href="#cb105-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb105-16"><a href="#cb105-16" aria-hidden="true" tabindex="-1"></a><span class="co"># Run the program:</span></span>
<span id="cb105-17"><a href="#cb105-17" aria-hidden="true" tabindex="-1"></a><span class="bu">exec</span> python run.py</span></code></pre></div>
<p>And here’s the <code>Dockerfile</code>:</p>
<pre class="dockerfile"><code>FROM continuumio/miniconda3

WORKDIR /app

# Create the environment:
COPY environment.yml .
# We give it a custom name, &quot;myenv&quot;:
RUN conda env create -n myenv

# Make RUN commands use the new environment:
RUN echo &quot;conda activate myenv&quot; &gt;&gt; ~/.bashrc
SHELL [&quot;/bin/bash&quot;, &quot;--login&quot;, &quot;-c&quot;]

# ... the rest of the build ...

# The code to run when container is started:
COPY run.py entrypoint.sh ./
ENTRYPOINT [&quot;./entrypoint.sh&quot;]</code></pre>
<h3 id="references-46">References</h3>
<ul>
<li><a
href="https://pythonspeed.com/articles/activate-conda-dockerfile/">Activating
a Conda environment in your Dockerfile</a> (pythonspeed.com)</li>
</ul>
<h2 id="conda-environments-require-activation">Conda environments
require activation</h2>
<p>You should not simply call binaries in the Conda <code>bin/</code>
directory without activating the environment. This will work fine for
virtualenvs, but might fail for Conda in certain cases.</p>
<p>This is because Conda packages can include activation scripts that
are necessary for the packages to work correctly. You therefore need to
make sure those activation scripts get called, which is part of what
Conda environment activation does.</p>
<h3 id="references-47">References</h3>
<ul>
<li><a
href="https://docs.conda.io/projects/conda-build/en/latest/resources/activate-scripts.html">Activate
scripts in Conda packages</a></li>
</ul>
<h2 id="reproducible-builds-with-conda-lock">Reproducible builds with
<code>conda-lock</code></h2>
<p>In order to get reproducible builds, you will want to transitively
pin your dependencies. For example, the following
<code>environment.yml</code> will install different packages each
time:</p>
<div class="sourceCode" id="cb107"><pre
class="sourceCode yaml"><code class="sourceCode yaml"><span id="cb107-1"><a href="#cb107-1" aria-hidden="true" tabindex="-1"></a><span class="fu">name</span><span class="kw">:</span><span class="at"> example</span></span>
<span id="cb107-2"><a href="#cb107-2" aria-hidden="true" tabindex="-1"></a><span class="fu">channels</span><span class="kw">:</span></span>
<span id="cb107-3"><a href="#cb107-3" aria-hidden="true" tabindex="-1"></a><span class="at">  </span><span class="kw">-</span><span class="at"> conda-forge</span></span>
<span id="cb107-4"><a href="#cb107-4" aria-hidden="true" tabindex="-1"></a><span class="fu">dependencies</span><span class="kw">:</span></span>
<span id="cb107-5"><a href="#cb107-5" aria-hidden="true" tabindex="-1"></a><span class="at">  </span><span class="kw">-</span><span class="at"> python=3.8</span></span>
<span id="cb107-6"><a href="#cb107-6" aria-hidden="true" tabindex="-1"></a><span class="at">  </span><span class="kw">-</span><span class="at"> numpy</span></span></code></pre></div>
<p>Even if you pinned specific packages, that won’t necessarily install
the same dependencies, you need transitive pinning.</p>
<p><code>conda-lock</code> is a tool that lets you pin packages
transitively, creating what’s known as a spec list, or lockfile, a list
of packages to download. The additional benefit of a spec list is that
you can skip the slow “Solving dependencies” stage you get when you
install an <code>environment.yml</code>.</p>
<p>There are two steps. First, you pin the dependencies, which you can
do on any computer with any operating system (Windows or macOS work
too). Note that you can also install <code>conda-lock</code> using
<code>pip</code>.</p>
<pre><code>$ conda install -c conda-forge conda-lock
$ conda-lock -f environment.yml -p linux-64</code></pre>
<p>This will generate a <code>conda-linux-64.lock</code> file. Any time
you want to update your dependencies you’ll need to regenerate this file
by rerunning <code>conda-lock</code>, and it will get the latest version
of dependencies in <code>environment.yml</code>.</p>
<p>You can then use this file to create a Conda environment in your
Docker image, using a slightly different method than what you’d use with
an <code>environment.yml</code>:</p>
<pre class="dockerfile"><code>FROM continuumio/miniconda3

COPY conda-linux-64.lock .
# Create environment called &quot;yourenv&quot;:
RUN conda create --name yourenv --file conda-linux-64.lock</code></pre>
<p>One limitation of this approach is that <code>pip</code> packages
aren’t supported by <code>conda-lock</code> quite yet. The next major
release of <code>conda-lock</code> will include support for
<code>pip</code> packages (at the time of writing the current release
was v0.13.2).</p>
<p>In the interim:</p>
<ul>
<li>List required <code>pip</code> packages outside of
<code>environment.yml</code>, in a <code>requirements.txt</code> for
example.</li>
<li>Install <code>pip</code> packages separately, with
<code>pip</code>.</li>
<li>Use the <a href="#pin-python"><code>pip</code>-specific package
pinning mechanisms</a> to keep them locked to specific versions.</li>
</ul>
<h3 id="references-48">References</h3>
<ul>
<li><a
href="github.com/conda-incubator/conda-lock/"><code>conda-lock</code></a>
(github.com)</li>
<li><a
href="https://pythonspeed.com/articles/conda-dependency-management/">Reproducible
and upgradable Conda environments: dependency management with
conda-lock</a> (pythonspeed.com)</li>
</ul>
<h2 id="make-conda-images-smaller-with-conda-clean">Make Conda images
smaller with <code>conda clean</code></h2>
<p>By default, Conda will cache a variety of files: index files,
downloaded packages, and so on. If you don’t anticipate doing any
further installs, you can delete all these files using <code>conda
run</code> to save some space.</p>
<p>Make sure to do this in the same <code>RUN</code> command where you
install the packages, to ensure the files aren’t saved in a previous
layer.</p>
<pre class="dockerfile"><code>FROM continuumio/miniconda3

COPY environment.yml .
RUN conda env create -n myenv &amp;&amp; \
    conda clean -afy</code></pre>
<h3 id="references-49">References</h3>
<ul>
<li><a
href="https://docs.conda.io/projects/conda/en/latest/commands/clean.html"><code>conda
clean</code> documentation</a> (docs.conda.io)</li>
</ul>
<h2 id="make-conda-images-smaller-by-using-openblas">Make Conda images
smaller by using OpenBLAS</h2>
<p>If you’re using NumPy, or packages like Pandas that depends on NumPy,
you need to choose which BLAS linear algebra library to use. The two
choices are MKL, which is often faster, and OpenBLAS, which uses much
less disk space.</p>
<p>Switching to OpenBLAS depends on where you’re installing packages
from:</p>
<ul>
<li>The packages you install with <code>pip</code> use OpenBLAS.</li>
<li>If you’re installing packages from the Conda-Forge channel, OpenBLAS
is the default, so you don’t need to take any further action to get
smaller images.</li>
<li>If you’re using the default Anaconda packages, MKL is the default,
and if you want OpenBLAS you’ll want to add <code>nomkl</code> to the
packages you’re installing. This will shave something like 700MB off
your Docker image!</li>
</ul>
<div class="sourceCode" id="cb111"><pre
class="sourceCode yaml"><code class="sourceCode yaml"><span id="cb111-1"><a href="#cb111-1" aria-hidden="true" tabindex="-1"></a><span class="fu">name</span><span class="kw">:</span><span class="at"> example</span></span>
<span id="cb111-2"><a href="#cb111-2" aria-hidden="true" tabindex="-1"></a><span class="fu">dependencies</span><span class="kw">:</span></span>
<span id="cb111-3"><a href="#cb111-3" aria-hidden="true" tabindex="-1"></a><span class="at">  </span><span class="kw">-</span><span class="at"> python=3.8</span></span>
<span id="cb111-4"><a href="#cb111-4" aria-hidden="true" tabindex="-1"></a><span class="at">  </span><span class="kw">-</span><span class="at"> numpy</span></span>
<span id="cb111-5"><a href="#cb111-5" aria-hidden="true" tabindex="-1"></a><span class="at">  </span><span class="kw">-</span><span class="at"> nomkl</span></span></code></pre></div>
<h3 id="references-50">References</h3>
<ul>
<li><a
href="https://docs.anaconda.com/mkl-optimizations/#uninstalling-mkl">Omitting
<code>mkl</code> in Anaconda</a> (docs.anaconda.com)</li>
<li><a href="https://jcristharif.com/conda-docker-tips.html">Smaller
Docker images with Conda</a> (jcristharif.com)</li>
</ul>
<h2
id="even-smaller-conda-images-with-conda-pack-and-multi-stage-builds">Even
smaller Conda images with <code>conda-pack</code> and multi-stage
builds</h2>
<p>In addition to the Conda environment you create, by default your
Conda-based Docker image will ship with the base environment, including
a full install of Python. By using <code>conda-pack</code>, a tool that
lets you convert a Conda environment into a standalone directory, you
can drop that extra environment and any other unneeded files from the
final runtime image.</p>
<pre class="dockerfile"><code># The build-stage image:
FROM continuumio/miniconda3 AS build

# Install the package as normal:
COPY environment.yml .
RUN conda env create -n myenv

# Install conda-pack:
RUN conda install -c conda-forge conda-pack

# Use conda-pack to create a standalone enviornment
# in /venv:
RUN conda-pack -n myenv -o /tmp/env.tar &amp;&amp; \
  mkdir /venv &amp;&amp; cd /venv &amp;&amp; tar xf /tmp/env.tar &amp;&amp; \
  rm /tmp/env.tar

# We&#39;ve put venv in same path it&#39;ll be in final image,
# so now fix up paths:
RUN /venv/bin/conda-unpack


# The runtime-stage image; we can use Debian as the
# base image since the Conda env also includes Python
# for us.
FROM debian:bullseye AS runtime

# Copy /venv from the previous stage:
COPY --from=build /venv /venv

# When image is run, run the code with the environment
# activated:
SHELL [&quot;/bin/bash&quot;, &quot;-c&quot;]
ENTRYPOINT source /venv/bin/activate &amp;&amp; \
           python -c &quot;import numpy; print(&#39;success!&#39;)&quot;</code></pre>
<p><code>conda-pack</code> may have issues if you are installing many
packages with <code>pip</code>.</p>
<h3 id="references-51">References</h3>
<ul>
<li><a
href="https://pythonspeed.com/articles/conda-docker-image-size/">Shrink
your Conda Docker images with conda-pack</a> (pythonspeed.com)</li>
<li><a
href="https://conda.github.io/conda-pack/"><code>conda-pack</code>
documentation</a> (conda.github.io)</li>
</ul>
<h2 id="faster-installs-with-mamba">Faster installs with Mamba</h2>
<p>Mamba is a re-implementation of the Conda package manager that runs
much faster. It also supports the same command-line options as Conda, so
once you’ve installed it you just need to replace <code>conda</code>
with <code>mamba</code>:</p>
<pre class="dockerfile"><code>FROM continuumio/miniconda3
RUN conda install -c conda-forge mamba
# Now proceed as normal, just using mamba instead:
RUN mamba env create -n myenv</code></pre>
<h3 id="references-52">References</h3>
<ul>
<li><a href="https://github.com/mamba-org/mamba">Mamba</a>
(github.com)</li>
</ul>
<h2 id="security-scans-for-conda-environments">Security scans for Conda
environments</h2>
<p>Conda packages can include more than just Python packages, so if you
want to scan your dependencies for security vulnerabilities you need a
security scanner that understands this. One such tool is Jake.</p>
<p>Assuming you have an environment named <code>myenv</code> you create
earlier in the <code>Dockerfile</code>, during the build, you can scan
for vulnerabilities like so:</p>
<pre class="dockerfile"><code># ... create a Conda env called &quot;myenv&quot; with your packages ...

# Make virtualenv for Jake, run a security scan, and then clean up:
RUN python3 -m venv /tmp/venv-jake &amp;&amp; \
    /tmp/venv-jake/bin/pip install jake &amp;&amp; \
    conda run -n myenv conda list | (set -e &amp;&amp; \
    /tmp/venv-jake/bin/jake ddt -c &amp;&amp; rm -rf /tmp/venv-jake)</code></pre>
<p>Note that Jake support for Conda-Forge may be limited.</p>
<h3 id="references-53">References</h3>
<ul>
<li><a
href="https://pythonspeed.com/articles/conda-security-scans/">Scanning
your Conda environment for security vulnerabilities</a>
(pythonspeed.com)</li>
<li><a href="https://github.com/sonatype-nexus-community/jake">Jake</a>
(github.com)</li>
<li><a href="https://github.com/sonatype/ossindex-public/issues/27">Jake
+ Conda-Forge issue</a> (github.com)</li>
</ul>
<h1 id="pipenv">Best practices: Pipenv</h1>
<h2 id="install-pipenv-separately-from-your-application-code">Install
Pipenv separately from your application code</h2>
<p>If you’re going to use Pipenv inside your Docker build, you need to
install it. Since it has a variety of dependencies, you don’t want to
install it into the same place as your code, in case the dependencies
conflict.</p>
<p>As you’ll see in the next best practice, you can either:</p>
<ol type="1">
<li>Install Pipenv as a system package, and have Pipenv install
dependencies in a virtualenv.</li>
<li>Export dependencies and install them using <code>pip</code>. If you
follow this path, you could just uninstall Pipenv once you’ve done the
export.</li>
</ol>
<h3 id="references-54">References</h3>
<ul>
<li><a href="https://pipenv.pypa.io/en/latest/"><code>pipenv</code>
documentation</a> (pipenv.pypa.io)</li>
</ul>
<h2 id="installing-dependencies-inside-a-container">Installing
dependencies inside a container</h2>
<p>There are two methods to install your dependencies inside your Docker
build using Pipenv. The first method involves exporting the dependencies
to a separate <code>requirements.txt</code> file, and then relying on
<code>pip</code> to install them:</p>
<pre class="dockerfile"><code>FROM python:3.9-slim-bullseye
RUN pip install pipenv

COPY Pipfile* .
RUN pipenv lock --keep-outdated --requirements &gt; /tmp/requirements.txt

RUN pip install -r /tmp/requirements.txt

# ... install application code, set entrypoint ...</code></pre>
<p>The <code>--keep-outdated</code> makes sure you use the pinned
versions in the <code>Pipfile.lock</code>, and don’t update the lock
file as a side-effect of exporting the packages.</p>
<p>This mechanism does not validate downloaded packages’ hashes, a
potential security risk; see the issue linked in the references for
details.</p>
<p>The other method involves creating a virtualenv and installing the
packages there using Pipenv. Pipenv wants to only install in a
virtualenv, and creating our own means it’s exactly where we want
it:</p>
<pre class="dockerfile"><code>FROM python:3.9-slim-bullseye
RUN pip install pipenv

# Create and activate virtualenv:
RUN python3 -m venv /venv
ENV PATH=/venv/bin:$PATH
ENV VIRTUAL_ENV=/venv

COPY Pipfile.lock .

# Install inside virtualenv:
RUN pipenv install --keep-outdated --ignore-pipfile</code></pre>
<p>The two options to <code>pipenv install</code> allow us to only copy
in the <code>Pipfile.lock</code>.</p>
<h3 id="references-55">References</h3>
<ul>
<li><a
href="https://pipenv.pypa.io/en/latest/cli/#pipenv-lock"><code>pipenv
lock</code></a> (pipenv.pypa.io)</li>
<li><a href="https://github.com/pypa/pipenv/issues/4189">Issue:
Generating hashes in <code>pipenv lock</code></a> (github.com)</li>
<li><a
href="https://pipenv.pypa.io/en/latest/cli/#pipenv-install"><code>pipenv
install</code></a> (pipenv.pypa.io)</li>
</ul>
<h1 id="poetry">Best practices: Poetry</h1>
<h2 id="install-poetry-separately-from-your-application-code">Install
Poetry separately from your application code</h2>
<p>If you’re going to use Poetry inside your Docker build, you need to
install it. Since it has a variety of dependencies, you don’t want to
install it into the same place as your code, in case the dependencies
conflict.</p>
<p>Some options:</p>
<ul>
<li>If you’re installing your code into a virtualenv, you can just
install Poetry as a system package and it won’t conflict with the
virtualenv’s libraries.</li>
<li>You can use Poetry’s installer to install it into its own isolated
directory.</li>
</ul>
<p>Here’s how you do the latter:</p>
<pre class="dockerfile"><code>FROM python:3.9-slim-bullseye

RUN apt-get update &amp;&amp; apt-get install -y curl
ENV POETRY_HOME=/tmp/poetry
RUN curl -sSL https://raw.githubusercontent.com/python-poetry/poetry/master/get-poetry.py | python - --no-modify-path
ENV PATH=$POETRY_HOME/bin:$PATH</code></pre>
<p>The <code>--no-modify-path</code> keeps Poetry from modifying
<code>.profile</code> in order to modify the <code>PATH</code>; instead,
we do that ourselves in the <code>Dockerfile</code>.</p>
<h3 id="references-56">References</h3>
<ul>
<li><a href="https://python-poetry.org/docs/#installation">Installing
<code>poetry</code></a> (python-poetry.org)</li>
</ul>
<h2 id="installing-outside-of-poetrys-managed-virtualenvs">Installing
outside of Poetry’s managed virtualenvs</h2>
<p>By default, Poetry will install code in a virtualenv that it creates
and manages. If you want to install either in the normal system packages
location, or in a virtualenv of your choosing, add this to your
<code>Dockerfile</code> before installing any packages:</p>
<pre class="dockerfile"><code>RUN poetry config virtualenvs.create false</code></pre>
<p>This will ensure <code>poetry install</code> doesn’t create its own
virtual environment. Instead, it will install in either the normal
system packages location, or the current activated virtualenv if there
is one.</p>
<h3 id="references-57">References</h3>
<ul>
<li><a
href="https://python-poetry.org/docs/configuration/#virtualenvscreate-boolean"><code>poetry's</code>
<code>virtualenv.create</code></a> (python-poetry.org)</li>
</ul>
<h2 id="poetry-dev-depencies">Don’t install development dependencies
(unless you want to)</h2>
<p>By default Poetry will install development dependencies, things like
<code>flake8</code> that you’ve configured in
<code>pyproject.toml</code> as only being required for development. To
disable installing dev dependencies, use the <code>--no-dev</code>
option.</p>
<pre class="dockerfile"><code>RUN poetry install --no-dev</code></pre>
<h3 id="references-58">References</h3>
<ul>
<li><a href="https://python-poetry.org/docs/cli/#install"><code>poetry
install</code> reference</a> (python-poetry.org)</li>
</ul>
<h2 id="poetry-dependencies-first">Speed up rebuilds by installing
dependencies separately</h2>
<p>For Poetry, you can install dependencies separately to get better
build caching by first doing <code>poetry install --no-root</code> to
just install the dependencies:</p>
<pre class="dockerfile"><code>FROM python:3.9-slim-bullseye

# Install Poetry:
RUN apt-get update &amp;&amp; apt-get install -y curl
ENV POETRY_HOME=/tmp/poetry
RUN curl -sSL https://raw.githubusercontent.com/python-poetry/poetry/master/get-poetry.py | python - --no-modify-path
ENV PATH=$POETRY_HOME/bin:$PATH

# Don&#39;t create virtualenvs:
RUN poetry config virtualenvs.create false

# Copy in the config files:
WORKDIR /myapp
COPY pyproject.toml poetry.lock ./

# Install only dependencies:
RUN poetry install --no-dev --no-root

# Copy in everything else and install app code:
COPY . .
RUN poetry install --no-dev</code></pre>
<p><strong>Important:</strong> When your application code is installed,
it is <em>not</em> copied in, as would be the case when you install some
other package like Flask, Django, or NumPy. Instead, Poetry creates a
link to the directory where your code resides; in the example above,
your code is expected to stay in <code>/myapp</code>. <strong>That means
you can’t delete the original code even though you’ve installed
it.</strong></p>
<p>If you do want to actually install the code in a way where the code
is copied in, instead of using <code>poetry install</code> to install
your code, do <code>poetry build</code> to create a wheel. You can then
<code>pip install</code> the wheel—you can find it in the
<code>dist/</code> directory of your project.</p>
<p>Or, you can use a so-called “PEP 517” install. Basically that means
you install via <code>pip</code>, but <code>pip</code> then figures out
from <code>pyproject.toml</code> how to get the information it needs
from Poetry. As a side-effect, it also builds your application code as a
wheel and installs it, so you can then delete the original
directory.</p>
<p>If you created your <code>pyproject.toml</code> with a sufficiently
new version of Poetry and <code>poetry new/init</code>, this will
already be set up. Otherwise make sure <code>pyproject.toml</code> has
the following section (see the Poetry docs linked below for
details):</p>
<div class="sourceCode" id="cb121"><pre
class="sourceCode toml"><code class="sourceCode toml"><span id="cb121-1"><a href="#cb121-1" aria-hidden="true" tabindex="-1"></a><span class="kw">[</span><span class="dt">build-system</span><span class="kw">]</span></span>
<span id="cb121-2"><a href="#cb121-2" aria-hidden="true" tabindex="-1"></a><span class="dt">requires</span> <span class="op">=</span> <span class="op">[</span><span class="st">&quot;poetry_core&gt;=1.0.0&quot;</span><span class="op">]</span></span>
<span id="cb121-3"><a href="#cb121-3" aria-hidden="true" tabindex="-1"></a><span class="dt">build-backend</span> <span class="op">=</span> <span class="st">&quot;poetry.core.masonry.api&quot;</span></span></code></pre></div>
<p>Then, you can change the last part of the <code>Dockerfile</code> to
install a different way:</p>
<pre class="dockerfile"><code># ...
# Install Poetry and your dependencies as above.
# ...

# Copy in everything else and install app code:
COPY . .
# Instead of `poetry install`:
RUN pip install .</code></pre>
<h3 id="references-59">References</h3>
<ul>
<li><a href="https://python-poetry.org/docs/cli/#install"><code>poetry
install</code> documentation</a> (python-poetry.org)</li>
<li><a href="https://python-poetry.org/docs/cli/#build"><code>poetry
build</code> documentation</a> (python-poetry.org)</li>
<li><a
href="https://python-poetry.org/docs/pyproject/#poetry-and-pep-517">Poetry
and PEP 517</a> (python-poetry.org)</li>
</ul>
<h2 id="poetry-version-vs-caching">Application version changes can lead
to slow rebuilds</h2>
<p>If you’re using Poetry in its default configuration, your
package/application version is stored in <code>pyproject.toml</code>.
That means that every time you change the version,
<code>pyproject.toml</code> will change. Which means you’ll have to
reinstall dependencies from scratch when you rebuild your Docker image,
even if the dependencies haven’t changed: the new
<code>pyproject.toml</code> will invalidate the Docker build cache.</p>
<p>How can you fix this?</p>
<p>First, you can choose not to care. If you don’t update application
versions very often, re-installing all dependencies on a image build
won’t happen that often.</p>
<p>Second, you can choose not to rely on Poetry’s application
versioning. Either you can choose not to set versions at all for your
package, or you can use something like the
<code>poetry-dynamic-versioning</code> package that lets you set
versions from Git tags (or some other version control). This introduces
a dependency on your Git repository during Docker build time, which has
its own set of issues (<a href="#git-dockerignore">ref</a>).</p>
<p>Third, you can install dependencies via <code>pip</code>. Outside of
your Docker build, in your build script, you can run <code>poetry
export</code> to dump the dependencies to a
<code>requirements.txt</code> file:</p>
<div class="sourceCode" id="cb123"><pre
class="sourceCode bash"><code class="sourceCode bash"><span id="cb123-1"><a href="#cb123-1" aria-hidden="true" tabindex="-1"></a><span class="co">#!/bin/bash</span></span>
<span id="cb123-2"><a href="#cb123-2" aria-hidden="true" tabindex="-1"></a><span class="ex">poetry</span> export <span class="at">-o</span> requirements.txt</span>
<span id="cb123-3"><a href="#cb123-3" aria-hidden="true" tabindex="-1"></a><span class="ex">docker</span> image build <span class="at">-t</span> myimage .</span></code></pre></div>
<p>In your <code>Dockerfile</code> you can then install dependencies
using <code>pip</code>:</p>
<pre class="dockerfile"><code>FROM python:3.9-slim-bullseye
COPY requirements.txt .
RUN pip install -r requirements.txt

# etc..</code></pre>
<h3 id="references-60">References</h3>
<ul>
<li><a href="https://python-poetry.org/docs/cli/#install"><code>poetry
export</code> documentation</a> (python-poetry.org)</li>
</ul>
<h1 id="multi-stage">Best practices: Multi-stage builds</h1>
<h2 id="omit-build-dependencies-from-your-runtime-image">Omit build
dependencies from your runtime image</h2>
<p>If you need to use a compiler to build some of your packages,
installing the compiler will make your image much bigger. But that
compiler isn’t necessary when running the image, it’s only necessary
during the build phase.</p>
<p>One solution is multi-stage builds: create a series of images, the
first with all the packages necessary to build your image, and the
second with only the runtime packages installed. As you build the
runtime image, you can copy files from the build image:</p>
<pre class="dockerfile"><code># This is the first image:
FROM ubuntu:18.04 AS compile-image
RUN apt-get update
RUN apt-get install -y --no-install-recommends gcc build-essential

WORKDIR /root
COPY hello.c .
RUN gcc -o helloworld hello.c

# This is the second and final image; it copies the compiled
# binary over but starts from the base ubuntu:18.04 image.
FROM ubuntu:18.04 AS runtime-image

COPY --from=compile-image /root/helloworld .
CMD [&quot;./helloworld&quot;]</code></pre>
<p>Notice that each step has a name, <code>compile-image</code> and
<code>runtime-image</code>.</p>
<p>If you just build normally, both stages will get built but any tags
will get set on the final stage, in this case
<code>runtime-image</code>. But you can also build specific stages, so
if for example you to build the <code>compile-image</code> and then give
it a tag you can do:</p>
<div class="sourceCode" id="cb126"><pre
class="sourceCode bash"><code class="sourceCode bash"><span id="cb126-1"><a href="#cb126-1" aria-hidden="true" tabindex="-1"></a><span class="ex">$</span> docker build <span class="at">--target</span> compile-image <span class="at">-t</span> myimage:compile-stage .</span></code></pre></div>
<p>In Docker Compose, you can also have <code>build</code> services
target a particular stage.</p>
<h3 id="references-61">References</h3>
<ul>
<li><a
href="https://pythonspeed.com/articles/smaller-python-docker-images/">Multi-stage
builds: Smaller images for compiled code</a> (pythonspeed.com) motivates
the problem</li>
<li><a
href="https://docs.docker.com/develop/develop-images/multistage-build/">Multi-stage
builds</a> (docs.docker.com)</li>
<li><a
href="https://docs.docker.com/compose/compose-file/#target">Targeting
multi-stage builds in Docker Compose</a> (docs.docker.com)</li>
</ul>
<h2 id="use-a-virtualenv-to-make-copying-across-stages-easier">Use a
virtualenv to make copying across stages easier</h2>
<p>If you’re creating a multi-stage Python Docker image, you need to
figure out how to copy images from the build image to the runtime image.
Python packages can install files in a variety of locations, so this can
be tricky.</p>
<p>One way to do that is by installing everything in a virtualenv:</p>
<pre class="dockerfile"><code>FROM python:3.9-slim-bullseye AS compile-image
RUN apt-get update
RUN apt-get install -y --no-install-recommends build-essential gcc

RUN python -m venv /opt/venv
# Make sure we use the virtualenv:
ENV PATH=&quot;/opt/venv/bin:$PATH&quot;

COPY requirements.txt .
RUN pip install -r requirements.txt

COPY setup.py .
COPY myapp/ .
RUN pip install .

FROM python:3.9-slim-bullseye AS build-image
COPY --from=compile-image /opt/venv /opt/venv

# Make sure we use the virtualenv:
ENV PATH=&quot;/opt/venv/bin:$PATH&quot;
CMD [&#39;myapp&#39;]</code></pre>
<p>Another way is to use <code>pip install --user</code>, and then you
just need to copy over the <code>~/.local</code> directory where user
installs happen.</p>
<h3 id="references-62">References</h3>
<ul>
<li><a
href="https://pythonspeed.com/articles/multi-stage-docker-python/">Multi-stage
builds, Python specifics: <code>virtualenv</code>, <code>-–user</code>,
and other methods</a> (pythonspeed.com)</li>
</ul>
<h2 id="avoid-unnecessary-complete-rebuilds">Avoid unnecessary complete
rebuilds</h2>
<p>One problem with multi-stage builds is rebuilds. If you only stored
the final runtime image, you won’t have the build/compile-stage image
available when you rebuild. Which means you won’t get any benefit from
Docker build caching: every rebuild will rebuild from scratch.</p>
<p>The solution is to tag both images, the compile-stage and the
runtime-stage, and to store them both:</p>
<div class="sourceCode" id="cb128"><pre
class="sourceCode bash"><code class="sourceCode bash"><span id="cb128-1"><a href="#cb128-1" aria-hidden="true" tabindex="-1"></a><span class="co">#!/bin/bash</span></span>
<span id="cb128-2"><a href="#cb128-2" aria-hidden="true" tabindex="-1"></a><span class="bu">set</span> <span class="at">-euo</span> pipefail</span>
<span id="cb128-3"><a href="#cb128-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Pull the latest version of the image, in order to</span></span>
<span id="cb128-4"><a href="#cb128-4" aria-hidden="true" tabindex="-1"></a><span class="co"># populate the build cache:</span></span>
<span id="cb128-5"><a href="#cb128-5" aria-hidden="true" tabindex="-1"></a><span class="ex">docker</span> pull itamarst/helloworld:compile-stage <span class="kw">||</span> <span class="fu">true</span></span>
<span id="cb128-6"><a href="#cb128-6" aria-hidden="true" tabindex="-1"></a><span class="ex">docker</span> pull itamarst/helloworld:latest        <span class="kw">||</span> <span class="fu">true</span></span>
<span id="cb128-7"><a href="#cb128-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb128-8"><a href="#cb128-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Build the runtime stage, using cached compile stage:</span></span>
<span id="cb128-9"><a href="#cb128-9" aria-hidden="true" tabindex="-1"></a><span class="ex">docker</span> build <span class="at">--target</span> runtime-image <span class="dt">\</span></span>
<span id="cb128-10"><a href="#cb128-10" aria-hidden="true" tabindex="-1"></a>       <span class="at">--cache-from</span><span class="op">=</span>itamarst/helloworld:compile-stage <span class="dt">\</span></span>
<span id="cb128-11"><a href="#cb128-11" aria-hidden="true" tabindex="-1"></a>       <span class="at">--cache-from</span><span class="op">=</span>itamarst/helloworld:latest <span class="dt">\</span></span>
<span id="cb128-12"><a href="#cb128-12" aria-hidden="true" tabindex="-1"></a>       <span class="at">--tag</span> itamarst/helloworld:latest .</span>
<span id="cb128-13"><a href="#cb128-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb128-14"><a href="#cb128-14" aria-hidden="true" tabindex="-1"></a><span class="co"># Build the compile stage so we tag it. In practice this will</span></span>
<span id="cb128-15"><a href="#cb128-15" aria-hidden="true" tabindex="-1"></a><span class="co"># all come from the local cache and run quickly.</span></span>
<span id="cb128-16"><a href="#cb128-16" aria-hidden="true" tabindex="-1"></a><span class="ex">docker</span> build <span class="at">--target</span> compile-image <span class="dt">\</span></span>
<span id="cb128-17"><a href="#cb128-17" aria-hidden="true" tabindex="-1"></a>       <span class="at">--cache-from</span><span class="op">=</span>itamarst/helloworld:latest <span class="dt">\</span></span>
<span id="cb128-18"><a href="#cb128-18" aria-hidden="true" tabindex="-1"></a>       <span class="at">--cache-from</span><span class="op">=</span>itamarst/helloworld:compile-stage <span class="dt">\</span></span>
<span id="cb128-19"><a href="#cb128-19" aria-hidden="true" tabindex="-1"></a>       <span class="at">--tag</span> itamarst/helloworld:compile-stage .</span>
<span id="cb128-20"><a href="#cb128-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb128-21"><a href="#cb128-21" aria-hidden="true" tabindex="-1"></a><span class="co"># Push the new versions:</span></span>
<span id="cb128-22"><a href="#cb128-22" aria-hidden="true" tabindex="-1"></a><span class="ex">docker</span> push itamarst/helloworld:compile-stage</span>
<span id="cb128-23"><a href="#cb128-23" aria-hidden="true" tabindex="-1"></a><span class="ex">docker</span> push itamarst/helloworld:latest</span></code></pre></div>
<p>We build the runtime stage first to enable parallelism when using
BuildKit; see the next best practice.</p>
<h3 id="references-63">References</h3>
<ul>
<li><a
href="https://pythonspeed.com/articles/faster-multi-stage-builds/">Multi-stage
builds: Why your build is surprisingly slow, and how to speed it up</a>
(pythonspeed.com)</li>
</ul>
<h2 id="optional-use-buildkit-for-faster-builds">Optional: Use BuildKit
for faster builds</h2>
<p>BuildKit is a new build backend for Docker. Among other features, if
you’re doing multi-stage builds it can build the different stages in
parallel, insofar as this is possible. That means BuildKit can make
multi-stage builds run faster, especially if you build the runtime stage
first (see above).</p>
<p>You can enable BuildKit by adding:</p>
<div class="sourceCode" id="cb129"><pre
class="sourceCode bash"><code class="sourceCode bash"><span id="cb129-1"><a href="#cb129-1" aria-hidden="true" tabindex="-1"></a><span class="bu">export</span> <span class="va">DOCKER_BUILDKIT</span><span class="op">=</span>1</span></code></pre></div>
<p>to the top of your build script.</p>
<h2 id="optional-optimizing-image-size-even-further">Optional:
Optimizing image size even further</h2>
<p>A multi-stage build gives you the opportunity to make your runtime
image even smaller, by deleting arbitrary files from the build stage.
Remember that because of layering, deleting files with a
<code>RUN</code> command won’t make the build-stage image smaller. It
will however let you copy over less files.</p>
<p>Some examples of files you might want to delete:</p>
<ul>
<li>Source code in C or another compiled language that is in a directory
you plan to copy over, now that you’re done compiling it.</li>
<li>Likewise, any build artifacts.</li>
<li><code>.pyc</code> files, but only if you’re OK with slower
startup.</li>
<li>JavaScript source map files, if you don’t care about the additional
debugging they enable.</li>
</ul>
<p>Your <code>Dockerfile</code> might therefore look like this:</p>
<pre class="dockerfile"><code>FROM python:3.9-slim-bullseye AS compile-image

RUN python -m venv /venv
ENV PATH=&quot;/venv/bin:$PATH&quot;
COPY requirements.txt .
RUN pip install -r requirements.txt

WORKDIR /app
COPY . .
RUN python setup.py build_ext --inplace

# Delete C, Cython, map files and build artifacts we 
# don&#39;t want in the runtime image:
RUN rm -f `find . -iname &quot;*.c&quot;` &amp;&amp; \
    rm -f `find . -iname &quot;*.pyx&quot;` &amp;&amp; \
    rm -f `find /venv -iname &quot;*.js.map&quot;` &amp;&amp; \
    rm -f build/

FROM python:3.9-slim-bullseye AS build-image
COPY --from=compile-image /opt/venv /opt/venv
COPY --from=compile-image /app /app

# ... etc. ...</code></pre>
<h1 class="unnumbered" id="some-final-recommendations">Some final
recommendations</h1>
<p>There are many best practices, and you won’t need all of them,
especially not to begin with. So start by following <a href="#plan">the
plan in the first chapter</a> and see how much you actually need in
practice.</p>
<p>If you’d like a working setup implementing many of these best
practices, either as an example or as a basis for your packaging, you
can also purchase <a
href="https://pythonspeed.com/products/pythoncontainer/">a
production-ready template for Pip, Poetry, or Pipenv-based projects</a>
that will allow you to get going in just an hour or two. You can also
purchase <a href="https://pythonspeed.com/products/condacontainer/">a
template designed for Conda</a>. As a purchaser of the handbook, you can
use the discount code <code>BPQUICKSTART</code> to get a 15% discount
off the templates.</p>
<p>And as always, if you have any questions or suggestions, please email
me at <a
href="mailto:itamar@pythonspeed.com">itamar@pythonspeed.com</a>.</p>
<p>—Itamar Turner-Trauring</p>
<h1 class="unnumbered" id="changelog">Changelog</h1>
<h3 class="unnumbered" id="january-26-2022">January 26, 2022</h3>
<ul>
<li>New best practices:
<ul>
<li>Security scans for Conda with Jake.</li>
<li>CentOS alternatives.</li>
</ul></li>
<li>Fix <code>conda activate</code> in the face of scripts with
semi-broken activation scripts, by disabling bash strict mode.</li>
<li>Update with some changes to <code>trivy</code> security
scanner.</li>
<li>Removed the <code>safety</code> security scanner, since the
non-commercial usage restriction is too limiting.</li>
<li>Note that <code>conda-lock</code> will support pip packages in the
near future.</li>
</ul>
<h3 class="unnumbered" id="august-31-2021">August 31, 2021</h3>
<ul>
<li>Debian “Bullseye” 11 has replaced Buster as the stable Debian of
choice.</li>
<li>Latest BuildKit backend is now v1.3.</li>
<li>Documented <code>tini -g</code>.</li>
<li>Added note on dealing with <code>pip</code> packages when using
<code>conda-lock</code>.</li>
<li>New best practices:
<ul>
<li>Conda environments require activation.</li>
<li>Faster Conda installs with Mamba.</li>
</ul></li>
</ul>
<h3 class="unnumbered" id="june-7-2021">June 7, 2021</h3>
<ul>
<li>Clarified <code>STOPSIGNAL</code> usage.</li>
</ul>
<h3 class="unnumbered" id="june-1-2021">June 1, 2021</h3>
<ul>
<li>Added best practices:
<ul>
<li>Don’t leak secret files.</li>
<li>Don’t leak runtime secrets.</li>
<li>Don’t store temporary files in layers.</li>
<li>Get rid of unneeded files.</li>
</ul></li>
<li>Added <code>--nodocs</code> to <code>dnf</code> instructions, for
even smaller images.</li>
<li>Added an alternative method for activating Conda environments.</li>
</ul>
<h3 class="unnumbered" id="march-16-2021">March 16, 2021</h3>
<ul>
<li>When using BuildKit you should <em>always</em> set <code>--build-arg
BUILDKIT_INLINE_CACHE=1</code> if you want <code>--cache-from</code> to
work.</li>
<li>Noted that official Python Docker image is slower than Ubuntu.</li>
</ul>
<h3 class="unnumbered" id="february-8-2021">February 8, 2021</h3>
<ul>
<li>The Quickstart has been renamed to the Handbook; at 100 pages, it’s
getting more than just introductory.</li>
<li>New best practices:
<ul>
<li>Installing packages with <code>pipenv</code> without exporting.</li>
<li>Keeping <code>pipenv</code> separate from application code.</li>
<li>Recommend <code>docker build --label</code> over
<code>LABEL</code>.</li>
</ul></li>
</ul>
<h3 class="unnumbered" id="february-3-2021">February 3, 2021</h3>
<ul>
<li>Removed references to CentOS, as it is no longer a stable base
image.</li>
<li>Added link to RedHat’s Docker base image.</li>
<li>Noted Podman can be used with BuildKit.</li>
<li>Noted need for <code>--keep-outdated</code> when using <code>pipenv
lock</code>.</li>
<li>Documented how to make Docker Compose use BuildKit.</li>
<li>New best practices:
<ul>
<li>Caching package downloads across builds using BuildKit.</li>
<li>Avoiding dev dependency installs in Poetry.</li>
<li>Conda dependency locking using <code>conda-lock</code>.</li>
</ul></li>
</ul>
<h3 class="unnumbered" id="january-28-2021">January 28, 2021</h3>
<ul>
<li>Noted need to pass <code>--no-capture-output</code> to <code>conda
run</code> (thanks to Joe Selvik).</li>
</ul>
<h3 class="unnumbered" id="december-17-2020">December 17, 2020</h3>
<p>Updates for Docker 20.10 and a stable BuildKit.</p>
<ul>
<li>Added new chapter covering different Docker releases and
BuildKit.</li>
<li>Switched all BuildKit examples to use the new stable
<code>docker/dockerfile:1.2</code> version.</li>
<li>Documented getting BuildKit secrets from environment variables.</li>
</ul>
<h3 class="unnumbered" id="november-13-2020">November 13, 2020</h3>
<ul>
<li>Documented PEP 517 Poetry installation usage.</li>
<li>A large number of minor code fixes throughout, as well as some typos
in the text.</li>
<li>HTML version is now included, for easier copy/pasting.</li>
<li>Noted an issue with Conda-Pack that will hopefully be fixed
soon.</li>
</ul>
<h3 class="unnumbered" id="october-22-2020">October 22, 2020</h3>
<ul>
<li>Expanded and more accurate Poetry two-step install, plus added three
more Poetry-related best practices.</li>
<li>Added best practice on adding <code>.git</code> to
<code>.dockerignore</code>, and what to do when you can’t.</li>
</ul>
<h3 class="unnumbered" id="september-22-2020">September 22, 2020</h3>
<p>Added best practices:</p>
<ul>
<li>Using <code>conda clean</code> for smaller Conda-based Docker
images.</li>
<li>Using OpenBLAS instead of MKL for smaller Docker images.</li>
<li>Using <code>conda-pack</code> and multi-stage builds for even
smaller Conda-based Docker images.</li>
</ul>
<p>Noted that the <code>safety</code> vulnerability scanner is not
licensed for commercial use.</p>
<h3 class="unnumbered" id="june-17-2020">June 17, 2020</h3>
<p>More best practices:</p>
<ul>
<li>Various BuildKit features that help speed up builds.</li>
<li>Dropping capabilities.</li>
<li>Avoiding listening on ports &lt; 1024.</li>
<li>Running a different command altogether based on command-line
arguments.</li>
<li>Bytecode compilation.</li>
<li>Additional image size optimization in multi-stage builds.</li>
<li>Warm up the build cache for per-branch builds.</li>
<li>Requirements for running on Heroku and Google Cloud Run.</li>
<li>BuildKit ssh-agent forwarding.</li>
<li>BuildKit secrets when using Docker Compose.</li>
</ul>
<p>Other tweaks and improvements throughout the text.</p>
<h3 class="unnumbered" id="june-8-2020">June 8, 2020</h3>
<p>The Checklist has been renamed, and is now known as a Quickstart. To
help make that change:</p>
<ul>
<li>Added a new introductory chapter with a plan to help you figure out
which best practices to implement when.</li>
<li>Tweaked the chapter structure.</li>
</ul>
<p>Additionally:</p>
<ul>
<li>Added a best practice about using <code>dive</code> to find large
layers.</li>
<li>Split off <code>init</code> into its own best practice.</li>
<li>Explained the goal of responding to health checks quickly more
broadly, rather than in specific implementation terms of process/thread
pool.</li>
<li>Added more nuance to the section on updating dependencies once a
month.</li>
<li>Restored the Pipenv instructions, since it’s now being maintained
again.</li>
<li>Make the <code>DEBIAN_FRONTEND=noninteractive</code> best practice
standalone.</li>
</ul>
<h3 class="unnumbered" id="june-2-2020">June 2, 2020</h3>
<p>Added multiple new best practices:</p>
<ul>
<li>Making sure <code>ARG</code> doesn’t break build caching.</li>
<li>Don’t use the <code>latest</code> tag.</li>
<li>For better reproducibility, you can create a custom base image.</li>
<li>Size checks for images.</li>
<li>Security scanners: <code>bandit</code>, <code>safety</code>,
<code>trivy</code>.</li>
</ul>
<p>Also updated existing best practices:</p>
<ul>
<li>You can make a build arg available at runtime by using
<code>ENV</code>.</li>
<li>For build secrets another alternative is short term keys.</li>
</ul>
<h3 class="unnumbered" id="april-27-2020">April 27, 2020</h3>
<ul>
<li>Added new best practice on timely security updates, with additional
information on automatic notifications.</li>
<li>Switched some examples from shell session transcripts to
<code>Dockerfile</code> or shell script.</li>
<li>Noted <code>docker build</code> and Docker Compose support for
targeting named stages.</li>
</ul>
<h3 class="unnumbered" id="april-1-2020">April 1, 2020</h3>
<ul>
<li>Documented two-stage install with Poetry.</li>
<li>Added link to <a
href="https://github.com/willfarrell/docker-autoheal">docker-autoheal</a>.</li>
</ul>
<h3 class="unnumbered" id="february-24-2020">February 24, 2020</h3>
<p>Added many more examples:</p>
<ul>
<li>Configuring <code>logging</code>.</li>
<li>A smoke test.</li>
<li>Passing in secrets with BuildKit.</li>
<li><code>.dockerignore</code> file.</li>
<li>System package upgrade script for CentOS/RHEL.</li>
<li><code>Dockerfile</code> healthcheck.</li>
<li>And a few more expanded examples here and there.</li>
</ul>
</body>
</html>
